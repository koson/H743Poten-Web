#!/usr/bin/env python3
"""
PLS Workflow with Enhanced Detector V4 Improved - Fixed Version
‡πÉ‡∏ä‡πâ Enhanced Detector V4 Improved ‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import os
import glob
import re
from pathlib import Path
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import Enhanced Detector V4 Improved
try:
    from enhanced_detector_v4_improved import EnhancedDetectorV4Improved
    print("‚úÖ Enhanced Detector V4 Improved ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    HAS_ENHANCED_V4_IMPROVED = True
except ImportError as e:
    print("‚ùå Enhanced Detector V4 Improved ‡πÑ‡∏°‡πà‡∏û‡∏ö")
    HAS_ENHANCED_V4_IMPROVED = False

class PLSWorkflowEnhancedV4ImprovedFixed:
    """
    PLS Workflow ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ Enhanced Detector V4 Improved ‡πÅ‡∏ö‡∏ö Fixed
    ‡∏≠‡πà‡∏≤‡∏ô peaks ‡∏à‡∏≤‡∏Å result['peaks'] ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô anodic_peaks/cathodic_peaks
    """
    
    def __init__(self, output_dir="pls_results_v4_improved_fixed"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize Enhanced Detector V4 Improved
        if HAS_ENHANCED_V4_IMPROVED:
            self.detector = EnhancedDetectorV4Improved(confidence_threshold=25.0)
        else:
            self.detector = None
            
        # Statistics
        self.stats = {
            'total_files': 0,
            'successful_detections': 0,
            'failed_detections': 0,
            'total_peaks': 0,
            'files_with_both_peaks': 0
        }
        
    def extract_metadata_from_filename(self, filename):
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• metadata ‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå"""
        # Example: "Palmsens_0.5mM_CV_100mVpS_E1_scan_01.csv"
        # Pattern: Device_Concentration_Method_ScanRate_Electrode_scan_Number.csv
        
        filename_only = Path(filename).stem
        
        # Regular expressions for different patterns
        patterns = [
            r'(\w+)_([0-9.]+mM)_CV_([0-9.]+mVpS)_(\w+)_scan_(\d+)',
            r'(\w+)_([0-9.]+)mM_CV_([0-9.]+)mVs_(\w+)_scan_(\d+)',
            r'(\w+)_([0-9.]+mM)_([0-9.]+mVpS)_(\w+)_scan_(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename_only)
            if match:
                groups = match.groups()
                if len(groups) >= 4:
                    device = groups[0]
                    concentration_str = groups[1]
                    scan_rate_str = groups[2] if len(groups) > 2 else "unknown"
                    electrode = groups[3] if len(groups) > 3 else "unknown"
                    scan_number = groups[4] if len(groups) > 4 else "1"
                    
                    # Extract numeric concentration
                    conc_match = re.search(r'([0-9.]+)', concentration_str)
                    concentration = float(conc_match.group(1)) if conc_match else 0.0
                    
                    # Extract scan rate
                    rate_match = re.search(r'([0-9.]+)', scan_rate_str)
                    scan_rate = float(rate_match.group(1)) if rate_match else 0.0
                    
                    return {
                        'device': device,
                        'concentration': concentration,
                        'scan_rate': scan_rate,
                        'electrode': electrode,
                        'scan_number': int(scan_number),
                        'filename': filename_only
                    }
        
        # Fallback
        return {
            'device': 'unknown',
            'concentration': 0.0,
            'scan_rate': 0.0,
            'electrode': 'unknown',
            'scan_number': 1,
            'filename': filename_only
        }
    
    def load_and_analyze_cv_file(self, filepath):
        """‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CV ‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ Enhanced Detector V4 Improved - Fixed"""
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå CSV (skip 2 header rows)
            data = pd.read_csv(filepath, skiprows=2, names=['voltage', 'current'])
            
            if data.empty or len(data) < 10:
                logger.warning(f"   ‚ùå ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: {len(data)} ‡∏à‡∏∏‡∏î")
                return None
            
            # ‡πÉ‡∏ä‡πâ Enhanced Detector V4 Improved ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            logger.info(f"üî¨ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {filepath.name} ‡∏î‡πâ‡∏ß‡∏¢ Enhanced Detector V4 Improved...")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á data structure ‡∏ó‡∏µ‡πà Enhanced V4 ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡πÅ‡∏ö‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ)
            cv_data = {
                'voltage': data['voltage'].tolist(),  # Convert to list like web API
                'current': data['current'].tolist()   # Convert to list like web API
            }
            
            # ‡∏£‡∏±‡∏ô Enhanced Detector V4 Improved
            results = self.detector.analyze_cv_data(cv_data)
            
            if not results:
                logger.warning(f"   ‚ùå Enhanced V4 Improved ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ")
                return None
            
            # **FIX: ‡∏≠‡πà‡∏≤‡∏ô peaks ‡∏à‡∏≤‡∏Å results['peaks'] ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô anodic_peaks/cathodic_peaks**
            all_peaks = results.get('peaks', [])
            
            if not all_peaks:
                logger.warning(f"   ‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö peaks ‡πÉ‡∏ô results['peaks']")
                return None
            
            # ‡πÅ‡∏¢‡∏Å peaks ‡∏ï‡∏≤‡∏° type
            anodic_peaks = [p for p in all_peaks if p.get('type') == 'oxidation']
            cathodic_peaks = [p for p in all_peaks if p.get('type') == 'reduction']
            
            logger.info(f"   ‚úÖ ‡∏û‡∏ö peaks: {len(anodic_peaks)} anodic + {len(cathodic_peaks)} cathodic = {len(all_peaks)} total")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ peak ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á 2 ‡∏î‡πâ‡∏≤‡∏ô
            has_both_peaks = len(anodic_peaks) > 0 and len(cathodic_peaks) > 0
            
            if has_both_peaks:
                self.stats['files_with_both_peaks'] += 1
                logger.info(f"   üéØ ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏µ peak ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á 2 ‡∏î‡πâ‡∏≤‡∏ô!")
            
            # Extract metadata
            metadata = self.extract_metadata_from_filename(filepath.name)
            
            return {
                'filename': filepath.name,
                'filepath': str(filepath),
                'metadata': metadata,
                'voltage': data['voltage'].values,
                'current': data['current'].values,
                'anodic_peaks': anodic_peaks,
                'cathodic_peaks': cathodic_peaks,
                'all_peaks': all_peaks,
                'has_both_peaks': has_both_peaks,
                'enhanced_v4_results': results,
                'data_points': len(data)
            }
            
        except Exception as e:
            logger.error(f"   ‚ùå Error loading {filepath.name}: {e}")
            return None
    
    def scan_directories(self, base_dirs):
        """‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå CV ‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ"""
        all_files = []
        
        for base_dir in base_dirs:
            if not os.path.exists(base_dir):
                logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ {base_dir} ‡πÑ‡∏°‡πà‡∏û‡∏ö")
                continue
                
            # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå .csv ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            pattern = os.path.join(base_dir, "**", "*.csv")
            files = glob.glob(pattern, recursive=True)
            
            logger.info(f"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {base_dir}: {len(files)} ‡πÑ‡∏ü‡∏•‡πå")
            all_files.extend([Path(f) for f in files])
        
        logger.info(f"üìÅ ‡∏£‡∏ß‡∏°‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(all_files)} ‡πÑ‡∏ü‡∏•‡πå")
        return all_files
    
    def run_analysis(self, data_directories):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå PLS"""
        logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô PLS Analysis ‡∏î‡πâ‡∏ß‡∏¢ Enhanced V4 Improved Fixed")
        
        if not HAS_ENHANCED_V4_IMPROVED:
            logger.error("‚ùå Enhanced Detector V4 Improved ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
            return None
        
        # 1. ‡∏™‡πÅ‡∏Å‡∏ô‡πÑ‡∏ü‡∏•‡πå
        cv_files = self.scan_directories(data_directories)
        
        if not cv_files:
            logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå CV")
            return None
        
        # 2. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
        logger.info(f"üî¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {len(cv_files)} ‡πÑ‡∏ü‡∏•‡πå...")
        
        valid_data = []
        
        for i, filepath in enumerate(cv_files[:10]):  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 10 ‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏£‡∏Å
            logger.info(f"üìÑ [{i+1}/{len(cv_files[:10])}] {filepath.name}")
            self.stats['total_files'] += 1
            
            result = self.load_and_analyze_cv_file(filepath)
            
            if result:
                valid_data.append(result)
                self.stats['successful_detections'] += 1
                self.stats['total_peaks'] += len(result['all_peaks'])
            else:
                self.stats['failed_detections'] += 1
        
        logger.info(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à: {len(valid_data)}/{len(cv_files[:10])} ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        
        if not valid_data:
            logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ")
            return None
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á visualization
        self.create_summary_plot(valid_data)
        
        # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        self.create_report(valid_data)
        
        return valid_data
    
    def create_summary_plot(self, data_list):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        if not data_list:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot 1: CV curves ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        ax1 = axes[0, 0]
        for i, data in enumerate(data_list[:5]):  # ‡πÅ‡∏™‡∏î‡∏á 5 curves ‡πÅ‡∏£‡∏Å
            ax1.plot(data['voltage'], data['current'], alpha=0.7, label=data['metadata']['device'])
            
            # Mark peaks
            for peak in data['all_peaks']:
                color = 'red' if peak['type'] == 'oxidation' else 'blue'
                ax1.scatter(peak['voltage'], peak['current'], color=color, s=50, zorder=5)
        
        ax1.set_xlabel('Voltage (V)')
        ax1.set_ylabel('Current (¬µA)')
        ax1.set_title('CV Curves with Detected Peaks')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # Plot 2: Peak count distribution
        ax2 = axes[0, 1]
        peak_counts = [len(data['all_peaks']) for data in data_list]
        ax2.hist(peak_counts, bins=range(max(peak_counts)+2), alpha=0.7, edgecolor='black')
        ax2.set_xlabel('Number of Peaks')
        ax2.set_ylabel('Number of Files')
        ax2.set_title('Peak Count Distribution')
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Concentration vs Peak info
        ax3 = axes[1, 0]
        concentrations = [data['metadata']['concentration'] for data in data_list]
        ox_counts = [len(data['anodic_peaks']) for data in data_list]
        red_counts = [len(data['cathodic_peaks']) for data in data_list]
        
        ax3.scatter(concentrations, ox_counts, color='red', label='Oxidation', alpha=0.7)
        ax3.scatter(concentrations, red_counts, color='blue', label='Reduction', alpha=0.7)
        ax3.set_xlabel('Concentration (mM)')
        ax3.set_ylabel('Peak Count')
        ax3.set_title('Concentration vs Peak Count')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Statistics summary
        ax4 = axes[1, 1]
        stats_data = [
            self.stats['successful_detections'],
            self.stats['failed_detections'],
            self.stats['files_with_both_peaks']
        ]
        labels = ['Successful', 'Failed', 'Both Peaks']
        colors = ['green', 'red', 'gold']
        
        ax4.bar(labels, stats_data, color=colors, alpha=0.7)
        ax4.set_ylabel('Number of Files')
        ax4.set_title('Detection Statistics')
        
        for i, v in enumerate(stats_data):
            ax4.text(i, v + 0.1, str(v), ha='center', va='bottom')
        
        plt.tight_layout()
        
        # Save plot
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = self.output_dir / f"pls_analysis_v4_improved_fixed_{timestamp}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        
        logger.info(f"üìä ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Å‡∏£‡∏≤‡∏ü: {plot_path}")
        
        plt.show()
        
    def create_report(self, data_list):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.output_dir / f"pls_report_v4_improved_fixed_{timestamp}.json"
        
        report = {
            'timestamp': timestamp,
            'method': 'Enhanced Detector V4 Improved - Fixed',
            'statistics': self.stats,
            'analysis_summary': {
                'total_files_analyzed': len(data_list),
                'unique_concentrations': len(set(d['metadata']['concentration'] for d in data_list)),
                'unique_devices': len(set(d['metadata']['device'] for d in data_list)),
                'average_peaks_per_file': self.stats['total_peaks'] / len(data_list) if data_list else 0,
                'success_rate': self.stats['successful_detections'] / self.stats['total_files'] * 100 if self.stats['total_files'] > 0 else 0
            },
            'files_analyzed': [
                {
                    'filename': data['filename'],
                    'metadata': data['metadata'],
                    'peak_count': len(data['all_peaks']),
                    'anodic_peaks': len(data['anodic_peaks']),
                    'cathodic_peaks': len(data['cathodic_peaks']),
                    'has_both_peaks': data['has_both_peaks']
                }
                for data in data_list
            ]
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìã ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {report_path}")

def main():
    """Main function"""
    if not HAS_ENHANCED_V4_IMPROVED:
        print("‚ùå Enhanced Detector V4 Improved ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
        return
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ data directories
    data_directories = [
        "Test_data/Palmsens",
        "Test_data/Stm32"
    ]
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á workflow instance
    workflow = PLSWorkflowEnhancedV4ImprovedFixed()
    
    # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    results = workflow.run_analysis(data_directories)
    
    if results:
        print(f"\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ {len(results)} ‡πÑ‡∏ü‡∏•‡πå")
        print(f"üìÅ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô: {workflow.output_dir}")
    else:
        print("\n‚ùå ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")

if __name__ == "__main__":
    main()
