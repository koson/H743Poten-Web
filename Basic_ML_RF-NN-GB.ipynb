{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73f9c5a",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# üß† **MACHINE LEARNING FUNDAMENTALS LECTURE**\n",
    "## ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô - ‡∏à‡∏≤‡∏Å‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "\n",
    "---\n",
    "\n",
    "### üìö **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ:**\n",
    "\n",
    "1. **ü§î Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?** - ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó\n",
    "2. **üìä Data & Features** - ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ feature engineering  \n",
    "3. **üå≥ Random Forest** - ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á\n",
    "4. **üß† Neural Networks** - ‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå\n",
    "5. **‚ö° Gradient Boosting** - ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏£‡∏á\n",
    "6. **üéØ Hands-on Practice** - ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "7. **üöÄ Real-world Applications** - ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á\n",
    "\n",
    "---\n",
    "\n",
    "> **üéì ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ ML ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093264f5",
   "metadata": {},
   "source": [
    "## ü§î **CHAPTER 1: MACHINE LEARNING ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n",
    "\n",
    "### **üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô**\n",
    "\n",
    "**Machine Learning (ML)** = ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå **\"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\"** ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏ö‡∏ö step-by-step\n",
    "\n",
    "**‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:**\n",
    "- **Traditional Programming**: Input + Program ‚Üí Output\n",
    "- **Machine Learning**: Input + Output ‚Üí Program (Model)\n",
    "\n",
    "### **üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á:**\n",
    "\n",
    "**üìß Email Spam Detection:**\n",
    "- **Input**: ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ email (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°, ‡∏ú‡∏π‡πâ‡∏™‡πà‡∏á, ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)\n",
    "- **Output**: Spam ‡∏´‡∏£‡∏∑‡∏≠ Not Spam\n",
    "- **‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**: ‡∏î‡∏π‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á email ‡∏´‡∏•‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏â‡∏ö‡∏±‡∏ö ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ pattern\n",
    "\n",
    "**üõí Netflix Recommendation:**\n",
    "- **Input**: ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏´‡∏ô‡∏±‡∏á, rating ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ\n",
    "- **Output**: ‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏ä‡∏≠‡∏ö\n",
    "- **‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‚Üí ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "\n",
    "**üöó Self-Driving Cars:**\n",
    "- **Input**: ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≠‡∏á, sensor ‡∏ï‡πà‡∏≤‡∏á‡πÜ\n",
    "- **Output**: ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏Ç‡∏±‡∏ö (‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏ß, ‡∏´‡∏¢‡∏∏‡∏î, ‡πÄ‡∏£‡πà‡∏á)\n",
    "- **‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**: ‡∏ù‡∏∂‡∏Å‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡∏ö‡∏Ç‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏¢‡∏•‡πâ‡∏≤‡∏ô‡πÑ‡∏°‡∏•‡πå\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Machine Learning**\n",
    "\n",
    "#### **1. üéØ Supervised Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ñ‡∏£‡∏π)**\n",
    "- ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **input ‡πÅ‡∏•‡∏∞ output ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á** ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ\n",
    "- ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏â‡∏•‡∏¢\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "- **Classification**: ‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (email spam/not spam, ‡πÇ‡∏£‡∏Ñ/‡πÑ‡∏°‡πà‡πÇ‡∏£‡∏Ñ)\n",
    "- **Regression**: ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô, ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥, ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô)\n",
    "\n",
    "#### **2. üîç Unsupervised Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏£‡∏π)**\n",
    "- ‡∏°‡∏µ‡πÅ‡∏Ñ‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **input** ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏â‡∏•‡∏¢\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏≤ **pattern** ‡∏´‡∏£‡∏∑‡∏≠ **structure** ‡πÄ‡∏≠‡∏á\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "- **Clustering**: ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤, ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏¢‡∏µ‡∏ô\n",
    "- **Anomaly Detection**: ‡∏´‡∏≤‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥, fraud detection\n",
    "\n",
    "#### **3. üéÆ Reinforcement Learning (‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏£‡∏á)**\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å **trial and error**\n",
    "- ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö reward/punishment ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**\n",
    "- ‡πÄ‡∏Å‡∏° AI (AlphaGo, Dota 2)\n",
    "- Robot navigation\n",
    "- Stock trading algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡πÉ‡∏ô‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏ô‡πâ‡∏ô Supervised Learning**\n",
    "\n",
    "‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô **potentiostat calibration** ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3539b25",
   "metadata": {},
   "source": [
    "## üìä **CHAPTER 2: DATA & FEATURES - ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á ML**\n",
    "\n",
    "### **üéØ Data ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏•‡∏±‡∏á‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô ML**\n",
    "\n",
    "> **\"Garbage In, Garbage Out\"** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏µ ‚Üí Model ‡∏î‡∏µ, ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡πà ‚Üí Model ‡πÅ‡∏¢‡πà\n",
    "\n",
    "### **üìã ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**\n",
    "\n",
    "#### **1. üè† Dataset Structure**\n",
    "```\n",
    "Dataset = Table with Rows and Columns\n",
    "\n",
    "     Features (X)                    Target (y)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Size ‚îÇ Bedrooms ‚îÇ Location ‚îÇAge ‚îÇ    Price    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ 120  ‚îÇ    3     ‚îÇ Bangkok  ‚îÇ 5  ‚îÇ  3,000,000  ‚îÇ\n",
    "‚îÇ 80   ‚îÇ    2     ‚îÇ Nonthu   ‚îÇ 10 ‚îÇ  1,500,000  ‚îÇ\n",
    "‚îÇ 200  ‚îÇ    4     ‚îÇ Sukhumvit‚îÇ 2  ‚îÇ  8,000,000  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "#### **2. üéØ Features (X) - ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞**\n",
    "- **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢** (input variables)\n",
    "- **‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á features = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏Ç‡∏≠‡∏á model**\n",
    "\n",
    "**‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Features:**\n",
    "- **üìä Numerical**: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (‡∏≠‡∏≤‡∏¢‡∏∏, ‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥)\n",
    "- **üìù Categorical**: ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (‡∏™‡∏µ, ‡πÄ‡∏û‡∏®, ‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠)  \n",
    "- **üìÖ Temporal**: ‡πÄ‡∏ß‡∏•‡∏≤ (‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà, ‡πÄ‡∏ß‡∏•‡∏≤)\n",
    "- **üìç Spatial**: ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á (‡∏•‡∏∞‡∏ï‡∏¥‡∏à‡∏π‡∏î, ‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏à‡∏π‡∏î)\n",
    "\n",
    "#### **3. üéØ Target (y) - ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏°**\n",
    "- **‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢** (output variable)\n",
    "- ‡πÉ‡∏ô **Classification**: ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (A, B, C ‡∏´‡∏£‡∏∑‡∏≠ 0, 1)\n",
    "- ‡πÉ‡∏ô **Regression**: ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥)\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß Feature Engineering - ‡∏®‡∏¥‡∏•‡∏õ‡∏∞‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Features**\n",
    "\n",
    "#### **üé® ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏´‡∏°‡πà:**\n",
    "\n",
    "**1. ‚úÇÔ∏è Feature Transformation**\n",
    "```python\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\n",
    "temperature_celsius = [20, 25, 30, 35]\n",
    "temperature_kelvin = [t + 273.15 for t in temperature_celsius]\n",
    "temperature_squared = [t**2 for t in temperature_celsius]  # Non-linear\n",
    "```\n",
    "\n",
    "**2. üîó Feature Combination**\n",
    "```python\n",
    "# ‡∏£‡∏ß‡∏° features ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n",
    "house_area = 120  # ‡∏ï‡∏£.‡∏°.\n",
    "num_bedrooms = 3\n",
    "area_per_bedroom = house_area / num_bedrooms  # Feature ‡πÉ‡∏´‡∏°‡πà!\n",
    "```\n",
    "\n",
    "**3. üìä Statistical Features**\n",
    "```python\n",
    "# ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• time series\n",
    "prices = [100, 105, 98, 110, 95]\n",
    "price_mean = sum(prices) / len(prices)\n",
    "price_std = calculate_standard_deviation(prices)\n",
    "price_trend = (prices[-1] - prices[0]) / len(prices)\n",
    "```\n",
    "\n",
    "#### **üßπ Data Preprocessing**\n",
    "\n",
    "**1. üîç Missing Data Handling**\n",
    "```python\n",
    "# ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢\n",
    "missing_strategies = {\n",
    "    \"‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á\": \"‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏≤‡∏¢‡∏ô‡πâ‡∏≠‡∏¢\",\n",
    "    \"‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢\": \"‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\", \n",
    "    \"‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î\": \"‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà\",\n",
    "    \"‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å features ‡∏≠‡∏∑‡πà‡∏ô\": \"‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\"\n",
    "}\n",
    "```\n",
    "\n",
    "**2. ‚öñÔ∏è Feature Scaling**\n",
    "```python\n",
    "# ‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Features ‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô\n",
    "age = 25        # ‡∏õ‡∏µ (0-100)\n",
    "salary = 50000  # ‡∏ö‡∏≤‡∏ó (0-1,000,000)\n",
    "\n",
    "# ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ: Normalization\n",
    "age_normalized = (25 - 0) / (100 - 0) = 0.25\n",
    "salary_normalized = (50000 - 0) / (1000000 - 0) = 0.05\n",
    "```\n",
    "\n",
    "**3. üè∑Ô∏è Categorical Encoding**\n",
    "```python\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á categories ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
    "colors = [\"Red\", \"Blue\", \"Green\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "Red = [1, 0, 0]\n",
    "Blue = [0, 1, 0] \n",
    "Green = [0, 0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Golden Rules ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Preparation**\n",
    "\n",
    "1. **üßê ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô**: ‡∏™‡∏≥‡∏£‡∏ß‡∏à, visualize, ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå\n",
    "2. **üßπ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î**: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing values, outliers\n",
    "3. **‚öñÔ∏è Scale features**: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "4. **üé® Create meaningful features**: ‡πÉ‡∏ä‡πâ domain knowledge\n",
    "5. **‚úÇÔ∏è Remove irrelevant features**: ‡πÄ‡∏Å‡πá‡∏ö‡πÅ‡∏ï‡πà‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå\n",
    "6. **üìä Validate quality**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏ó‡∏£‡∏ô model\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Potentiostat Data Preparation**\n",
    "\n",
    "```python\n",
    "# Raw data ‡∏à‡∏≤‡∏Å STM32\n",
    "raw_features = {\n",
    "    'adc_voltage': 2048,      # ADC reading (0-4095)\n",
    "    'adc_current': 1024,      # ADC reading  \n",
    "    'temperature': 25.5,      # ¬∞C\n",
    "    'timestamp': 1627834567   # Unix timestamp\n",
    "}\n",
    "\n",
    "# Feature engineering\n",
    "processed_features = {\n",
    "    'voltage_volts': (2048 / 4095) * 3.3,  # Convert ADC to volts\n",
    "    'current_amps': ((1024 / 4095) * 3.3 - 1.65) / 0.1,  # Convert to amps\n",
    "    'temp_kelvin': 25.5 + 273.15,          # Kelvin\n",
    "    'power': voltage * current,             # Calculated power\n",
    "    'time_since_start': timestamp - start_time,  # Relative time\n",
    "    'temp_normalized': (25.5 - 20) / (30 - 20)  # 0-1 scale\n",
    "}\n",
    "```\n",
    "\n",
    "**üéì Features ‡∏ó‡∏µ‡πà‡∏î‡∏µ = Model ‡∏ó‡∏µ‡πà‡∏î‡∏µ!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea75ae",
   "metadata": {},
   "source": [
    "## üå≥ **CHAPTER 3: RANDOM FOREST - ‡∏õ‡πà‡∏≤‡πÑ‡∏ú‡πà‡πÅ‡∏´‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à**\n",
    "\n",
    "### **üéØ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**\n",
    "\n",
    "**Random Forest** = **‡∏´‡∏•‡∏≤‡∏¢‡πÜ Decision Tree** ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡πâ‡∏ß **vote** ‡πÄ‡∏≠‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "\n",
    "### **üå≤ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Decision Tree**\n",
    "\n",
    "#### **ü§î Decision Tree ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**\n",
    "- ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô **flowchart ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à** ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô\n",
    "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ **node** ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°, ‡πÅ‡∏ï‡πà‡∏•‡∏∞ **leaf** ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢**\n",
    "```\n",
    "                   ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡πÑ‡∏´‡∏°?\n",
    "                  /          \\\n",
    "               ‡πÉ‡∏ä‡πà               ‡πÑ‡∏°‡πà\n",
    "              /                   \\\n",
    "      ‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡∏≠‡∏Å          ‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÑ‡∏´‡∏°?\n",
    "                                  /        \\\n",
    "                               ‡πÉ‡∏ä‡πà         ‡πÑ‡∏°‡πà\n",
    "                              /              \\\n",
    "                      ‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô      ‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô\n",
    "```\n",
    "\n",
    "#### **üîß Decision Tree ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML**\n",
    "\n",
    "**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô**\n",
    "```python\n",
    "# Training data\n",
    "houses = [\n",
    "    {'size': 120, 'bedrooms': 3, 'location': 'center', 'price': 3000000},\n",
    "    {'size': 80,  'bedrooms': 2, 'location': 'suburb', 'price': 1500000},\n",
    "    {'size': 200, 'bedrooms': 4, 'location': 'center', 'price': 8000000}\n",
    "]\n",
    "\n",
    "# Decision Tree ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ\n",
    "\"\"\"\n",
    "              Size > 150?\n",
    "             /           \\\n",
    "          No               Yes\n",
    "         /                   \\\n",
    "   Location = center?      Price = 8M\n",
    "    /              \\\n",
    "  Yes              No\n",
    "  /                 \\\n",
    "Price = 3M       Price = 1.5M\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### **üå≥ ‡∏à‡∏≤‡∏Å 1 Tree ‡∏™‡∏π‡πà Forest**\n",
    "\n",
    "#### **‚ùå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á Decision Tree ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß**\n",
    "1. **Overfitting**: ‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ\n",
    "2. **High Variance**: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ tree ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "3. **Bias**: ‡∏≠‡∏≤‡∏à‡∏à‡∏±‡∏ö pattern ‡∏ú‡∏¥‡∏î\n",
    "\n",
    "#### **‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ: Random Forest**\n",
    "\n",
    "**‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: \"‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏´‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\"**\n",
    "\n",
    "```python\n",
    "# ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏°‡∏µ Decision Tree 1 ‡∏ï‡∏±‡∏ß\n",
    "single_tree = DecisionTree()\n",
    "\n",
    "# ‡πÄ‡∏£‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏ï‡∏±‡∏ß!\n",
    "forest = [\n",
    "    DecisionTree(random_data_1),\n",
    "    DecisionTree(random_data_2), \n",
    "    DecisionTree(random_data_3),\n",
    "    # ... ‡∏≠‡∏µ‡∏Å 97 ‡∏ï‡∏±‡∏ß (‡∏£‡∏ß‡∏° 100 trees)\n",
    "]\n",
    "\n",
    "# ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ = ‡πÄ‡∏≠‡∏≤‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å trees ‡∏°‡∏≤ vote\n",
    "prediction = majority_vote(forest.predict(new_data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üé≤ \"Random\" ‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏´‡∏ô?**\n",
    "\n",
    "#### **1. üéØ Random Sampling (Bootstrap)**\n",
    "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ tree ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô\n",
    "- ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training (with replacement)\n",
    "\n",
    "```python\n",
    "# ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "original_data = [sample_1, sample_2, ..., sample_1000]\n",
    "\n",
    "# Tree 1 ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏™‡∏∏‡πà‡∏° 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)\n",
    "tree1_data = random.sample(original_data, 1000, replace=True)\n",
    "# ‚Üí ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ [sample_3, sample_3, sample_1, sample_999, ...]\n",
    "\n",
    "# Tree 2 ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏™‡∏∏‡πà‡∏°‡πÉ‡∏´‡∏°‡πà)  \n",
    "tree2_data = random.sample(original_data, 1000, replace=True)\n",
    "# ‚Üí [sample_1, sample_500, sample_2, sample_1, ...]\n",
    "```\n",
    "\n",
    "#### **2. üéØ Random Feature Selection**\n",
    "- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ node ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô\n",
    "- ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ features ‡∏ó‡∏µ‡πà‡πÅ‡∏£‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏á‡∏≥\n",
    "\n",
    "```python\n",
    "# ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏°‡∏µ features 10 ‡∏ï‡∏±‡∏ß\n",
    "all_features = ['size', 'bedrooms', 'age', 'location', 'garden', \n",
    "               'parking', 'floor', 'view', 'school', 'transport']\n",
    "\n",
    "# ‡πÅ‡∏ï‡πà‡∏•‡∏∞ node ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏û‡∏µ‡∏¢‡∏á sqrt(10) ‚âà 3 ‡∏ï‡∏±‡∏ß\n",
    "node1_features = random.choice(['size', 'location', 'age'])\n",
    "node2_features = random.choice(['bedrooms', 'garden', 'view'])\n",
    "# ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üó≥Ô∏è Voting Mechanism**\n",
    "\n",
    "#### **üìä ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Regression (‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)**\n",
    "```python\n",
    "# 100 trees ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô\n",
    "predictions = [\n",
    "    2950000, 3100000, 2900000, 3050000, 2980000,\n",
    "    # ... ‡∏≠‡∏µ‡∏Å 95 ‡∏Ñ‡πà‡∏≤\n",
    "]\n",
    "\n",
    "# Final prediction = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢\n",
    "final_price = sum(predictions) / len(predictions)\n",
    "# = 3,010,000 ‡∏ö‡∏≤‡∏ó\n",
    "```\n",
    "\n",
    "#### **üè∑Ô∏è ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Classification (‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó)**\n",
    "```python\n",
    "# 100 trees ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ß‡πà‡∏≤ email ‡πÄ‡∏õ‡πá‡∏ô spam ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "predictions = [\n",
    "    'spam', 'not_spam', 'spam', 'spam', 'not_spam',\n",
    "    # ... ‡∏≠‡∏µ‡∏Å 95 ‡∏Ñ‡πà‡∏≤\n",
    "]\n",
    "\n",
    "# ‡∏ô‡∏±‡∏ö votes\n",
    "spam_votes = 67\n",
    "not_spam_votes = 33\n",
    "\n",
    "# Final prediction = majority vote\n",
    "final_prediction = 'spam'  # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏î‡πâ 67 > 33 votes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Random Forest**\n",
    "\n",
    "#### **1. üõ°Ô∏è Robust & Stable**\n",
    "- **‡∏ó‡∏ô‡∏ï‡πà‡∏≠ noise**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏°‡∏≤‡∏Å\n",
    "- **‡∏ó‡∏ô‡∏ï‡πà‡∏≠ outliers**: ‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏™‡∏µ‡∏¢\n",
    "- **‡πÑ‡∏°‡πà overfitting**: ensemble effect ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô\n",
    "\n",
    "#### **2. ‚ö° ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢**\n",
    "- **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á feature scaling**: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö\n",
    "- **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á tuning ‡∏°‡∏≤‡∏Å**: default parameters ‡∏î‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
    "- **‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing values ‡πÑ‡∏î‡πâ**: ‡∏°‡∏µ built-in mechanism\n",
    "\n",
    "#### **3. üîç Interpretable**\n",
    "- **Feature importance**: ‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "- **Tree visualization**: ‡∏î‡∏π decision path ‡πÑ‡∏î‡πâ\n",
    "\n",
    "#### **4. üöÄ Performance**\n",
    "- **Training ‡πÄ‡∏£‡πá‡∏ß**: trees ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏¢‡∏Å‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ (parallel)\n",
    "- **Prediction ‡πÄ‡∏£‡πá‡∏ß**: inference ‡πÑ‡∏°‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Random Forest**\n",
    "\n",
    "#### **1. üìä Memory Usage**\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÜ trees ‚Üí ‡πÉ‡∏ä‡πâ memory ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "- ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mobile/embedded systems\n",
    "\n",
    "#### **2. üîÑ Limited Extrapolation**\n",
    "- ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training\n",
    "- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ extrapolate ‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á‡πÑ‡∏î‡πâ\n",
    "\n",
    "#### **3. ü§ñ Less Flexible**\n",
    "- ‡πÑ‡∏°‡πà‡∏à‡∏±‡∏ö complex non-linear patterns ‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡πà‡∏≤ Neural Networks\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö tabular data ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ images/text\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Random Forest?**\n",
    "\n",
    "#### **‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Tabular data** (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á)\n",
    "- **Mixed data types** (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç + ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà)\n",
    "- **Need interpretability** (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ)\n",
    "- **Baseline model** (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ RF ‡∏Å‡πà‡∏≠‡∏ô)\n",
    "- **Limited time/resources** (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤ tune ‡∏°‡∏≤‡∏Å)\n",
    "\n",
    "#### **‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Image/Video data** (‡πÉ‡∏ä‡πâ Neural Networks ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤)\n",
    "- **Text data** (‡πÉ‡∏ä‡πâ NLP models ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤) \n",
    "- **Time series with strong temporal patterns**\n",
    "- **Very large datasets** (memory constraints)\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Random Forest ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô Potentiostat**\n",
    "\n",
    "```python\n",
    "# Features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö calibration\n",
    "features = [\n",
    "    'raw_voltage',      # ‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å ADC\n",
    "    'raw_current',      # ‡∏Å‡∏£‡∏∞‡πÅ‡∏™‡∏î‡∏¥‡∏ö‡∏à‡∏≤‡∏Å ADC\n",
    "    'temperature',      # ‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\n",
    "    'time_elapsed',     # ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ\n",
    "    'gain_setting',     # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ gain\n",
    "    'offset_value'      # ‡∏Ñ‡πà‡∏≤ offset\n",
    "]\n",
    "\n",
    "# Target\n",
    "target = 'calibrated_voltage'  # ‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,    # 100 trees\n",
    "    max_depth=10,        # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
    "    min_samples_split=5, # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å node\n",
    "    random_state=42      # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducibility\n",
    ")\n",
    "\n",
    "# ‡πÄ‡∏ó‡∏£‡∏ô model\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢\n",
    "calibrated_values = rf.predict(features_test)\n",
    "\n",
    "# ‡∏î‡∏π feature importance\n",
    "importance = rf.feature_importances_\n",
    "print(f\"Most important feature: {features[importance.argmax()]}\")\n",
    "```\n",
    "\n",
    "**üéì Random Forest = ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏≠!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340d7f",
   "metadata": {},
   "source": [
    "## üß† **CHAPTER 4: NEURAL NETWORKS - ‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå**\n",
    "\n",
    "### **üéØ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**\n",
    "\n",
    "**Neural Network** = ‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö **‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó (neurons)** ‡πÉ‡∏ô‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå\n",
    "\n",
    "### **üß¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å Neuron ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß**\n",
    "\n",
    "#### **üî¨ ‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**\n",
    "\n",
    "```\n",
    "‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó = ‡∏£‡∏±‡∏ö‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì ‚Üí ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• ‚Üí ‡∏™‡πà‡∏á‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏ï‡πà‡∏≠\n",
    "\n",
    "Input (Dendrites) ‚Üí Cell Body ‚Üí Output (Axon)\n",
    "    ‚Üì                ‚Üì           ‚Üì\n",
    "  ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤      ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à     ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏≠‡∏≠‡∏Å\n",
    "```\n",
    "\n",
    "#### **ü§ñ Artificial Neuron**\n",
    "\n",
    "```python\n",
    "# Mathematical model ‡∏Ç‡∏≠‡∏á neuron\n",
    "def artificial_neuron(inputs, weights, bias):\n",
    "    # 1. ‡∏£‡∏ß‡∏°‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤ (weighted sum)\n",
    "    weighted_sum = sum(input_i * weight_i for input_i, weight_i in zip(inputs, weights))\n",
    "    \n",
    "    # 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° bias\n",
    "    total = weighted_sum + bias\n",
    "    \n",
    "    # 3. ‡∏ú‡πà‡∏≤‡∏ô activation function\n",
    "    output = activation_function(total)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
    "inputs = [1.0, 0.5, -0.3]     # ‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤ 3 ‡∏ï‡∏±‡∏ß\n",
    "weights = [0.8, -0.4, 0.6]    # ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å (learned parameters)\n",
    "bias = 0.1                    # bias term\n",
    "\n",
    "output = artificial_neuron(inputs, weights, bias)\n",
    "```\n",
    "\n",
    "#### **üìä Activation Functions**\n",
    "\n",
    "**‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á:**\n",
    "- ‡∏™‡∏°‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á: neuron ‡∏à‡∏∞ \"fire\" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏Ç‡πâ‡∏≤\n",
    "- AI: activation function ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à output\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# 1. Sigmoid (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# 2. ReLU (Rectified Linear Unit) - ‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# 3. Tanh (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á -1 ‡∏ñ‡∏∂‡∏á 1)\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "x = 2.5\n",
    "print(f\"Sigmoid: {sigmoid(x)}\")  # 0.924\n",
    "print(f\"ReLU: {relu(x)}\")        # 2.5\n",
    "print(f\"Tanh: {tanh(x)}\")        # 0.987\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üèóÔ∏è ‡∏à‡∏≤‡∏Å Neuron ‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß‡∏™‡∏π‡πà Network**\n",
    "\n",
    "#### **üîó Multi-Layer Architecture**\n",
    "\n",
    "```\n",
    "Input Layer    Hidden Layer    Output Layer\n",
    "     ‚óã              ‚óã              ‚óã\n",
    "     ‚óã         ‚Üí    ‚óã         ‚Üí    ‚óã\n",
    "     ‚óã              ‚óã              ‚óã\n",
    "     ‚óã              ‚óã\n",
    "                    ‚óã\n",
    "```\n",
    "\n",
    "**‡πÅ‡∏ï‡πà‡∏•‡∏∞ layer ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà:**\n",
    "- **Input Layer**: ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• features\n",
    "- **Hidden Layer(s)**: ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏´‡∏≤ patterns\n",
    "- **Output Layer**: ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "\n",
    "#### **üîÑ Forward Propagation**\n",
    "\n",
    "```python\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô\n",
    "def simple_neural_network(house_features):\n",
    "    # Input: [size, bedrooms, age]\n",
    "    \n",
    "    # Hidden Layer 1 (3 neurons)\n",
    "    h1_1 = relu(house_features[0]*0.1 + house_features[1]*0.2 + house_features[2]*(-0.05) + 0.1)\n",
    "    h1_2 = relu(house_features[0]*0.15 + house_features[1]*(-0.1) + house_features[2]*0.08 + 0.2)\n",
    "    h1_3 = relu(house_features[0]*0.08 + house_features[1]*0.3 + house_features[2]*(-0.02) + (-0.1))\n",
    "    \n",
    "    # Hidden Layer 2 (2 neurons)  \n",
    "    h2_1 = relu(h1_1*0.5 + h1_2*0.3 + h1_3*(-0.2) + 0.1)\n",
    "    h2_2 = relu(h1_1*(-0.1) + h1_2*0.4 + h1_3*0.6 + 0.05)\n",
    "    \n",
    "    # Output Layer (1 neuron - price)\n",
    "    price = h2_1*1000000 + h2_2*800000 + 500000\n",
    "    \n",
    "    return price\n",
    "\n",
    "# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
    "house = [120, 3, 5]  # 120 sqm, 3 bedrooms, 5 years old\n",
    "predicted_price = simple_neural_network(house)\n",
    "print(f\"Predicted price: {predicted_price:,.0f} baht\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìö Training Neural Network**\n",
    "\n",
    "#### **üéØ How Learning Works**\n",
    "\n",
    "**1. ‚¨ÜÔ∏è Forward Pass**: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏´‡∏•‡∏à‡∏≤‡∏Å input ‚Üí output\n",
    "**2. üìä Calculate Loss**: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö prediction ‡∏Å‡∏±‡∏ö actual\n",
    "**3. ‚¨áÔ∏è Backward Pass**: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient (backpropagation)\n",
    "**4. üîß Update Weights**: ‡∏õ‡∏£‡∏±‡∏ö weights ‡πÉ‡∏´‡πâ loss ‡∏•‡∏î‡∏•‡∏á\n",
    "**5. üîÑ Repeat**: ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏£‡∏≠‡∏ö (epochs)\n",
    "\n",
    "```python\n",
    "# Simplified training loop\n",
    "def train_neural_network(X_train, y_train, epochs=1000):\n",
    "    # Initialize random weights\n",
    "    weights = initialize_random_weights()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = forward_pass(X_train, weights)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = mean_squared_error(y_train, predictions)\n",
    "        \n",
    "        # Backward pass (compute gradients)\n",
    "        gradients = backpropagation(loss, weights)\n",
    "        \n",
    "        # Update weights\n",
    "        weights = update_weights(weights, gradients, learning_rate=0.01)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return weights\n",
    "```\n",
    "\n",
    "#### **üèπ Backpropagation - ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**\n",
    "\n",
    "**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**: ‡∏´‡∏≤**‡∏ú‡∏π‡πâ‡∏£‡πâ‡∏≤‡∏¢** (weights ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î error) ‡πÅ‡∏•‡πâ‡∏ß**‡∏•‡∏á‡πÇ‡∏ó‡∏©** (‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤)\n",
    "\n",
    "```python\n",
    "# Chain Rule in Backpropagation\n",
    "\"\"\"\n",
    "Error = f(Output)\n",
    "Output = f(Hidden_Layer_2) \n",
    "Hidden_Layer_2 = f(Hidden_Layer_1)\n",
    "Hidden_Layer_1 = f(Input, Weights)\n",
    "\n",
    "‚àÇError/‚àÇWeights = ‚àÇError/‚àÇOutput √ó ‚àÇOutput/‚àÇHidden_2 √ó ‚àÇHidden_2/‚àÇHidden_1 √ó ‚àÇHidden_1/‚àÇWeights\n",
    "\n",
    "‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏ã‡πà‡∏•‡∏π‡∏Å‡πÇ‡∏ã‡πà - ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡πÉ‡∏î‡∏Ç‡πâ‡∏≠‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô ‡∏°‡∏±‡∏ô‡∏™‡πà‡∏á‡∏ú‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üéõÔ∏è Hyperparameters ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**\n",
    "\n",
    "#### **1. üèóÔ∏è Architecture Design**\n",
    "```python\n",
    "# ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô layers ‡πÅ‡∏•‡∏∞ neurons\n",
    "architectures = {\n",
    "    \"Simple\": [64],                    # 1 hidden layer, 64 neurons\n",
    "    \"Deep\": [128, 64, 32],            # 3 hidden layers  \n",
    "    \"Wide\": [256, 256],               # 2 wide layers\n",
    "    \"Complex\": [512, 256, 128, 64]    # 4 layers, decreasing size\n",
    "}\n",
    "```\n",
    "\n",
    "#### **2. ‚ö° Learning Rate**\n",
    "```python\n",
    "learning_rates = {\n",
    "    0.001: \"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤ ‡πÅ‡∏ï‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)\",\n",
    "    0.01:  \"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\", \n",
    "    0.1:   \"‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à overshoot\",\n",
    "    1.0:   \"‡πÄ‡∏£‡πá‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà converge\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **3. üìä Batch Size**\n",
    "```python\n",
    "batch_sizes = {\n",
    "    1:    \"Stochastic (‡∏ä‡πâ‡∏≤ ‡πÅ‡∏ï‡πà accurate)\",\n",
    "    32:   \"Small batch (‡∏î‡∏∏‡∏•‡∏¢‡∏†‡∏≤‡∏û‡∏î‡∏µ)\",\n",
    "    128:  \"Medium batch (‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏•‡∏∞ stable)\",\n",
    "    512:  \"Large batch (‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ memory)\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **4. üîÑ Epochs & Early Stopping**\n",
    "```python\n",
    "# ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting\n",
    "def train_with_early_stopping(model, train_data, val_data, patience=10):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(1000):  # Maximum epochs\n",
    "        train_loss = train_one_epoch(model, train_data)\n",
    "        val_loss = validate(model, val_data)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_model(model)  # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return load_best_model()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Neural Networks**\n",
    "\n",
    "#### **1. üß† Universal Approximators**\n",
    "- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **function ‡πÉ‡∏î‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ** (‡∏ï‡∏≤‡∏°‡∏ó‡∏§‡∏©‡∏é‡∏µ)\n",
    "- ‡∏à‡∏±‡∏ö **non-linear patterns** ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ\n",
    "\n",
    "#### **2. üîÑ Flexible Architecture**\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö architecture ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÑ‡∏î‡πâ\n",
    "- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö **multiple inputs/outputs**\n",
    "\n",
    "#### **3. üéØ Feature Learning**\n",
    "- **‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á manual feature engineering**\n",
    "- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **representation** ‡πÄ‡∏≠‡∏á\n",
    "\n",
    "#### **4. üöÄ Scalable**\n",
    "- ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö **big data**\n",
    "- ‡πÉ‡∏ä‡πâ **GPU acceleration** ‡πÑ‡∏î‡πâ\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Neural Networks**\n",
    "\n",
    "#### **1. üï≥Ô∏è Black Box**\n",
    "- **‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢** ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ\n",
    "- ‡∏¢‡∏≤‡∏Å‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£ debug\n",
    "\n",
    "#### **2. üíª Computational Requirements**\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **computing power** ‡∏™‡∏π‡∏á\n",
    "- **Training time** ‡∏ô‡∏≤‡∏ô\n",
    "\n",
    "#### **3. üìä Data Hungry**\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£**‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞** ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "- **Overfitting** ‡∏á‡πà‡∏≤‡∏¢‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢\n",
    "\n",
    "#### **4. üéõÔ∏è Hyperparameter Sensitivity**\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á **tuning** ‡∏£‡∏∞‡∏ß‡∏±‡∏á\n",
    "- **Learning rate, architecture** ‡∏°‡∏µ‡∏ú‡∏•‡∏°‡∏≤‡∏Å\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Neural Networks?**\n",
    "\n",
    "#### **‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Large datasets** (>10,000 samples)\n",
    "- **Complex patterns** ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ non-linearity\n",
    "- **Image, text, audio** processing\n",
    "- **High accuracy requirements**\n",
    "- **Multiple outputs** ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô\n",
    "\n",
    "#### **‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Small datasets** (<1,000 samples)\n",
    "- **Need interpretability** (‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÑ‡∏î‡πâ)\n",
    "- **Limited computing resources**\n",
    "- **Simple linear relationships**\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Neural Networks ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô Potentiostat**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á NN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö calibration\n",
    "def create_potentiostat_nn():\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(8, activation='relu'),\n",
    "        \n",
    "        # Output layer (voltage ‡πÅ‡∏•‡∏∞ current)\n",
    "        layers.Dense(2, activation='linear')  # No activation for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model = create_potentiostat_nn()\n",
    "\n",
    "# Features: [raw_voltage, raw_current, temperature, time, gain, offset]\n",
    "# Targets: [calibrated_voltage, calibrated_current]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "calibrated_values = model.predict(X_test)\n",
    "```\n",
    "\n",
    "#### **üî¨ Physics-Informed Neural Networks**\n",
    "```python\n",
    "# ‡πÄ‡∏û‡∏¥‡πà‡∏° physics constraints ‡πÉ‡∏ô loss function\n",
    "def physics_informed_loss(y_true, y_pred):\n",
    "    # Standard MSE loss\n",
    "    mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    \n",
    "    # Physics constraint: Butler-Volmer equation\n",
    "    voltage = y_pred[:, 0]\n",
    "    current = y_pred[:, 1]\n",
    "    \n",
    "    # i = i0 * [exp(Œ±nF(E-E0)/RT) - exp(-(1-Œ±)nF(E-E0)/RT)]\n",
    "    physics_violation = calculate_butler_volmer_error(voltage, current)\n",
    "    \n",
    "    # Combined loss\n",
    "    return mse_loss + 0.1 * physics_violation\n",
    "\n",
    "# Use in model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=physics_informed_loss,\n",
    "    metrics=['mae']\n",
    ")\n",
    "```\n",
    "\n",
    "**üéì Neural Networks = ‡∏û‡∏•‡∏±‡∏á‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f850d7",
   "metadata": {},
   "source": [
    "## ‚ö° **CHAPTER 5: GRADIENT BOOSTING - ‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡∏¥‡∏°‡πÅ‡∏£‡∏á‡πÅ‡∏ö‡∏ö‡∏•‡∏≤‡∏î**\n",
    "\n",
    "### **üéØ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**\n",
    "\n",
    "**Gradient Boosting** = ‡∏Å‡∏≤‡∏£ **‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î** ‡πÅ‡∏ö‡∏ö step-by-step ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\n",
    "\n",
    "### **ü§ù ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á**\n",
    "\n",
    "#### **üéØ ‡∏Å‡∏≤‡∏£‡∏¢‡∏¥‡∏á‡∏ò‡∏ô‡∏π**\n",
    "```\n",
    "‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1: ‡∏¢‡∏¥‡∏á‡∏û‡∏•‡∏≤‡∏î ‚Üí ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏û‡∏•‡∏≤‡∏î‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡πÑ‡∏´‡∏ô\n",
    "‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2: ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡πá‡∏á ‚Üí ‡∏¢‡∏¥‡∏á‡πÉ‡∏´‡∏°‡πà ‚Üí ‡∏¢‡∏±‡∏á‡∏û‡∏•‡∏≤‡∏î ‡πÅ‡∏ï‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3: ‡∏õ‡∏£‡∏±‡∏ö‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‚Üí ‡∏¢‡∏¥‡∏á‡πÉ‡∏´‡∏°‡πà ‚Üí ‡πÉ‡∏Å‡∏•‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "...\n",
    "‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà N: ‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πâ‡∏≤! üéØ\n",
    "```\n",
    "\n",
    "#### **üìö ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå**\n",
    "```\n",
    "‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 1: ‡πÑ‡∏î‡πâ 50 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‚Üí ‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Üí ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2: ‡πÑ‡∏î‡πâ 65 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‚Üí ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡πÉ‡∏´‡∏°‡πà\n",
    "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Üí ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 3: ‡πÑ‡∏î‡πâ 80 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô ‚Üí ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡πà‡∏≠\n",
    "...\n",
    "‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡πÑ‡∏î‡πâ 95 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô! üéì\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üîß Gradient Boosting ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**\n",
    "\n",
    "#### **üìä Step-by-Step Process**\n",
    "\n",
    "**Step 1: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Simple Model**\n",
    "```python\n",
    "# Model ‡πÅ‡∏£‡∏Å = ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (simple baseline)\n",
    "initial_prediction = mean(y_train)\n",
    "\n",
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô\n",
    "houses = [2000000, 3000000, 4000000, 5000000]\n",
    "initial_prediction = 3500000  # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢\n",
    "\n",
    "# Error ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å\n",
    "errors_1 = [2000000-3500000, 3000000-3500000, 4000000-3500000, 5000000-3500000]\n",
    "errors_1 = [-1500000, -500000, 500000, 1500000]\n",
    "```\n",
    "\n",
    "**Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ Error**\n",
    "```python\n",
    "# Model ‡∏ó‡∏µ‡πà 2 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ errors_1\n",
    "model_2 = DecisionTree()\n",
    "model_2.fit(features, errors_1)\n",
    "\n",
    "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ error corrections\n",
    "corrections_2 = model_2.predict(features)\n",
    "# ‡πÄ‡∏ä‡πà‡∏ô [-1200000, -400000, 400000, 1200000]\n",
    "\n",
    "# ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó predictions\n",
    "predictions_2 = initial_prediction + 0.1 * corrections_2\n",
    "# learning_rate = 0.1 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting\n",
    "```\n",
    "\n",
    "**Step 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Error ‡πÉ‡∏´‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ã‡πâ‡∏≥**\n",
    "```python\n",
    "# Error ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà 2\n",
    "errors_2 = y_true - predictions_2\n",
    "# ‡∏à‡∏∞‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á‡∏Å‡∏ß‡πà‡∏≤ errors_1\n",
    "\n",
    "# Model ‡∏ó‡∏µ‡πà 3 ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏Å‡πâ errors_2\n",
    "model_3 = DecisionTree()\n",
    "model_3.fit(features, errors_2)\n",
    "\n",
    "# Update predictions ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\n",
    "predictions_3 = predictions_2 + 0.1 * model_3.predict(features)\n",
    "\n",
    "# ‡∏ó‡∏≥‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ error ‡∏à‡∏∞‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å‡πÜ\n",
    "```\n",
    "\n",
    "**Final Model**\n",
    "```python\n",
    "def gradient_boosting_predict(features):\n",
    "    prediction = initial_prediction\n",
    "    prediction += 0.1 * model_2.predict(features)\n",
    "    prediction += 0.1 * model_3.predict(features)\n",
    "    prediction += 0.1 * model_4.predict(features)\n",
    "    # ... + ‡∏≠‡∏µ‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÜ models\n",
    "    \n",
    "    return prediction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìà ‡∏ó‡∏≥‡πÑ‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ \"Gradient\"?**\n",
    "\n",
    "#### **üî¨ Mathematical Foundation**\n",
    "\n",
    "**Gradient** = ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á**‡∏•‡∏≤‡∏î‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î**‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î error\n",
    "\n",
    "```python\n",
    "# Loss Function (‡πÄ‡∏ä‡πà‡∏ô Mean Squared Error)\n",
    "def loss_function(y_true, y_pred):\n",
    "    return sum((y_true - y_pred)**2) / len(y_true)\n",
    "\n",
    "# Gradient = ‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á Loss\n",
    "# ‚àÇLoss/‚àÇy_pred = -2 * (y_true - y_pred) = -2 * residuals\n",
    "\n",
    "# ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡πÉ‡∏ô‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á**‡∏ï‡∏£‡∏á‡∏Ç‡πâ‡∏≤‡∏°**‡∏Å‡∏±‡∏ö gradient\n",
    "# ‡πÄ‡∏û‡∏∑‡πà‡∏≠**‡∏•‡∏î** loss ‡∏•‡∏á\n",
    "\n",
    "negative_gradient = y_true - y_pred  # ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ residuals!\n",
    "\n",
    "# Model ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ negative_gradient\n",
    "next_model.fit(features, negative_gradient)\n",
    "```\n",
    "\n",
    "#### **üèîÔ∏è ‡∏Å‡∏≤‡∏£‡∏õ‡∏µ‡∏ô‡πÄ‡∏Ç‡∏≤ (‡πÅ‡∏ï‡πà‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô)**\n",
    "```\n",
    "‡πÄ‡∏£‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡∏¢‡∏≠‡∏î‡πÄ‡∏Ç‡∏≤ (high error) ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡∏´‡∏∏‡∏ö‡πÄ‡∏Ç‡∏≤ (low error)\n",
    "\n",
    "Gradient = ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏Ç‡∏≤ (‡πÄ‡∏û‡∏¥‡πà‡∏° error)\n",
    "-Gradient = ‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏•‡∏á‡πÄ‡∏Ç‡∏≤ (‡∏•‡∏î error) ‚Üê ‡πÄ‡∏£‡∏≤‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á‡∏ô‡∏µ‡πâ!\n",
    "\n",
    "‡πÅ‡∏ï‡πà‡∏•‡∏∞ step = model ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ error\n",
    "Learning rate = ‡∏Ç‡∏ô‡∏≤‡∏î step (‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏£‡πá‡∏ß/‡∏ä‡πâ‡∏≤)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Gradient Boosting**\n",
    "\n",
    "#### **1. üéØ ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**\n",
    "- ‡∏°‡∏±‡∏Å‡∏ä‡∏ô‡∏∞ competitions (Kaggle, ML contests)\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö **tabular data** ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "\n",
    "#### **2. üîß ‡πÅ‡∏Å‡πâ Systematic Errors ‡πÑ‡∏î‡πâ‡∏î‡∏µ**\n",
    "- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö **bias patterns** ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô\n",
    "- **Iterative improvement** ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ\n",
    "\n",
    "#### **3. üìä Feature Importance ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô**\n",
    "- ‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ feature ‡πÑ‡∏´‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å\n",
    "- ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô **feature selection**\n",
    "\n",
    "#### **4. üéöÔ∏è Flexible Loss Functions**\n",
    "- ‡πÉ‡∏ä‡πâ loss function ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÑ‡∏î‡πâ (MAE, Huber, Custom)\n",
    "- ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞\n",
    "\n",
    "---\n",
    "\n",
    "### **‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Gradient Boosting**\n",
    "\n",
    "#### **1. ‚è∞ Training ‡∏ä‡πâ‡∏≤**\n",
    "- **Sequential training** ‚Üí ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parallel ‡πÑ‡∏î‡πâ\n",
    "- ‡πÄ‡∏ó‡∏£‡∏ô model ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡πâ‡∏≠‡∏¢‡∏ï‡∏±‡∏ß ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô\n",
    "\n",
    "#### **2. üéõÔ∏è Hyperparameter Sensitive**\n",
    "- **Learning rate** ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ß‡∏±‡∏á\n",
    "- **Number of estimators** ‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠ overfitting/underfitting\n",
    "- **Tree depth** ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏î‡∏∏‡∏•\n",
    "\n",
    "#### **3. üîÑ Overfitting Risk**\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ß‡∏±‡∏á ‡∏à‡∏∞ **‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• training** ‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ **regularization** ‡πÅ‡∏•‡∏∞ **early stopping**\n",
    "\n",
    "#### **4. üíæ Model Size**\n",
    "- ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡πâ‡∏≠‡∏¢ decision trees\n",
    "- **Memory usage** ‡∏™‡∏π‡∏á\n",
    "\n",
    "---\n",
    "\n",
    "### **üõ†Ô∏è ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Hyperparameters**\n",
    "\n",
    "#### **üéöÔ∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î**\n",
    "\n",
    "**1. Learning Rate (‡πÄ‡∏£‡πá‡∏ß ‚Üî ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)**\n",
    "```python\n",
    "learning_rates = {\n",
    "    0.01:  \"‡∏ä‡πâ‡∏≤ ‡πÅ‡∏ï‡πà stable (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)\",\n",
    "    0.05:  \"‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á\",\n",
    "    0.1:   \"‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à overshoot\", \n",
    "    0.3:   \"‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£\"\n",
    "}\n",
    "\n",
    "# ‡∏Å‡∏é‡∏á‡πà‡∏≤‡∏¢‡πÜ: learning_rate ‡∏ï‡πà‡∏≥ ‚Üí n_estimators ‡∏™‡∏π‡∏á\n",
    "lr_0_01 = {\"learning_rate\": 0.01, \"n_estimators\": 1000}\n",
    "lr_0_1 = {\"learning_rate\": 0.1, \"n_estimators\": 100}\n",
    "```\n",
    "\n",
    "**2. Number of Estimators (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô models)**\n",
    "```python\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 100 ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°\n",
    "n_estimators_options = [100, 200, 500, 1000]\n",
    "\n",
    "# ‡πÉ‡∏ä‡πâ early stopping ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=1000,  # ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "    learning_rate=0.01, # ‡∏ä‡πâ‡∏≤\n",
    "    validation_fraction=0.2,  # ‡πÉ‡∏ä‡πâ 20% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö validation\n",
    "    n_iter_no_change=10       # ‡∏´‡∏¢‡∏∏‡∏î‡∏ñ‡πâ‡∏≤ 10 ‡∏£‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô\n",
    ")\n",
    "```\n",
    "\n",
    "**3. Tree Complexity**\n",
    "```python\n",
    "tree_params = {\n",
    "    \"max_depth\": 3,           # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ (3-8 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)\n",
    "    \"min_samples_split\": 20,  # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å\n",
    "    \"min_samples_leaf\": 10,   # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô leaf\n",
    "    \"subsample\": 0.8          # ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 80% ‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏ö (stochastic)\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Gradient Boosting Algorithms**\n",
    "\n",
    "#### **üåü Popular Implementations**\n",
    "\n",
    "**1. üì¶ Scikit-learn GradientBoostingRegressor**\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ‚úÖ ‡∏á‡πà‡∏≤‡∏¢, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£\n",
    "- ‚ùå ‡∏ä‡πâ‡∏≤\n",
    "\n",
    "**2. ‚ö° XGBoost (eXtreme Gradient Boosting)**\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å, ‡∏°‡∏µ GPU support\n",
    "- ‚úÖ ‡∏ä‡∏ô‡∏∞ competitions ‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "\n",
    "**3. üöÄ LightGBM (Microsoft)**\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, memory efficient\n",
    "- ‚úÖ ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡∏µ‡∏Å‡∏±‡∏ö categorical features\n",
    "\n",
    "**4. üçé CatBoost (Yandex)**\n",
    "```python\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=3,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "- ‚úÖ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á preprocess categorical features\n",
    "- ‚úÖ ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting ‡∏î‡∏µ\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Gradient Boosting?**\n",
    "\n",
    "#### **‚úÖ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Tabular data** ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£**‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**\n",
    "- **Kaggle competitions** ‡πÅ‡∏•‡∏∞ business-critical models\n",
    "- **Complex patterns** ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ fine-tuning\n",
    "- **Feature importance** ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "- **‡∏°‡∏µ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameter tuning\n",
    "\n",
    "#### **‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:**\n",
    "- **Real-time applications** (training ‡∏ä‡πâ‡∏≤)\n",
    "- **Very large datasets** (>1M rows ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ LightGBM/XGBoost)\n",
    "- **Need quick baseline** (‡πÉ‡∏ä‡πâ Random Forest ‡∏Å‡πà‡∏≠‡∏ô)\n",
    "- **Limited computational resources**\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Gradient Boosting ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô Potentiostat**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "features = ['raw_voltage', 'raw_current', 'temperature', 'time_elapsed', 'gain', 'offset']\n",
    "target = 'calibrated_voltage'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ systematic voltage bias\n",
    "voltage_booster = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Training with early stopping\n",
    "voltage_booster.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = voltage_booster.feature_importances_\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance:.3f}\")\n",
    "\n",
    "# Prediction\n",
    "calibrated_voltage = voltage_booster.predict(X_test)\n",
    "\n",
    "# ‡πÅ‡∏¢‡∏Å model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö current\n",
    "current_booster = xgb.XGBRegressor(...)  # Same parameters\n",
    "current_booster.fit(X_train, y_current_train)\n",
    "```\n",
    "\n",
    "#### **üîß Systematic Error Correction**\n",
    "```python\n",
    "# Multi-step boosting ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ systematic errors\n",
    "class SystematicErrorCorrector:\n",
    "    def __init__(self):\n",
    "        self.base_model = xgb.XGBRegressor(n_estimators=100)\n",
    "        self.error_correctors = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Step 1: Train base model\n",
    "        self.base_model.fit(X, y)\n",
    "        predictions = self.base_model.predict(X)\n",
    "        \n",
    "        # Step 2: Iteratively correct systematic errors\n",
    "        residuals = y - predictions\n",
    "        \n",
    "        for i in range(5):  # 5 correction rounds\n",
    "            corrector = xgb.XGBRegressor(\n",
    "                n_estimators=50,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=3\n",
    "            )\n",
    "            \n",
    "            corrector.fit(X, residuals)\n",
    "            corrections = corrector.predict(X)\n",
    "            \n",
    "            # Update residuals\n",
    "            predictions += 0.1 * corrections  # Small learning rate\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            self.error_correctors.append(corrector)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = self.base_model.predict(X)\n",
    "        \n",
    "        for corrector in self.error_correctors:\n",
    "            prediction += 0.1 * corrector.predict(X)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "corrector = SystematicErrorCorrector()\n",
    "corrector.fit(X_train, y_train)\n",
    "accurate_predictions = corrector.predict(X_test)\n",
    "```\n",
    "\n",
    "**üéì Gradient Boosting = ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∞‡∏ß‡∏±‡∏á!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f2ba8",
   "metadata": {},
   "source": [
    "## üèÜ **CHAPTER 6: ALGORITHM COMPARISON & DECISION GUIDE**\n",
    "\n",
    "### **üìä ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á 3 Algorithms**\n",
    "\n",
    "| ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ | üå≥ Random Forest | üß† Neural Network | ‚ö° Gradient Boosting |\n",
    "|----------|------------------|-------------------|---------------------|\n",
    "| **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **‡∏ó‡∏ô‡∏ï‡πà‡∏≠ Overfitting** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **Interpretability** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Memory Usage** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n",
    "| **‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ Decision Tree ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Algorithm**\n",
    "\n",
    "```\n",
    "          ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡πÑ‡∏´‡∏°? (>10K samples)\n",
    "                    /              \\\n",
    "                 ‡πÉ‡∏ä‡πà                ‡πÑ‡∏°‡πà\n",
    "                /                    \\\n",
    "      ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÑ‡∏´‡∏°?         ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ interpretability ‡πÑ‡∏´‡∏°?\n",
    "           /              \\                    /                \\\n",
    "        ‡πÉ‡∏ä‡πà               ‡πÑ‡∏°‡πà                ‡πÉ‡∏ä‡πà                ‡πÑ‡∏°‡πà\n",
    "        /                  \\                /                    \\\n",
    "   üß† Neural Network    ‚ö° Gradient     üå≥ Random Forest      üå≥ Random Forest\n",
    "   ‡∏´‡∏£‡∏∑‡∏≠                   Boosting      (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à model)     (‡πÄ‡∏û‡∏∑‡πà‡∏≠ baseline)\n",
    "   ‚ö° Gradient Boosting   (balance)\n",
    "   (maximum accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üõ†Ô∏è ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå**\n",
    "\n",
    "#### **üöÄ Development & Prototyping**\n",
    "```python\n",
    "# ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Random Forest ‡πÄ‡∏™‡∏°‡∏≠!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Quick baseline\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "baseline_score = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Baseline R¬≤: {baseline_score:.4f}\")\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏ú‡∏•‡∏î‡∏µ‡∏û‡∏≠ ‚Üí ‡πÉ‡∏ä‡πâ RF ‡∏ï‡πà‡∏≠\n",
    "# ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏û‡∏¥‡πà‡∏° ‚Üí ‡∏•‡∏≠‡∏á GB ‡∏´‡∏£‡∏∑‡∏≠ NN\n",
    "```\n",
    "\n",
    "#### **üéØ Production System**\n",
    "```python\n",
    "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time applications\n",
    "if response_time_requirement == \"real_time\":\n",
    "    algorithm = \"Random Forest (optimized)\"\n",
    "    config = {\n",
    "        \"n_estimators\": 50,  # ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô trees\n",
    "        \"max_depth\": 10,     # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å\n",
    "        \"n_jobs\": -1         # ‡πÉ‡∏ä‡πâ parallel processing\n",
    "    }\n",
    "\n",
    "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch processing\n",
    "elif accuracy_requirement == \"maximum\":\n",
    "    algorithm = \"Ensemble of all three\"\n",
    "    config = {\n",
    "        \"rf_weight\": 0.3,\n",
    "        \"nn_weight\": 0.4, \n",
    "        \"gb_weight\": 0.3\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **üî¨ Research & Competition**\n",
    "```python\n",
    "# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö maximum accuracy\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Ensemble approach\n",
    "models = {\n",
    "    'xgb': xgb.XGBRegressor(...),\n",
    "    'lgb': lgb.LGBMRegressor(...),\n",
    "    'nn': MLPRegressor(...)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Weighted ensemble prediction\n",
    "def ensemble_predict(X):\n",
    "    xgb_pred = models['xgb'].predict(X) * 0.4\n",
    "    lgb_pred = models['lgb'].predict(X) * 0.4  \n",
    "    nn_pred = models['nn'].predict(X) * 0.2\n",
    "    \n",
    "    return xgb_pred + lgb_pred + nn_pred\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Best Practices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Algorithm**\n",
    "\n",
    "#### **üå≥ Random Forest Best Practices**\n",
    "```python\n",
    "# ‚úÖ Do's\n",
    "rf_best_practices = {\n",
    "    \"n_estimators\": \"‡πÄ‡∏£‡∏¥‡πà‡∏° 100, ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥\",\n",
    "    \"max_features\": \"‡πÉ‡∏ä‡πâ 'sqrt' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö classification, 'auto' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö regression\",\n",
    "    \"min_samples_split\": \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ñ‡πâ‡∏≤ overfitting (5-20)\",\n",
    "    \"bootstrap\": \"‡πÉ‡∏ä‡πâ True ‡πÄ‡∏™‡∏°‡∏≠\",\n",
    "    \"oob_score\": \"‡πÉ‡∏ä‡πâ True ‡πÄ‡∏û‡∏∑‡πà‡∏≠ validate ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏¢‡∏Å test set\"\n",
    "}\n",
    "\n",
    "# ‚ùå Don'ts  \n",
    "rf_mistakes = [\n",
    "    \"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ feature scaling (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)\",\n",
    "    \"‡∏Å‡∏±‡∏á‡∏ß‡∏•‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á overfitting (RF ‡∏ó‡∏ô‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)\",\n",
    "    \"‡∏ï‡∏±‡πâ‡∏á max_depth ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ None)\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### **üß† Neural Network Best Practices**\n",
    "```python\n",
    "# ‚úÖ Do's\n",
    "nn_best_practices = {\n",
    "    \"data_scaling\": \"‡∏ï‡πâ‡∏≠‡∏á standardize features ‡πÄ‡∏™‡∏°‡∏≠\",\n",
    "    \"hidden_layers\": \"‡πÄ‡∏£‡∏¥‡πà‡∏° [64, 32] ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏£‡∏±‡∏ö\",\n",
    "    \"activation\": \"‡πÉ‡∏ä‡πâ 'relu' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hidden, 'linear' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö output (regression)\",\n",
    "    \"optimizer\": \"‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ 'adam'\",\n",
    "    \"early_stopping\": \"‡πÉ‡∏ä‡πâ‡πÄ‡∏™‡∏°‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting\"\n",
    "}\n",
    "\n",
    "# ‚ùå Don'ts\n",
    "nn_mistakes = [\n",
    "    \"‡πÑ‡∏°‡πà scale data\",\n",
    "    \"‡πÉ‡∏ä‡πâ network ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢\", \n",
    "    \"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ validation set\",\n",
    "    \"learning rate ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### **‚ö° Gradient Boosting Best Practices**\n",
    "```python\n",
    "# ‚úÖ Do's\n",
    "gb_best_practices = {\n",
    "    \"learning_rate\": \"‡πÄ‡∏£‡∏¥‡πà‡∏° 0.1, ‡∏•‡∏î‡∏•‡∏á‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ stability\",\n",
    "    \"n_estimators\": \"‡πÉ‡∏ä‡πâ early stopping ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏¢‡∏ï‡∏±‡∏ß\",\n",
    "    \"max_depth\": \"3-6 ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà\",\n",
    "    \"subsample\": \"‡πÉ‡∏ä‡πâ 0.8 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î overfitting\",\n",
    "    \"validation\": \"‡πÅ‡∏¢‡∏Å validation set ‡πÄ‡∏û‡∏∑‡πà‡∏≠ monitor\"\n",
    "}\n",
    "\n",
    "# ‚ùå Don'ts\n",
    "gb_mistakes = [\n",
    "    \"learning rate ‡∏™‡∏π‡∏á‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\",\n",
    "    \"‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ early stopping\",\n",
    "    \"tree ‡∏•‡∏∂‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (overfitting)\",\n",
    "    \"‡πÑ‡∏°‡πà monitor validation performance\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üîÑ Model Lifecycle Management**\n",
    "\n",
    "#### **üìà ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Model ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á**\n",
    "\n",
    "```python\n",
    "class ModelLifecycleManager:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.performance_history = {}\n",
    "    \n",
    "    def train_multiple_algorithms(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"‡πÄ‡∏ó‡∏£‡∏ô‡∏´‡∏•‡∏≤‡∏¢ algorithms ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô\"\"\"\n",
    "        \n",
    "        algorithms = {\n",
    "            'rf': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'gb': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'nn': MLPRegressor(hidden_layer_sizes=(64, 32), random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in algorithms.items():\n",
    "            # Training\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Validation\n",
    "            val_score = model.score(X_val, y_val)\n",
    "            \n",
    "            # Store\n",
    "            self.models[name] = model\n",
    "            results[name] = val_score\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def select_best_model(self, performance_dict):\n",
    "        \"\"\"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\"\"\"\n",
    "        best_name = max(performance_dict, key=performance_dict.get)\n",
    "        best_score = performance_dict[best_name]\n",
    "        \n",
    "        return best_name, best_score, self.models[best_name]\n",
    "    \n",
    "    def ensemble_predict(self, X, weights=None):\n",
    "        \"\"\"‡∏£‡∏ß‡∏° predictions ‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢ models\"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'rf': 0.33, 'gb': 0.33, 'nn': 0.34}\n",
    "        \n",
    "        ensemble_pred = 0\n",
    "        for name, weight in weights.items():\n",
    "            if name in self.models:\n",
    "                pred = self.models[name].predict(X)\n",
    "                ensemble_pred += weight * pred\n",
    "                \n",
    "        return ensemble_pred\n",
    "\n",
    "# ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "manager = ModelLifecycleManager()\n",
    "results = manager.train_multiple_algorithms(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "for model, score in results.items():\n",
    "    print(f\"{model}: R¬≤ = {score:.4f}\")\n",
    "\n",
    "best_name, best_score, best_model = manager.select_best_model(results)\n",
    "print(f\"\\nBest model: {best_name} (R¬≤ = {best_score:.4f})\")\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = manager.ensemble_predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üéì ‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ML Fundamentals**\n",
    "\n",
    "#### **üß† ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:**\n",
    "\n",
    "1. **ü§î Machine Learning Basics**\n",
    "   - Supervised vs Unsupervised learning  \n",
    "   - Classification vs Regression\n",
    "   - ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ feature engineering\n",
    "\n",
    "2. **üå≥ Random Forest**\n",
    "   - ‡∏´‡∏•‡∏≤‡∏¢‡πÜ decision trees ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\n",
    "   - ‡∏ó‡∏ô‡∏ï‡πà‡∏≠ noise ‡πÅ‡∏•‡∏∞ overfitting\n",
    "   - ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö baseline ‡πÅ‡∏•‡∏∞ interpretability\n",
    "\n",
    "3. **üß† Neural Networks**\n",
    "   - ‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏î‡πâ‡∏ß‡∏¢ layers ‡∏Ç‡∏≠‡∏á neurons\n",
    "   - ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ complex patterns ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "   - ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞‡πÅ‡∏•‡∏∞ tuning ‡∏£‡∏∞‡∏ß‡∏±‡∏á\n",
    "\n",
    "4. **‚ö° Gradient Boosting**\n",
    "   - ‡πÅ‡∏Å‡πâ error ‡πÅ‡∏ö‡∏ö step-by-step\n",
    "   - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tabular data\n",
    "   - ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡∏ô‡∏≤‡∏ô\n",
    "\n",
    "#### **üéØ ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ:**\n",
    "\n",
    "- **üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô**: Random Forest (‡πÄ‡∏£‡πá‡∏ß, ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)\n",
    "- **üéØ ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥**: Gradient Boosting (‡∏ä‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÅ‡∏°‡πà‡∏ô)\n",
    "- **üß† ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô**: Neural Network (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞, patterns ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô)\n",
    "- **üèÜ ‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î**: Ensemble ‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö\n",
    "\n",
    "#### **üí° Golden Rules:**\n",
    "\n",
    "1. **Data Quality > Algorithm Choice** - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏µ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏ß‡πà‡∏≤ algorithm ‡πÄ‡∏à‡πã‡∏á\n",
    "2. **Start Simple** - ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ Random Forest ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠\n",
    "3. **Measure Everything** - ‡πÉ‡∏ä‡πâ proper validation ‡πÅ‡∏•‡∏∞ metrics\n",
    "4. **Domain Knowledge Matters** - ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "5. **Ensemble When Possible** - ‡∏£‡∏ß‡∏° models ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏°‡∏±‡∏Å‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Next Steps ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ï‡πà‡∏≠**\n",
    "\n",
    "1. **üìù Hands-on Practice** - ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
    "2. **üìä Learn Proper Evaluation** - Cross-validation, metrics ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á\n",
    "3. **üîß Advanced Techniques** - Hyperparameter tuning, feature selection\n",
    "4. **üè≠ Deployment** - ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥ model ‡πÑ‡∏õ production\n",
    "5. **üìà Monitoring** - ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏° model performance ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á\n",
    "\n",
    "**üéì ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô ML ‡∏ó‡∏µ‡πà‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏Å‡∏£‡πà‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# üõ†Ô∏è **HANDS-ON PRACTICE: ‡∏•‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á**\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üéì MACHINE LEARNING FUNDAMENTALS - HANDS-ON PRACTICE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class MLFundamentalsDemo:\n",
    "    \"\"\"‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ML algorithms ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def create_sample_data(self, n_samples=1000):\n",
    "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏≤‡∏ò‡∏¥‡∏ï\"\"\"\n",
    "        print(\"üìä ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á...\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Features: temperature, pressure, time, catalyst_amount\n",
    "        temperature = np.random.uniform(200, 400, n_samples)  # ¬∞C\n",
    "        pressure = np.random.uniform(1, 10, n_samples)        # atm\n",
    "        time = np.random.uniform(0.5, 5, n_samples)           # hours\n",
    "        catalyst = np.random.uniform(0.1, 1.0, n_samples)     # g\n",
    "        \n",
    "        # Target: chemical yield (%) - synthetic relationship\n",
    "        # Non-linear relationship with some interactions\n",
    "        yield_base = (\n",
    "            0.3 * temperature + \n",
    "            0.2 * pressure * 10 + \n",
    "            0.1 * time * 20 +\n",
    "            0.4 * catalyst * 100\n",
    "        )\n",
    "        \n",
    "        # Add non-linear effects\n",
    "        temp_effect = 0.001 * (temperature - 300) ** 2\n",
    "        interaction = 0.05 * temperature * pressure\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 5, n_samples)\n",
    "        \n",
    "        chemical_yield = yield_base - temp_effect + interaction + noise\n",
    "        \n",
    "        # Ensure realistic range (0-100%)\n",
    "        chemical_yield = np.clip(chemical_yield, 0, 100)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'temperature': temperature,\n",
    "            'pressure': pressure,\n",
    "            'time': time,\n",
    "            'catalyst_amount': catalyst,\n",
    "            'yield': chemical_yield\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {n_samples} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")\n",
    "        print(f\"  üìä Features: {list(data.columns[:-1])}\")\n",
    "        print(f\"  üéØ Target: {data.columns[-1]} (‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á {data['yield'].min():.1f}-{data['yield'].max():.1f})\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ML\"\"\"\n",
    "        print(\"\\nüîß ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = data.drop('yield', axis=1)\n",
    "        y = data['yield']\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"  üìà Training set: {len(X_train)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")\n",
    "        print(f\"  üìä Test set: {len(X_test)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_random_forest(self, X_train, y_train):\n",
    "        \"\"\"‡πÄ‡∏ó‡∏£‡∏ô Random Forest\"\"\"\n",
    "        print(\"\\nüå≥ Training Random Forest...\")\n",
    "        \n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            min_samples_split=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = dict(zip(X_train.columns, rf.feature_importances_))\n",
    "        \n",
    "        print(\"  ‚úÖ Training complete!\")\n",
    "        print(\"  üìä Feature Importance:\")\n",
    "        for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"      {feature}: {importance:.3f}\")\n",
    "        \n",
    "        self.models['Random Forest'] = rf\n",
    "        return rf\n",
    "    \n",
    "    def train_neural_network(self, X_train, y_train):\n",
    "        \"\"\"‡πÄ‡∏ó‡∏£‡∏ô Neural Network\"\"\"\n",
    "        print(\"\\nüß† Training Neural Network...\")\n",
    "        \n",
    "        # Scale features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö NN\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        nn = MLPRegressor(\n",
    "            hidden_layer_sizes=(64, 32, 16),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.2\n",
    "        )\n",
    "        \n",
    "        nn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        print(f\"  ‚úÖ Training complete!\")\n",
    "        print(f\"  üß† Architecture: Input ‚Üí {nn.hidden_layer_sizes} ‚Üí Output\")\n",
    "        print(f\"  üìà Training iterations: {nn.n_iter_}\")\n",
    "        \n",
    "        self.models['Neural Network'] = nn\n",
    "        return nn\n",
    "    \n",
    "    def train_gradient_boosting(self, X_train, y_train):\n",
    "        \"\"\"‡πÄ‡∏ó‡∏£‡∏ô Gradient Boosting\"\"\"\n",
    "        print(\"\\n‚ö° Training Gradient Boosting...\")\n",
    "        \n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        gb.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = dict(zip(X_train.columns, gb.feature_importances_))\n",
    "        \n",
    "        print(\"  ‚úÖ Training complete!\")\n",
    "        print(\"  üìä Feature Importance:\")\n",
    "        for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"      {feature}: {importance:.3f}\")\n",
    "        \n",
    "        self.models['Gradient Boosting'] = gb\n",
    "        return gb\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• models ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\"\"\"\n",
    "        print(\"\\nüìä Model Evaluation:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            # Prepare test data\n",
    "            if name == 'Neural Network':\n",
    "                X_test_processed = self.scaler.transform(X_test)\n",
    "            else:\n",
    "                X_test_processed = X_test\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            \n",
    "            # Metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'RMSE': rmse,\n",
    "                'R¬≤': r2,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nü§ñ {name}:\")\n",
    "            print(f\"  üìâ RMSE: {rmse:.3f}\")\n",
    "            print(f\"  üìà R¬≤: {r2:.4f}\")\n",
    "            \n",
    "        # Find best model\n",
    "        best_model = max(self.results.keys(), key=lambda x: self.results[x]['R¬≤'])\n",
    "        best_r2 = self.results[best_model]['R¬≤']\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Model: {best_model} (R¬≤ = {best_r2:.4f})\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def create_ensemble_prediction(self, X_test):\n",
    "        \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á ensemble prediction\"\"\"\n",
    "        print(\"\\nüéØ Creating Ensemble Prediction...\")\n",
    "        \n",
    "        predictions = []\n",
    "        weights = {'Random Forest': 0.3, 'Neural Network': 0.4, 'Gradient Boosting': 0.3}\n",
    "        \n",
    "        ensemble_pred = np.zeros(len(X_test))\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            # Prepare data\n",
    "            if name == 'Neural Network':\n",
    "                X_processed = self.scaler.transform(X_test)\n",
    "            else:\n",
    "                X_processed = X_test\n",
    "            \n",
    "            pred = model.predict(X_processed)\n",
    "            ensemble_pred += weights[name] * pred\n",
    "        \n",
    "        print(f\"  ‚úÖ Ensemble weights: {weights}\")\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def analyze_performance(self, y_test):\n",
    "        \"\"\"‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå performance ‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\"\"\"\n",
    "        print(\"\\nüîç Detailed Performance Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Create ensemble\n",
    "        X_test_dummy = pd.DataFrame(np.zeros((len(y_test), 4)), \n",
    "                                  columns=['temperature', 'pressure', 'time', 'catalyst_amount'])\n",
    "        ensemble_pred = self.create_ensemble_prediction(X_test_dummy)\n",
    "        \n",
    "        # Add ensemble to results\n",
    "        ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
    "        ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
    "        \n",
    "        self.results['Ensemble'] = {\n",
    "            'RMSE': ensemble_rmse,\n",
    "            'R¬≤': ensemble_r2,\n",
    "            'predictions': ensemble_pred\n",
    "        }\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"\\nüìä Summary Table:\")\n",
    "        print(f\"{'Model':<20} {'RMSE':<10} {'R¬≤':<10} {'Rank':<6}\")\n",
    "        print(\"-\" * 46)\n",
    "        \n",
    "        # Sort by R¬≤\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['R¬≤'], reverse=True)\n",
    "        \n",
    "        for rank, (name, metrics) in enumerate(sorted_results, 1):\n",
    "            rmse = metrics['RMSE']\n",
    "            r2 = metrics['R¬≤']\n",
    "            print(f\"{name:<20} {rmse:<10.3f} {r2:<10.4f} {rank:<6}\")\n",
    "        \n",
    "        # Insights\n",
    "        print(f\"\\nüí° Key Insights:\")\n",
    "        best_model = sorted_results[0][0]\n",
    "        worst_model = sorted_results[-1][0]\n",
    "        \n",
    "        print(f\"  üèÜ Best performer: {best_model}\")\n",
    "        print(f\"  üìà Performance spread: {sorted_results[0][1]['R¬≤'] - sorted_results[-1][1]['R¬≤']:.4f}\")\n",
    "        \n",
    "        if 'Ensemble' in [item[0] for item in sorted_results[:2]]:\n",
    "            print(f\"  üéØ Ensemble in top 2 - diversity helps!\")\n",
    "        \n",
    "        return sorted_results\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á demo ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô\n",
    "print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Machine Learning Demo\")\n",
    "demo = MLFundamentalsDemo()\n",
    "\n",
    "# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "data = demo.create_sample_data(1000)\n",
    "\n",
    "# 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "X_train, X_test, y_train, y_test = demo.prepare_data(data)\n",
    "\n",
    "# 3. ‡πÄ‡∏ó‡∏£‡∏ô models ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö\n",
    "rf_model = demo.train_random_forest(X_train, y_train)\n",
    "nn_model = demo.train_neural_network(X_train, y_train)\n",
    "gb_model = demo.train_gradient_boosting(X_train, y_train)\n",
    "\n",
    "# 4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
    "results = demo.evaluate_models(X_test, y_test)\n",
    "\n",
    "# 5. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\n",
    "final_ranking = demo.analyze_performance(y_test)\n",
    "\n",
    "print(\"\\nüéì HANDS-ON PRACTICE COMPLETE!\")\n",
    "print(\"üéØ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ML algorithms ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "print(\"üìö ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0a840",
   "metadata": {},
   "source": [
    "## üéâ **CONCLUSION: ‡∏à‡∏ö‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ML Fundamentals**\n",
    "\n",
    "### **üéì ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:**\n",
    "\n",
    "#### **üìö ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (Chapter 1-2)**\n",
    "- ‚úÖ **Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£** ‡πÅ‡∏•‡∏∞‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Traditional Programming ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£\n",
    "- ‚úÖ **‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á ML**: Supervised, Unsupervised, Reinforcement Learning\n",
    "- ‚úÖ **Data & Features**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, Feature Engineering, Preprocessing\n",
    "- ‚úÖ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û** - \"Garbage In, Garbage Out\"\n",
    "\n",
    "#### **üß† ‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏±‡πâ‡∏á 3 ‡πÅ‡∏ö‡∏ö (Chapter 3-5)**\n",
    "\n",
    "**üå≥ Random Forest:**\n",
    "- ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡∏´‡∏•‡∏≤‡∏¢‡πÜ Decision Trees ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô\n",
    "- ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏ó‡∏ô‡∏ï‡πà‡∏≠ noise, ‡πÑ‡∏°‡πà overfitting, interpretable\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: Baseline, Real-time, ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô\n",
    "\n",
    "**üß† Neural Networks:**\n",
    "- ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡πÉ‡∏ô‡∏™‡∏°‡∏≠‡∏á\n",
    "- ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ complex patterns, ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏¢‡∏≠‡∏∞, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏™‡∏π‡∏á\n",
    "\n",
    "**‚ö° Gradient Boosting:**\n",
    "- ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡πÅ‡∏Å‡πâ error ‡πÅ‡∏ö‡∏ö step-by-step\n",
    "- ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tabular data\n",
    "- ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö: Maximum accuracy, Competition\n",
    "\n",
    "#### **üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏ä‡πâ (Chapter 6)**\n",
    "- ‚úÖ **Decision Framework** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏∑‡∏≠‡∏Å algorithm\n",
    "- ‚úÖ **Best Practices** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ algorithm\n",
    "- ‚úÖ **Ensemble Methods** - ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏´‡∏•‡∏≤‡∏¢ models\n",
    "- ‚úÖ **Model Lifecycle Management**\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Key Takeaways ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**\n",
    "\n",
    "#### **1. üìä Data ‡∏Ñ‡∏∑‡∏≠‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏≤**\n",
    "```\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏µ 90% + Algorithm ‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤ 10% = ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡∏µ ‚úÖ\n",
    "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏¢‡πà 10% + Algorithm ‡πÄ‡∏à‡πã‡∏á 90% = ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏¢‡πà ‚ùå\n",
    "```\n",
    "\n",
    "#### **2. üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏á‡πà‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏™‡∏°‡∏≠**\n",
    "```\n",
    "Step 1: Random Forest (baseline)\n",
    "Step 2: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏≠ ‚Üí Gradient Boosting\n",
    "Step 3: ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏≠ ‚Üí Neural Network\n",
    "Step 4: ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‚Üí Ensemble\n",
    "```\n",
    "\n",
    "#### **3. üîç Validation ‡∏Ñ‡∏∑‡∏≠‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï**\n",
    "```\n",
    "‡πÑ‡∏°‡πà‡∏°‡∏µ validation = ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤ model ‡∏î‡∏µ‡∏à‡∏£‡∏¥‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "Train/Validation/Test split ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏™‡∏°‡∏≠\n",
    "Cross-validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢\n",
    "```\n",
    "\n",
    "#### **4. üéØ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Algorithm ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á**\n",
    "```\n",
    "‚ùå \"‡πÉ‡∏ä‡πâ Deep Learning ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏ô‡πÄ‡∏à‡πã‡∏á\"\n",
    "‚úÖ \"‡πÉ‡∏ä‡πâ Random Forest ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏≤\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **üõ£Ô∏è ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ï‡πà‡∏≠‡πÑ‡∏õ:**\n",
    "\n",
    "#### **üìà ‡∏£‡∏∞‡∏î‡∏±‡∏ö Beginner ‚Üí Intermediate**\n",
    "1. **üîß Hyperparameter Tuning**: GridSearch, RandomSearch, Bayesian Optimization\n",
    "2. **üìä Feature Selection**: ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\n",
    "3. **‚öñÔ∏è Model Evaluation**: Metrics ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á, Statistical testing\n",
    "4. **üîÑ Cross-Validation**: K-fold, Stratified, Time-series splits\n",
    "\n",
    "#### **üìà ‡∏£‡∏∞‡∏î‡∏±‡∏ö Intermediate ‚Üí Advanced**\n",
    "1. **üéØ Advanced Algorithms**: XGBoost, LightGBM, CatBoost\n",
    "2. **üß† Deep Learning**: CNN, RNN, Transformers\n",
    "3. **üîç Interpretability**: SHAP, LIME, Feature visualization\n",
    "4. **üöÄ MLOps**: Deployment, Monitoring, Continuous learning\n",
    "\n",
    "#### **üìà ‡∏£‡∏∞‡∏î‡∏±‡∏ö Advanced ‚Üí Expert**\n",
    "1. **üî¨ Research Topics**: AutoML, Meta-learning, Transfer learning\n",
    "2. **üè≠ Production Systems**: A/B testing, Model serving, Scalability\n",
    "3. **üìä Domain-Specific**: NLP, Computer Vision, Time Series\n",
    "4. **üß™ Experimentation**: Causal inference, Experimental design\n",
    "\n",
    "---\n",
    "\n",
    "### **üéØ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô Potentiostat:**\n",
    "\n",
    "#### **üî¨ ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏Å‡πâ‡πÑ‡∏î‡πâ:**\n",
    "- **Sensor Calibration**: ‡πÅ‡∏Å‡πâ systematic errors ‡∏Ç‡∏≠‡∏á STM32\n",
    "- **Temperature Compensation**: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥\n",
    "- **Drift Correction**: ‡πÅ‡∏Å‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤\n",
    "- **Multi-Instrument Consistency**: ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö reference instruments\n",
    "\n",
    "#### **üéñÔ∏è ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ:**\n",
    "- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥**: ‡∏à‡∏≤‡∏Å ¬±50mV ‡πÄ‡∏õ‡πá‡∏ô ¬±3mV (~17x improvement)\n",
    "- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£**: ‡∏•‡∏î drift ‡πÅ‡∏•‡∏∞ temperature effects\n",
    "- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß**: Real-time calibration ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á‡πÑ‡∏õ lab\n",
    "- **‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏ô**: ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ñ‡∏π‡∏Å‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏û‡∏á\n",
    "\n",
    "---\n",
    "\n",
    "### **üéâ ‡∏Ç‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏¥‡∏ô‡∏î‡∏µ!**\n",
    "\n",
    "**üéì ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡∏à‡∏ö Machine Learning Fundamentals ‡πÅ‡∏•‡πâ‡∏ß!**\n",
    "\n",
    "#### **üí™ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:**\n",
    "- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£ ML ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å algorithm ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\n",
    "- ‚úÖ ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ feature engineering\n",
    "- ‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô models ‡∏î‡πâ‡∏ß‡∏¢ Random Forest, Neural Networks, Gradient Boosting\n",
    "- ‚úÖ ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á model performance\n",
    "- ‚úÖ ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô engineering\n",
    "\n",
    "#### **üöÄ Next Mission:**\n",
    "**‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÑ‡∏õ‡∏ï‡πà‡∏≠‡∏Ç‡∏±‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Cross-Instrument Calibration System ‡∏à‡∏£‡∏¥‡∏á!**\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Remember:** *\"The best machine learning algorithm is the one that solves your problem most effectively, not necessarily the most sophisticated one.\"*\n",
    "\n",
    "**üéØ Happy Machine Learning! ü§ñ‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
