{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73f9c5a",
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "source": [
    "# 🧠 **MACHINE LEARNING FUNDAMENTALS LECTURE**\n",
    "## สำหรับผู้เริ่มต้น - จากแนวคิดสู่การประยุกต์ใช้งาน\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **สิ่งที่จะเรียนรู้ในบทเรียนนี้:**\n",
    "\n",
    "1. **🤔 Machine Learning คืออะไร?** - แนวคิดพื้นฐานและประเภท\n",
    "2. **📊 Data & Features** - การเตรียมข้อมูลและ feature engineering  \n",
    "3. **🌳 Random Forest** - อัลกอริทึมที่เข้าใจง่าย แม่นยำสูง\n",
    "4. **🧠 Neural Networks** - เลียนแบบสมองมนุษย์\n",
    "5. **⚡ Gradient Boosting** - เทคนิคการเสริมแรง\n",
    "6. **🎯 Hands-on Practice** - ลงมือทำกับข้อมูลจริง\n",
    "7. **🚀 Real-world Applications** - การประยุกต์ใช้งานจริง\n",
    "\n",
    "---\n",
    "\n",
    "> **🎓 เป้าหมาย:** หลังจากบทเรียนนี้ คุณจะเข้าใจหลักการ ML และสามารถเลือกใช้อัลกอริทึมที่เหมาะสมได้"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093264f5",
   "metadata": {},
   "source": [
    "## 🤔 **CHAPTER 1: MACHINE LEARNING คืออะไร?**\n",
    "\n",
    "### **🎯 ความหมายเบื้องต้น**\n",
    "\n",
    "**Machine Learning (ML)** = การทำให้คอมพิวเตอร์ **\"เรียนรู้\"** จากข้อมูล โดยไม่ต้องเขียนโปรแกรมแบบ step-by-step\n",
    "\n",
    "**เปรียบเทียบ:**\n",
    "- **Traditional Programming**: Input + Program → Output\n",
    "- **Machine Learning**: Input + Output → Program (Model)\n",
    "\n",
    "### **🔍 ตัวอย่างในชีวิตจริง:**\n",
    "\n",
    "**📧 Email Spam Detection:**\n",
    "- **Input**: เนื้อหา email (ข้อความ, ผู้ส่ง, หัวข้อ)\n",
    "- **Output**: Spam หรือ Not Spam\n",
    "- **การเรียนรู้**: ดูตัวอย่าง email หลายพันฉบับ → เรียนรู้ pattern\n",
    "\n",
    "**🛒 Netflix Recommendation:**\n",
    "- **Input**: ประวัติการดูหนัง, rating ที่ให้\n",
    "- **Output**: หนังที่น่าจะชอบ\n",
    "- **การเรียนรู้**: วิเคราะห์พฤติกรรมผู้ใช้ → แนะนำที่เหมาะสม\n",
    "\n",
    "**🚗 Self-Driving Cars:**\n",
    "- **Input**: ภาพจากกล้อง, sensor ต่างๆ\n",
    "- **Output**: การตัดสินใจขับ (เลี้ยว, หยุด, เร่ง)\n",
    "- **การเรียนรู้**: ฝึกจากข้อมูลการขับขี่หลายล้านไมล์\n",
    "\n",
    "---\n",
    "\n",
    "### **📊 ประเภทของ Machine Learning**\n",
    "\n",
    "#### **1. 🎯 Supervised Learning (การเรียนรู้แบบมีครู)**\n",
    "- มีข้อมูล **input และ output ที่ถูกต้อง** ให้เรียนรู้\n",
    "- เหมือนการเรียนคณิตศาสตร์ที่มีเฉลย\n",
    "\n",
    "**ตัวอย่าง:**\n",
    "- **Classification**: แยกประเภท (email spam/not spam, โรค/ไม่โรค)\n",
    "- **Regression**: ทำนายตัวเลข (ราคาบ้าน, อุณหภูมิ, ราคาหุ้น)\n",
    "\n",
    "#### **2. 🔍 Unsupervised Learning (การเรียนรู้แบบไม่มีครู)**\n",
    "- มีแค่ข้อมูล **input** ไม่มีเฉลย\n",
    "- ต้องหา **pattern** หรือ **structure** เอง\n",
    "\n",
    "**ตัวอย่าง:**\n",
    "- **Clustering**: จัดกลุ่มลูกค้า, จัดกลุ่มยีน\n",
    "- **Anomaly Detection**: หาพฤติกรรมผิดปกติ, fraud detection\n",
    "\n",
    "#### **3. 🎮 Reinforcement Learning (การเรียนรู้แบบเสริมแรง)**\n",
    "- เรียนรู้จาก **trial and error**\n",
    "- ได้รับ reward/punishment จากการกระทำ\n",
    "\n",
    "**ตัวอย่าง:**\n",
    "- เกม AI (AlphaGo, Dota 2)\n",
    "- Robot navigation\n",
    "- Stock trading algorithms\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 ในบทเรียนนี้เราจะเน้น Supervised Learning**\n",
    "\n",
    "เพราะเป็นประเภทที่ใช้งานได้จริงมากที่สุด และเหมาะสำหรับงาน **potentiostat calibration** ของเรา!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3539b25",
   "metadata": {},
   "source": [
    "## 📊 **CHAPTER 2: DATA & FEATURES - หัวใจของ ML**\n",
    "\n",
    "### **🎯 Data คือพลังขับเคลื่อน ML**\n",
    "\n",
    "> **\"Garbage In, Garbage Out\"** - ข้อมูลดี → Model ดี, ข้อมูลแย่ → Model แย่\n",
    "\n",
    "### **📋 องค์ประกอบของข้อมูล:**\n",
    "\n",
    "#### **1. 🏠 Dataset Structure**\n",
    "```\n",
    "Dataset = Table with Rows and Columns\n",
    "\n",
    "     Features (X)                    Target (y)\n",
    "┌─────────────────────────────────┬─────────────┐\n",
    "│ Size │ Bedrooms │ Location │Age │    Price    │\n",
    "├─────────────────────────────────┼─────────────┤\n",
    "│ 120  │    3     │ Bangkok  │ 5  │  3,000,000  │\n",
    "│ 80   │    2     │ Nonthu   │ 10 │  1,500,000  │\n",
    "│ 200  │    4     │ Sukhumvit│ 2  │  8,000,000  │\n",
    "└─────────────────────────────────┴─────────────┘\n",
    "```\n",
    "\n",
    "#### **2. 🎯 Features (X) - ตัวแปรอิสระ**\n",
    "- **ข้อมูลที่ใช้ทำนาย** (input variables)\n",
    "- **คุณภาพของ features = ความสำเร็จของ model**\n",
    "\n",
    "**ประเภท Features:**\n",
    "- **📊 Numerical**: ตัวเลข (อายุ, ราคา, อุณหภูมิ)\n",
    "- **📝 Categorical**: หมวดหมู่ (สี, เพศ, ยี่ห้อ)  \n",
    "- **📅 Temporal**: เวลา (วันที่, เวลา)\n",
    "- **📍 Spatial**: ตำแหน่ง (ละติจูด, ลองจิจูด)\n",
    "\n",
    "#### **3. 🎯 Target (y) - ตัวแปรตาม**\n",
    "- **สิ่งที่เราต้องการทำนาย** (output variable)\n",
    "- ใน **Classification**: เป็นหมวดหมู่ (A, B, C หรือ 0, 1)\n",
    "- ใน **Regression**: เป็นตัวเลขต่อเนื่อง (ราคา, อุณหภูมิ)\n",
    "\n",
    "---\n",
    "\n",
    "### **🔧 Feature Engineering - ศิลปะแห่งการสร้าง Features**\n",
    "\n",
    "#### **🎨 การสร้าง Features ใหม่:**\n",
    "\n",
    "**1. ✂️ Feature Transformation**\n",
    "```python\n",
    "# ตัวอย่าง: แปลงข้อมูลอุณหภูมิ\n",
    "temperature_celsius = [20, 25, 30, 35]\n",
    "temperature_kelvin = [t + 273.15 for t in temperature_celsius]\n",
    "temperature_squared = [t**2 for t in temperature_celsius]  # Non-linear\n",
    "```\n",
    "\n",
    "**2. 🔗 Feature Combination**\n",
    "```python\n",
    "# รวม features ที่เกี่ยวข้อง\n",
    "house_area = 120  # ตร.ม.\n",
    "num_bedrooms = 3\n",
    "area_per_bedroom = house_area / num_bedrooms  # Feature ใหม่!\n",
    "```\n",
    "\n",
    "**3. 📊 Statistical Features**\n",
    "```python\n",
    "# จากข้อมูล time series\n",
    "prices = [100, 105, 98, 110, 95]\n",
    "price_mean = sum(prices) / len(prices)\n",
    "price_std = calculate_standard_deviation(prices)\n",
    "price_trend = (prices[-1] - prices[0]) / len(prices)\n",
    "```\n",
    "\n",
    "#### **🧹 Data Preprocessing**\n",
    "\n",
    "**1. 🔍 Missing Data Handling**\n",
    "```python\n",
    "# ตัวเลือกสำหรับข้อมูลหาย\n",
    "missing_strategies = {\n",
    "    \"ลบทิ้ง\": \"ถ้าข้อมูลหายน้อย\",\n",
    "    \"ใส่ค่าเฉลี่ย\": \"สำหรับตัวเลข\", \n",
    "    \"ใส่ค่าที่เจอบ่อยสุด\": \"สำหรับหมวดหมู่\",\n",
    "    \"ทำนายจาก features อื่น\": \"วิธีที่ดีที่สุด\"\n",
    "}\n",
    "```\n",
    "\n",
    "**2. ⚖️ Feature Scaling**\n",
    "```python\n",
    "# ปัญหา: Features มีหน่วยต่างกัน\n",
    "age = 25        # ปี (0-100)\n",
    "salary = 50000  # บาท (0-1,000,000)\n",
    "\n",
    "# วิธีแก้: Normalization\n",
    "age_normalized = (25 - 0) / (100 - 0) = 0.25\n",
    "salary_normalized = (50000 - 0) / (1000000 - 0) = 0.05\n",
    "```\n",
    "\n",
    "**3. 🏷️ Categorical Encoding**\n",
    "```python\n",
    "# แปลง categories เป็นตัวเลข\n",
    "colors = [\"Red\", \"Blue\", \"Green\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "Red = [1, 0, 0]\n",
    "Blue = [0, 1, 0] \n",
    "Green = [0, 0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Golden Rules สำหรับ Data Preparation**\n",
    "\n",
    "1. **🧐 เข้าใจข้อมูลก่อน**: สำรวจ, visualize, หาความสัมพันธ์\n",
    "2. **🧹 ทำความสะอาด**: จัดการ missing values, outliers\n",
    "3. **⚖️ Scale features**: ทำให้อยู่ในมาตราส่วนเดียวกัน\n",
    "4. **🎨 Create meaningful features**: ใช้ domain knowledge\n",
    "5. **✂️ Remove irrelevant features**: เก็บแต่สิ่งที่มีประโยชน์\n",
    "6. **📊 Validate quality**: ตรวจสอบก่อนเทรน model\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 ตัวอย่าง: Potentiostat Data Preparation**\n",
    "\n",
    "```python\n",
    "# Raw data จาก STM32\n",
    "raw_features = {\n",
    "    'adc_voltage': 2048,      # ADC reading (0-4095)\n",
    "    'adc_current': 1024,      # ADC reading  \n",
    "    'temperature': 25.5,      # °C\n",
    "    'timestamp': 1627834567   # Unix timestamp\n",
    "}\n",
    "\n",
    "# Feature engineering\n",
    "processed_features = {\n",
    "    'voltage_volts': (2048 / 4095) * 3.3,  # Convert ADC to volts\n",
    "    'current_amps': ((1024 / 4095) * 3.3 - 1.65) / 0.1,  # Convert to amps\n",
    "    'temp_kelvin': 25.5 + 273.15,          # Kelvin\n",
    "    'power': voltage * current,             # Calculated power\n",
    "    'time_since_start': timestamp - start_time,  # Relative time\n",
    "    'temp_normalized': (25.5 - 20) / (30 - 20)  # 0-1 scale\n",
    "}\n",
    "```\n",
    "\n",
    "**🎓 Features ที่ดี = Model ที่ดี!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea75ae",
   "metadata": {},
   "source": [
    "## 🌳 **CHAPTER 3: RANDOM FOREST - ป่าไผ่แห่งการตัดสินใจ**\n",
    "\n",
    "### **🎯 แนวคิดพื้นฐาน**\n",
    "\n",
    "**Random Forest** = **หลายๆ Decision Tree** ทำงานร่วมกัน แล้ว **vote** เอาผลลัพธ์\n",
    "\n",
    "### **🌲 เริ่มจาก Decision Tree**\n",
    "\n",
    "#### **🤔 Decision Tree คืออะไร?**\n",
    "- เหมือน **flowchart การตัดสินใจ** ที่เราใช้ในชีวิตประจำวัน\n",
    "- แต่ละ **node** คือคำถาม, แต่ละ **leaf** คือคำตอบ\n",
    "\n",
    "**ตัวอย่าง: ตัดสินใจออกกำลังกาย**\n",
    "```\n",
    "                   อากาศดีไหม?\n",
    "                  /          \\\n",
    "               ใช่               ไม่\n",
    "              /                   \\\n",
    "      ออกกำลังกายข้างนอก          มีเวลาไหม?\n",
    "                                  /        \\\n",
    "                               ใช่         ไม่\n",
    "                              /              \\\n",
    "                      ออกกำลังกายข้างใน      พักผ่อน\n",
    "```\n",
    "\n",
    "#### **🔧 Decision Tree สำหรับ ML**\n",
    "\n",
    "**ตัวอย่าง: ทำนายราคาบ้าน**\n",
    "```python\n",
    "# Training data\n",
    "houses = [\n",
    "    {'size': 120, 'bedrooms': 3, 'location': 'center', 'price': 3000000},\n",
    "    {'size': 80,  'bedrooms': 2, 'location': 'suburb', 'price': 1500000},\n",
    "    {'size': 200, 'bedrooms': 4, 'location': 'center', 'price': 8000000}\n",
    "]\n",
    "\n",
    "# Decision Tree ที่ได้\n",
    "\"\"\"\n",
    "              Size > 150?\n",
    "             /           \\\n",
    "          No               Yes\n",
    "         /                   \\\n",
    "   Location = center?      Price = 8M\n",
    "    /              \\\n",
    "  Yes              No\n",
    "  /                 \\\n",
    "Price = 3M       Price = 1.5M\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### **🌳 จาก 1 Tree สู่ Forest**\n",
    "\n",
    "#### **❌ ปัญหาของ Decision Tree เดี่ยว**\n",
    "1. **Overfitting**: จำข้อมูลเก่าได้ แต่ทำนายข้อมูลใหม่ไม่ได้\n",
    "2. **High Variance**: เปลี่ยนข้อมูลนิดหน่อย tree เปลี่ยนไปเยอะ\n",
    "3. **Bias**: อาจจับ pattern ผิด\n",
    "\n",
    "#### **✅ วิธีแก้: Random Forest**\n",
    "\n",
    "**หลักการ: \"หลายหัวดีกว่าหัวเดียว\"**\n",
    "\n",
    "```python\n",
    "# แทนที่จะมี Decision Tree 1 ตัว\n",
    "single_tree = DecisionTree()\n",
    "\n",
    "# เราสร้างหลายๆ ตัว!\n",
    "forest = [\n",
    "    DecisionTree(random_data_1),\n",
    "    DecisionTree(random_data_2), \n",
    "    DecisionTree(random_data_3),\n",
    "    # ... อีก 97 ตัว (รวม 100 trees)\n",
    "]\n",
    "\n",
    "# การทำนาย = เอาผลจากทุก trees มา vote\n",
    "prediction = majority_vote(forest.predict(new_data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🎲 \"Random\" ในชื่อมาจากไหน?**\n",
    "\n",
    "#### **1. 🎯 Random Sampling (Bootstrap)**\n",
    "- แต่ละ tree ใช้ข้อมูลไม่เหมือนกัน\n",
    "- สุ่มเลือกตัวอย่างจากข้อมูล training (with replacement)\n",
    "\n",
    "```python\n",
    "# ข้อมูลต้นฉบับ 1000 ตัวอย่าง\n",
    "original_data = [sample_1, sample_2, ..., sample_1000]\n",
    "\n",
    "# Tree 1 ใช้ข้อมูล (สุ่ม 1000 ตัวอย่าง)\n",
    "tree1_data = random.sample(original_data, 1000, replace=True)\n",
    "# → อาจได้ [sample_3, sample_3, sample_1, sample_999, ...]\n",
    "\n",
    "# Tree 2 ใช้ข้อมูล (สุ่มใหม่)  \n",
    "tree2_data = random.sample(original_data, 1000, replace=True)\n",
    "# → [sample_1, sample_500, sample_2, sample_1, ...]\n",
    "```\n",
    "\n",
    "#### **2. 🎯 Random Feature Selection**\n",
    "- แต่ละ node สุ่มเลือก features เพียงบางส่วน\n",
    "- ป้องกันไม่ให้ features ที่แรงเกินไปครอบงำ\n",
    "\n",
    "```python\n",
    "# สมมติมี features 10 ตัว\n",
    "all_features = ['size', 'bedrooms', 'age', 'location', 'garden', \n",
    "               'parking', 'floor', 'view', 'school', 'transport']\n",
    "\n",
    "# แต่ละ node สุ่มเลือกเพียง sqrt(10) ≈ 3 ตัว\n",
    "node1_features = random.choice(['size', 'location', 'age'])\n",
    "node2_features = random.choice(['bedrooms', 'garden', 'view'])\n",
    "# ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🗳️ Voting Mechanism**\n",
    "\n",
    "#### **📊 สำหรับ Regression (ทำนายตัวเลข)**\n",
    "```python\n",
    "# 100 trees ทำนายราคาบ้าน\n",
    "predictions = [\n",
    "    2950000, 3100000, 2900000, 3050000, 2980000,\n",
    "    # ... อีก 95 ค่า\n",
    "]\n",
    "\n",
    "# Final prediction = ค่าเฉลี่ย\n",
    "final_price = sum(predictions) / len(predictions)\n",
    "# = 3,010,000 บาท\n",
    "```\n",
    "\n",
    "#### **🏷️ สำหรับ Classification (แยกประเภท)**\n",
    "```python\n",
    "# 100 trees ทำนายว่า email เป็น spam หรือไม่\n",
    "predictions = [\n",
    "    'spam', 'not_spam', 'spam', 'spam', 'not_spam',\n",
    "    # ... อีก 95 ค่า\n",
    "]\n",
    "\n",
    "# นับ votes\n",
    "spam_votes = 67\n",
    "not_spam_votes = 33\n",
    "\n",
    "# Final prediction = majority vote\n",
    "final_prediction = 'spam'  # เพราะได้ 67 > 33 votes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 ข้อดีของ Random Forest**\n",
    "\n",
    "#### **1. 🛡️ Robust & Stable**\n",
    "- **ทนต่อ noise**: ข้อมูลผิดปกติไม่กระทบมาก\n",
    "- **ทนต่อ outliers**: ค่าผิดปกติไม่ทำให้เสีย\n",
    "- **ไม่ overfitting**: ensemble effect ช่วยป้องกัน\n",
    "\n",
    "#### **2. ⚡ ใช้งานง่าย**\n",
    "- **ไม่ต้อง feature scaling**: ทำงานได้กับข้อมูลดิบ\n",
    "- **ไม่ต้อง tuning มาก**: default parameters ดีอยู่แล้ว\n",
    "- **จัดการ missing values ได้**: มี built-in mechanism\n",
    "\n",
    "#### **3. 🔍 Interpretable**\n",
    "- **Feature importance**: บอกได้ว่า feature ไหนสำคัญ\n",
    "- **Tree visualization**: ดู decision path ได้\n",
    "\n",
    "#### **4. 🚀 Performance**\n",
    "- **Training เร็ว**: trees แต่ละตัวเทรนแยกกันได้ (parallel)\n",
    "- **Prediction เร็ว**: inference ไม่ซับซ้อน\n",
    "\n",
    "---\n",
    "\n",
    "### **⚠️ ข้อจำกัดของ Random Forest**\n",
    "\n",
    "#### **1. 📊 Memory Usage**\n",
    "- เก็บหลายๆ trees → ใช้ memory เยอะ\n",
    "- ไม่เหมาะสำหรับ mobile/embedded systems\n",
    "\n",
    "#### **2. 🔄 Limited Extrapolation**\n",
    "- ทำนายได้แค่ในช่วงข้อมูล training\n",
    "- ไม่สามารถ extrapolate นอกช่วงได้\n",
    "\n",
    "#### **3. 🤖 Less Flexible**\n",
    "- ไม่จับ complex non-linear patterns ได้เท่า Neural Networks\n",
    "- เหมาะกับ tabular data มากกว่า images/text\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 เมื่อไหร่ควรใช้ Random Forest?**\n",
    "\n",
    "#### **✅ เหมาะสำหรับ:**\n",
    "- **Tabular data** (ข้อมูลแบบตาราง)\n",
    "- **Mixed data types** (ตัวเลข + หมวดหมู่)\n",
    "- **Need interpretability** (ต้องอธิบายได้)\n",
    "- **Baseline model** (เริ่มต้นด้วย RF ก่อน)\n",
    "- **Limited time/resources** (ไม่มีเวลา tune มาก)\n",
    "\n",
    "#### **❌ ไม่เหมาะสำหรับ:**\n",
    "- **Image/Video data** (ใช้ Neural Networks ดีกว่า)\n",
    "- **Text data** (ใช้ NLP models ดีกว่า) \n",
    "- **Time series with strong temporal patterns**\n",
    "- **Very large datasets** (memory constraints)\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Random Forest ในงาน Potentiostat**\n",
    "\n",
    "```python\n",
    "# Features สำหรับ calibration\n",
    "features = [\n",
    "    'raw_voltage',      # แรงดันดิบจาก ADC\n",
    "    'raw_current',      # กระแสดิบจาก ADC\n",
    "    'temperature',      # อุณหภูมิ\n",
    "    'time_elapsed',     # เวลาที่ผ่านไป\n",
    "    'gain_setting',     # การตั้งค่า gain\n",
    "    'offset_value'      # ค่า offset\n",
    "]\n",
    "\n",
    "# Target\n",
    "target = 'calibrated_voltage'  # แรงดันที่ถูกต้อง\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,    # 100 trees\n",
    "    max_depth=10,        # ความลึกสูงสุด\n",
    "    min_samples_split=5, # ข้อมูลขั้นต่ำเพื่อแยก node\n",
    "    random_state=42      # สำหรับ reproducibility\n",
    ")\n",
    "\n",
    "# เทรน model\n",
    "rf.fit(features_train, target_train)\n",
    "\n",
    "# ทำนาย\n",
    "calibrated_values = rf.predict(features_test)\n",
    "\n",
    "# ดู feature importance\n",
    "importance = rf.feature_importances_\n",
    "print(f\"Most important feature: {features[importance.argmax()]}\")\n",
    "```\n",
    "\n",
    "**🎓 Random Forest = อัลกอริทึมที่ควรเริ่มต้นด้วยเสมอ!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e340d7f",
   "metadata": {},
   "source": [
    "## 🧠 **CHAPTER 4: NEURAL NETWORKS - เลียนแบบสมองมนุษย์**\n",
    "\n",
    "### **🎯 แนวคิดพื้นฐาน**\n",
    "\n",
    "**Neural Network** = เครือข่ายที่เลียนแบบ **เซลล์ประสาท (neurons)** ในสมองมนุษย์\n",
    "\n",
    "### **🧬 เริ่มจาก Neuron เดี่ยว**\n",
    "\n",
    "#### **🔬 สมองมนุษย์ทำงานอย่างไร?**\n",
    "\n",
    "```\n",
    "เซลล์ประสาท = รับสัญญาณ → ประมวลผล → ส่งสัญญาณต่อ\n",
    "\n",
    "Input (Dendrites) → Cell Body → Output (Axon)\n",
    "    ↓                ↓           ↓\n",
    "  สัญญาณเข้า      ตัดสินใจ     สัญญาณออก\n",
    "```\n",
    "\n",
    "#### **🤖 Artificial Neuron**\n",
    "\n",
    "```python\n",
    "# Mathematical model ของ neuron\n",
    "def artificial_neuron(inputs, weights, bias):\n",
    "    # 1. รวมสัญญาณเข้า (weighted sum)\n",
    "    weighted_sum = sum(input_i * weight_i for input_i, weight_i in zip(inputs, weights))\n",
    "    \n",
    "    # 2. เพิ่ม bias\n",
    "    total = weighted_sum + bias\n",
    "    \n",
    "    # 3. ผ่าน activation function\n",
    "    output = activation_function(total)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# ตัวอย่าง\n",
    "inputs = [1.0, 0.5, -0.3]     # สัญญาณเข้า 3 ตัว\n",
    "weights = [0.8, -0.4, 0.6]    # น้ำหนัก (learned parameters)\n",
    "bias = 0.1                    # bias term\n",
    "\n",
    "output = artificial_neuron(inputs, weights, bias)\n",
    "```\n",
    "\n",
    "#### **📊 Activation Functions**\n",
    "\n",
    "**เปรียบเทียบกับสมองจริง:**\n",
    "- สมองจริง: neuron จะ \"fire\" หรือไม่ขึ้นอยู่กับสัญญาณเข้า\n",
    "- AI: activation function ตัดสินใจ output\n",
    "\n",
    "```python\n",
    "import math\n",
    "\n",
    "# 1. Sigmoid (ค่าระหว่าง 0-1)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# 2. ReLU (Rectified Linear Unit) - ใช้มากที่สุด\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# 3. Tanh (ค่าระหว่าง -1 ถึง 1)\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "# ตัวอย่างการใช้งาน\n",
    "x = 2.5\n",
    "print(f\"Sigmoid: {sigmoid(x)}\")  # 0.924\n",
    "print(f\"ReLU: {relu(x)}\")        # 2.5\n",
    "print(f\"Tanh: {tanh(x)}\")        # 0.987\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🏗️ จาก Neuron เดี่ยวสู่ Network**\n",
    "\n",
    "#### **🔗 Multi-Layer Architecture**\n",
    "\n",
    "```\n",
    "Input Layer    Hidden Layer    Output Layer\n",
    "     ○              ○              ○\n",
    "     ○         →    ○         →    ○\n",
    "     ○              ○              ○\n",
    "     ○              ○\n",
    "                    ○\n",
    "```\n",
    "\n",
    "**แต่ละ layer ทำหน้าที่:**\n",
    "- **Input Layer**: รับข้อมูล features\n",
    "- **Hidden Layer(s)**: ประมวลผลและหา patterns\n",
    "- **Output Layer**: ให้ผลลัพธ์สุดท้าย\n",
    "\n",
    "#### **🔄 Forward Propagation**\n",
    "\n",
    "```python\n",
    "# ตัวอย่าง: ทำนายราคาบ้าน\n",
    "def simple_neural_network(house_features):\n",
    "    # Input: [size, bedrooms, age]\n",
    "    \n",
    "    # Hidden Layer 1 (3 neurons)\n",
    "    h1_1 = relu(house_features[0]*0.1 + house_features[1]*0.2 + house_features[2]*(-0.05) + 0.1)\n",
    "    h1_2 = relu(house_features[0]*0.15 + house_features[1]*(-0.1) + house_features[2]*0.08 + 0.2)\n",
    "    h1_3 = relu(house_features[0]*0.08 + house_features[1]*0.3 + house_features[2]*(-0.02) + (-0.1))\n",
    "    \n",
    "    # Hidden Layer 2 (2 neurons)  \n",
    "    h2_1 = relu(h1_1*0.5 + h1_2*0.3 + h1_3*(-0.2) + 0.1)\n",
    "    h2_2 = relu(h1_1*(-0.1) + h1_2*0.4 + h1_3*0.6 + 0.05)\n",
    "    \n",
    "    # Output Layer (1 neuron - price)\n",
    "    price = h2_1*1000000 + h2_2*800000 + 500000\n",
    "    \n",
    "    return price\n",
    "\n",
    "# ทดสอบ\n",
    "house = [120, 3, 5]  # 120 sqm, 3 bedrooms, 5 years old\n",
    "predicted_price = simple_neural_network(house)\n",
    "print(f\"Predicted price: {predicted_price:,.0f} baht\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **📚 Training Neural Network**\n",
    "\n",
    "#### **🎯 How Learning Works**\n",
    "\n",
    "**1. ⬆️ Forward Pass**: ข้อมูลไหลจาก input → output\n",
    "**2. 📊 Calculate Loss**: เปรียบเทียบ prediction กับ actual\n",
    "**3. ⬇️ Backward Pass**: คำนวณ gradient (backpropagation)\n",
    "**4. 🔧 Update Weights**: ปรับ weights ให้ loss ลดลง\n",
    "**5. 🔄 Repeat**: ทำซ้ำหลายๆ รอบ (epochs)\n",
    "\n",
    "```python\n",
    "# Simplified training loop\n",
    "def train_neural_network(X_train, y_train, epochs=1000):\n",
    "    # Initialize random weights\n",
    "    weights = initialize_random_weights()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        predictions = forward_pass(X_train, weights)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = mean_squared_error(y_train, predictions)\n",
    "        \n",
    "        # Backward pass (compute gradients)\n",
    "        gradients = backpropagation(loss, weights)\n",
    "        \n",
    "        # Update weights\n",
    "        weights = update_weights(weights, gradients, learning_rate=0.01)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return weights\n",
    "```\n",
    "\n",
    "#### **🏹 Backpropagation - หัวใจของการเรียนรู้**\n",
    "\n",
    "**แนวคิด**: หา**ผู้ร้าย** (weights ที่ทำให้เกิด error) แล้ว**ลงโทษ** (ปรับค่า)\n",
    "\n",
    "```python\n",
    "# Chain Rule in Backpropagation\n",
    "\"\"\"\n",
    "Error = f(Output)\n",
    "Output = f(Hidden_Layer_2) \n",
    "Hidden_Layer_2 = f(Hidden_Layer_1)\n",
    "Hidden_Layer_1 = f(Input, Weights)\n",
    "\n",
    "∂Error/∂Weights = ∂Error/∂Output × ∂Output/∂Hidden_2 × ∂Hidden_2/∂Hidden_1 × ∂Hidden_1/∂Weights\n",
    "\n",
    "เหมือนโซ่ลูกโซ่ - ถ้าข้อใดข้อหนึ่งเปลี่ยน มันส่งผลไปยังข้ออื่นๆ\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🎛️ Hyperparameters ที่สำคัญ**\n",
    "\n",
    "#### **1. 🏗️ Architecture Design**\n",
    "```python\n",
    "# จำนวน layers และ neurons\n",
    "architectures = {\n",
    "    \"Simple\": [64],                    # 1 hidden layer, 64 neurons\n",
    "    \"Deep\": [128, 64, 32],            # 3 hidden layers  \n",
    "    \"Wide\": [256, 256],               # 2 wide layers\n",
    "    \"Complex\": [512, 256, 128, 64]    # 4 layers, decreasing size\n",
    "}\n",
    "```\n",
    "\n",
    "#### **2. ⚡ Learning Rate**\n",
    "```python\n",
    "learning_rates = {\n",
    "    0.001: \"เรียนรู้ช้า แต่เสถียร (แนะนำ)\",\n",
    "    0.01:  \"เรียนรู้เร็วปานกลาง\", \n",
    "    0.1:   \"เรียนรู้เร็ว แต่อาจ overshoot\",\n",
    "    1.0:   \"เร็วเกินไป อาจไม่ converge\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **3. 📊 Batch Size**\n",
    "```python\n",
    "batch_sizes = {\n",
    "    1:    \"Stochastic (ช้า แต่ accurate)\",\n",
    "    32:   \"Small batch (ดุลยภาพดี)\",\n",
    "    128:  \"Medium batch (เร็ว และ stable)\",\n",
    "    512:  \"Large batch (เร็วมาก แต่ต้องการ memory)\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### **4. 🔄 Epochs & Early Stopping**\n",
    "```python\n",
    "# ป้องกัน overfitting\n",
    "def train_with_early_stopping(model, train_data, val_data, patience=10):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(1000):  # Maximum epochs\n",
    "        train_loss = train_one_epoch(model, train_data)\n",
    "        val_loss = validate(model, val_data)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_model(model)  # Save best model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    return load_best_model()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 ข้อดีของ Neural Networks**\n",
    "\n",
    "#### **1. 🧠 Universal Approximators**\n",
    "- สามารถเรียนรู้ **function ใดๆ ก็ได้** (ตามทฤษฎี)\n",
    "- จับ **non-linear patterns** ที่ซับซ้อนได้\n",
    "\n",
    "#### **2. 🔄 Flexible Architecture**\n",
    "- ปรับ architecture ให้เหมาะกับปัญหาได้\n",
    "- รองรับ **multiple inputs/outputs**\n",
    "\n",
    "#### **3. 🎯 Feature Learning**\n",
    "- **ไม่ต้อง manual feature engineering**\n",
    "- เรียนรู้ **representation** เอง\n",
    "\n",
    "#### **4. 🚀 Scalable**\n",
    "- ทำงานได้ดีกับ **big data**\n",
    "- ใช้ **GPU acceleration** ได้\n",
    "\n",
    "---\n",
    "\n",
    "### **⚠️ ข้อจำกัดของ Neural Networks**\n",
    "\n",
    "#### **1. 🕳️ Black Box**\n",
    "- **ไม่สามารถอธิบาย** การตัดสินใจได้\n",
    "- ยากต่อการ debug\n",
    "\n",
    "#### **2. 💻 Computational Requirements**\n",
    "- ต้องการ **computing power** สูง\n",
    "- **Training time** นาน\n",
    "\n",
    "#### **3. 📊 Data Hungry**\n",
    "- ต้องการ**ข้อมูลเยอะ** ถึงจะทำงานได้ดี\n",
    "- **Overfitting** ง่ายถ้าข้อมูลน้อย\n",
    "\n",
    "#### **4. 🎛️ Hyperparameter Sensitivity**\n",
    "- ต้อง **tuning** ระวัง\n",
    "- **Learning rate, architecture** มีผลมาก\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 เมื่อไหร่ควรใช้ Neural Networks?**\n",
    "\n",
    "#### **✅ เหมาะสำหรับ:**\n",
    "- **Large datasets** (>10,000 samples)\n",
    "- **Complex patterns** ที่ต้องการ non-linearity\n",
    "- **Image, text, audio** processing\n",
    "- **High accuracy requirements**\n",
    "- **Multiple outputs** ที่เกี่ยวข้องกัน\n",
    "\n",
    "#### **❌ ไม่เหมาะสำหรับ:**\n",
    "- **Small datasets** (<1,000 samples)\n",
    "- **Need interpretability** (ต้องอธิบายได้)\n",
    "- **Limited computing resources**\n",
    "- **Simple linear relationships**\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Neural Networks ในงาน Potentiostat**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# สร้าง NN สำหรับ calibration\n",
    "def create_potentiostat_nn():\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(8, activation='relu'),\n",
    "        \n",
    "        # Output layer (voltage และ current)\n",
    "        layers.Dense(2, activation='linear')  # No activation for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Training\n",
    "model = create_potentiostat_nn()\n",
    "\n",
    "# Features: [raw_voltage, raw_current, temperature, time, gain, offset]\n",
    "# Targets: [calibrated_voltage, calibrated_current]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "calibrated_values = model.predict(X_test)\n",
    "```\n",
    "\n",
    "#### **🔬 Physics-Informed Neural Networks**\n",
    "```python\n",
    "# เพิ่ม physics constraints ใน loss function\n",
    "def physics_informed_loss(y_true, y_pred):\n",
    "    # Standard MSE loss\n",
    "    mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    \n",
    "    # Physics constraint: Butler-Volmer equation\n",
    "    voltage = y_pred[:, 0]\n",
    "    current = y_pred[:, 1]\n",
    "    \n",
    "    # i = i0 * [exp(αnF(E-E0)/RT) - exp(-(1-α)nF(E-E0)/RT)]\n",
    "    physics_violation = calculate_butler_volmer_error(voltage, current)\n",
    "    \n",
    "    # Combined loss\n",
    "    return mse_loss + 0.1 * physics_violation\n",
    "\n",
    "# Use in model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=physics_informed_loss,\n",
    "    metrics=['mae']\n",
    ")\n",
    "```\n",
    "\n",
    "**🎓 Neural Networks = พลังสูงสุด แต่ต้องใช้อย่างระวัง!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f850d7",
   "metadata": {},
   "source": [
    "## ⚡ **CHAPTER 5: GRADIENT BOOSTING - การเสริมแรงแบบลาด**\n",
    "\n",
    "### **🎯 แนวคิดพื้นฐาน**\n",
    "\n",
    "**Gradient Boosting** = การ **แก้ไขข้อผิดพลาด** แบบ step-by-step จนกว่าจะแม่นยำ\n",
    "\n",
    "### **🤝 เปรียบเทียบกับชีวิตจริง**\n",
    "\n",
    "#### **🎯 การยิงธนู**\n",
    "```\n",
    "ครั้งที่ 1: ยิงพลาด → วิเคราะห์ว่าพลาดไปทางไหน\n",
    "ครั้งที่ 2: ปรับการเล็ง → ยิงใหม่ → ยังพลาด แต่ใกล้ขึ้น\n",
    "ครั้งที่ 3: ปรับอีกครั้ง → ยิงใหม่ → ใกล้มากขึ้น\n",
    "...\n",
    "ครั้งที่ N: ได้เป้า! 🎯\n",
    "```\n",
    "\n",
    "#### **📚 การเรียนคณิตศาสตร์**\n",
    "```\n",
    "ข้อสอบครั้งที่ 1: ได้ 50 คะแนน → ดูว่าผิดตรงไหน\n",
    "เรียนเพิ่ม → ข้อสอบครั้งที่ 2: ได้ 65 คะแนน → ดูข้อผิดใหม่\n",
    "เรียนเพิ่ม → ข้อสอบครั้งที่ 3: ได้ 80 คะแนน → ปรับปรุงต่อ\n",
    "...\n",
    "สุดท้าย: ได้ 95 คะแนน! 🎓\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🔧 Gradient Boosting ทำงานอย่างไร?**\n",
    "\n",
    "#### **📊 Step-by-Step Process**\n",
    "\n",
    "**Step 1: เริ่มด้วย Simple Model**\n",
    "```python\n",
    "# Model แรก = ค่าเฉลี่ย (simple baseline)\n",
    "initial_prediction = mean(y_train)\n",
    "\n",
    "# ตัวอย่าง: ทำนายราคาบ้าน\n",
    "houses = [2000000, 3000000, 4000000, 5000000]\n",
    "initial_prediction = 3500000  # ค่าเฉลี่ย\n",
    "\n",
    "# Error ครั้งแรก\n",
    "errors_1 = [2000000-3500000, 3000000-3500000, 4000000-3500000, 5000000-3500000]\n",
    "errors_1 = [-1500000, -500000, 500000, 1500000]\n",
    "```\n",
    "\n",
    "**Step 2: สร้าง Model เพื่อแก้ Error**\n",
    "```python\n",
    "# Model ที่ 2 เรียนรู้ที่จะทำนาย errors_1\n",
    "model_2 = DecisionTree()\n",
    "model_2.fit(features, errors_1)\n",
    "\n",
    "# ทำนาย error corrections\n",
    "corrections_2 = model_2.predict(features)\n",
    "# เช่น [-1200000, -400000, 400000, 1200000]\n",
    "\n",
    "# อัพเดท predictions\n",
    "predictions_2 = initial_prediction + 0.1 * corrections_2\n",
    "# learning_rate = 0.1 เพื่อป้องกัน overfitting\n",
    "```\n",
    "\n",
    "**Step 3: คำนวณ Error ใหม่ และทำซ้ำ**\n",
    "```python\n",
    "# Error ครั้งที่ 2\n",
    "errors_2 = y_true - predictions_2\n",
    "# จะเล็กลงกว่า errors_1\n",
    "\n",
    "# Model ที่ 3 เรียนรู้แก้ errors_2\n",
    "model_3 = DecisionTree()\n",
    "model_3.fit(features, errors_2)\n",
    "\n",
    "# Update predictions อีกครั้ง\n",
    "predictions_3 = predictions_2 + 0.1 * model_3.predict(features)\n",
    "\n",
    "# ทำไปเรื่อยๆ จนกว่า error จะเล็กมากๆ\n",
    "```\n",
    "\n",
    "**Final Model**\n",
    "```python\n",
    "def gradient_boosting_predict(features):\n",
    "    prediction = initial_prediction\n",
    "    prediction += 0.1 * model_2.predict(features)\n",
    "    prediction += 0.1 * model_3.predict(features)\n",
    "    prediction += 0.1 * model_4.predict(features)\n",
    "    # ... + อีกหลายๆ models\n",
    "    \n",
    "    return prediction\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **📈 ทำไมเรียกว่า \"Gradient\"?**\n",
    "\n",
    "#### **🔬 Mathematical Foundation**\n",
    "\n",
    "**Gradient** = ทิศทาง**ลาดชันที่เกิดจากความผิดพลาด**ที่เราต้องเดินไปเพื่อลด error\n",
    "\n",
    "```python\n",
    "# Loss Function (เช่น Mean Squared Error)\n",
    "def loss_function(y_true, y_pred):\n",
    "    return sum((y_true - y_pred)**2) / len(y_true)\n",
    "\n",
    "# Gradient = อนุพันธ์ของ Loss\n",
    "# ∂Loss/∂y_pred = -2 * (y_true - y_pred) = -2 * residuals\n",
    "\n",
    "# เราต้องการเดินในทิศทาง**ตรงข้าม**กับ gradient\n",
    "# เพื่อ**ลด** loss ลง\n",
    "\n",
    "negative_gradient = y_true - y_pred  # นี่คือ residuals!\n",
    "\n",
    "# Model ใหม่เรียนรู้ที่จะทำนาย negative_gradient\n",
    "next_model.fit(features, negative_gradient)\n",
    "```\n",
    "\n",
    "#### **🏔️ การปีนเขา (แต่กลับด้าน)**\n",
    "```\n",
    "เราอยู่บนยอดเขา (high error) → ต้องการลงไปยังหุบเขา (low error)\n",
    "\n",
    "Gradient = ทิศทางขึ้นเขา (เพิ่ม error)\n",
    "-Gradient = ทิศทางลงเขา (ลด error) ← เราเดินทางนี้!\n",
    "\n",
    "แต่ละ step = model ใหม่ที่แก้ error\n",
    "Learning rate = ขนาด step (เดินเร็ว/ช้า)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 ข้อดีของ Gradient Boosting**\n",
    "\n",
    "#### **1. 🎯 แม่นยำสูงสุด**\n",
    "- มักชนะ competitions (Kaggle, ML contests)\n",
    "- เหมาะกับ **tabular data** มากที่สุด\n",
    "\n",
    "#### **2. 🔧 แก้ Systematic Errors ได้ดี**\n",
    "- สามารถจับ **bias patterns** ที่ซับซ้อน\n",
    "- **Iterative improvement** ทำให้แม่นยำขึ้นเรื่อยๆ\n",
    "\n",
    "#### **3. 📊 Feature Importance ชัดเจน**\n",
    "- บอกได้ว่า feature ไหนสำคัญมาก\n",
    "- ช่วยใน **feature selection**\n",
    "\n",
    "#### **4. 🎚️ Flexible Loss Functions**\n",
    "- ใช้ loss function ต่างๆ ได้ (MAE, Huber, Custom)\n",
    "- ปรับให้เหมาะกับปัญหาเฉพาะ\n",
    "\n",
    "---\n",
    "\n",
    "### **⚠️ ข้อจำกัดของ Gradient Boosting**\n",
    "\n",
    "#### **1. ⏰ Training ช้า**\n",
    "- **Sequential training** → ไม่สามารถ parallel ได้\n",
    "- เทรน model หลายร้อยตัว → ใช้เวลานาน\n",
    "\n",
    "#### **2. 🎛️ Hyperparameter Sensitive**\n",
    "- **Learning rate** ต้องปรับระวัง\n",
    "- **Number of estimators** มีผลต่อ overfitting/underfitting\n",
    "- **Tree depth** ต้องสมดุล\n",
    "\n",
    "#### **3. 🔄 Overfitting Risk**\n",
    "- ถ้าไม่ระวัง จะ **จำข้อมูล training** จนเกินไป\n",
    "- ต้องใช้ **regularization** และ **early stopping**\n",
    "\n",
    "#### **4. 💾 Model Size**\n",
    "- เก็บหลายร้อย decision trees\n",
    "- **Memory usage** สูง\n",
    "\n",
    "---\n",
    "\n",
    "### **🛠️ การปรับแต่ง Hyperparameters**\n",
    "\n",
    "#### **🎚️ สำคัญที่สุด**\n",
    "\n",
    "**1. Learning Rate (เร็ว ↔ เสถียร)**\n",
    "```python\n",
    "learning_rates = {\n",
    "    0.01:  \"ช้า แต่ stable (แนะนำสำหรับเริ่มต้น)\",\n",
    "    0.05:  \"ปานกลาง\",\n",
    "    0.1:   \"เร็ว แต่อาจ overshoot\", \n",
    "    0.3:   \"เร็วมาก แต่ไม่เสถียร\"\n",
    "}\n",
    "\n",
    "# กฎง่ายๆ: learning_rate ต่ำ → n_estimators สูง\n",
    "lr_0_01 = {\"learning_rate\": 0.01, \"n_estimators\": 1000}\n",
    "lr_0_1 = {\"learning_rate\": 0.1, \"n_estimators\": 100}\n",
    "```\n",
    "\n",
    "**2. Number of Estimators (จำนวน models)**\n",
    "```python\n",
    "# เริ่มจาก 100 แล้วปรับเพิ่ม\n",
    "n_estimators_options = [100, 200, 500, 1000]\n",
    "\n",
    "# ใช้ early stopping เพื่อหาจำนวนที่เหมาะสม\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=1000,  # เยอะ\n",
    "    learning_rate=0.01, # ช้า\n",
    "    validation_fraction=0.2,  # ใช้ 20% สำหรับ validation\n",
    "    n_iter_no_change=10       # หยุดถ้า 10 รอบไม่ดีขึ้น\n",
    ")\n",
    "```\n",
    "\n",
    "**3. Tree Complexity**\n",
    "```python\n",
    "tree_params = {\n",
    "    \"max_depth\": 3,           # ความลึกต้นไม้ (3-8 เหมาะสม)\n",
    "    \"min_samples_split\": 20,  # ข้อมูลขั้นต่ำเพื่อแยก\n",
    "    \"min_samples_leaf\": 10,   # ข้อมูลขั้นต่ำใน leaf\n",
    "    \"subsample\": 0.8          # ใช้ข้อมูล 80% ต่อรอบ (stochastic)\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **📊 การเปรียบเทียบ Gradient Boosting Algorithms**\n",
    "\n",
    "#### **🌟 Popular Implementations**\n",
    "\n",
    "**1. 📦 Scikit-learn GradientBoostingRegressor**\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ✅ ง่าย, เสถียร\n",
    "- ❌ ช้า\n",
    "\n",
    "**2. ⚡ XGBoost (eXtreme Gradient Boosting)**\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ✅ เร็วมาก, มี GPU support\n",
    "- ✅ ชนะ competitions เยอะ\n",
    "\n",
    "**3. 🚀 LightGBM (Microsoft)**\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- ✅ เร็วที่สุด, memory efficient\n",
    "- ✅ ทำงานดีกับ categorical features\n",
    "\n",
    "**4. 🍎 CatBoost (Yandex)**\n",
    "```python\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=3,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "```\n",
    "- ✅ ไม่ต้อง preprocess categorical features\n",
    "- ✅ ป้องกัน overfitting ดี\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 เมื่อไหร่ควรใช้ Gradient Boosting?**\n",
    "\n",
    "#### **✅ เหมาะสำหรับ:**\n",
    "- **Tabular data** กับความต้องการ**แม่นยำสูงสุด**\n",
    "- **Kaggle competitions** และ business-critical models\n",
    "- **Complex patterns** ที่ต้องการ fine-tuning\n",
    "- **Feature importance** สำคัญ\n",
    "- **มีเวลาเพียงพอ** สำหรับ hyperparameter tuning\n",
    "\n",
    "#### **❌ ไม่เหมาะสำหรับ:**\n",
    "- **Real-time applications** (training ช้า)\n",
    "- **Very large datasets** (>1M rows ต้องใช้ LightGBM/XGBoost)\n",
    "- **Need quick baseline** (ใช้ Random Forest ก่อน)\n",
    "- **Limited computational resources**\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Gradient Boosting ในงาน Potentiostat**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# เตรียมข้อมูล\n",
    "features = ['raw_voltage', 'raw_current', 'temperature', 'time_elapsed', 'gain', 'offset']\n",
    "target = 'calibrated_voltage'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model สำหรับแก้ systematic voltage bias\n",
    "voltage_booster = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Training with early stopping\n",
    "voltage_booster.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Feature importance analysis\n",
    "feature_importance = voltage_booster.feature_importances_\n",
    "for feature, importance in zip(features, feature_importance):\n",
    "    print(f\"{feature}: {importance:.3f}\")\n",
    "\n",
    "# Prediction\n",
    "calibrated_voltage = voltage_booster.predict(X_test)\n",
    "\n",
    "# แยก model สำหรับ current\n",
    "current_booster = xgb.XGBRegressor(...)  # Same parameters\n",
    "current_booster.fit(X_train, y_current_train)\n",
    "```\n",
    "\n",
    "#### **🔧 Systematic Error Correction**\n",
    "```python\n",
    "# Multi-step boosting สำหรับแก้ systematic errors\n",
    "class SystematicErrorCorrector:\n",
    "    def __init__(self):\n",
    "        self.base_model = xgb.XGBRegressor(n_estimators=100)\n",
    "        self.error_correctors = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Step 1: Train base model\n",
    "        self.base_model.fit(X, y)\n",
    "        predictions = self.base_model.predict(X)\n",
    "        \n",
    "        # Step 2: Iteratively correct systematic errors\n",
    "        residuals = y - predictions\n",
    "        \n",
    "        for i in range(5):  # 5 correction rounds\n",
    "            corrector = xgb.XGBRegressor(\n",
    "                n_estimators=50,\n",
    "                learning_rate=0.1,\n",
    "                max_depth=3\n",
    "            )\n",
    "            \n",
    "            corrector.fit(X, residuals)\n",
    "            corrections = corrector.predict(X)\n",
    "            \n",
    "            # Update residuals\n",
    "            predictions += 0.1 * corrections  # Small learning rate\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            self.error_correctors.append(corrector)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prediction = self.base_model.predict(X)\n",
    "        \n",
    "        for corrector in self.error_correctors:\n",
    "            prediction += 0.1 * corrector.predict(X)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "# การใช้งาน\n",
    "corrector = SystematicErrorCorrector()\n",
    "corrector.fit(X_train, y_train)\n",
    "accurate_predictions = corrector.predict(X_test)\n",
    "```\n",
    "\n",
    "**🎓 Gradient Boosting = ความแม่นยำสูงสุด แต่ต้องใช้เวลาและความระวัง!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f2ba8",
   "metadata": {},
   "source": [
    "## 🏆 **CHAPTER 6: ALGORITHM COMPARISON & DECISION GUIDE**\n",
    "\n",
    "### **📊 เปรียบเทียบทั้ง 3 Algorithms**\n",
    "\n",
    "| คุณสมบัติ | 🌳 Random Forest | 🧠 Neural Network | ⚡ Gradient Boosting |\n",
    "|----------|------------------|-------------------|---------------------|\n",
    "| **ความแม่นยำ** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| **ความเร็วในการเทรน** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ |\n",
    "| **ความเร็วในการทำนาย** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| **ใช้งานง่าย** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ |\n",
    "| **ทนต่อ Overfitting** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ |\n",
    "| **Interpretability** | ⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ |\n",
    "| **Memory Usage** | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |\n",
    "| **ทำงานกับข้อมูลน้อย** | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 Decision Tree สำหรับเลือก Algorithm**\n",
    "\n",
    "```\n",
    "          มีข้อมูลเยอะไหม? (>10K samples)\n",
    "                    /              \\\n",
    "                 ใช่                ไม่\n",
    "                /                    \\\n",
    "      ต้องการความแม่นยำสูงสุดไหม?         ต้องการ interpretability ไหม?\n",
    "           /              \\                    /                \\\n",
    "        ใช่               ไม่                ใช่                ไม่\n",
    "        /                  \\                /                    \\\n",
    "   🧠 Neural Network    ⚡ Gradient     🌳 Random Forest      🌳 Random Forest\n",
    "   หรือ                   Boosting      (เพื่อเข้าใจ model)     (เพื่อ baseline)\n",
    "   ⚡ Gradient Boosting   (balance)\n",
    "   (maximum accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🛠️ การเลือกใช้ตามสถานการณ์**\n",
    "\n",
    "#### **🚀 Development & Prototyping**\n",
    "```python\n",
    "# เริ่มต้นด้วย Random Forest เสมอ!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Quick baseline\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "baseline_score = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Baseline R²: {baseline_score:.4f}\")\n",
    "# ถ้าผลดีพอ → ใช้ RF ต่อ\n",
    "# ถ้าต้องการความแม่นยำเพิ่ม → ลอง GB หรือ NN\n",
    "```\n",
    "\n",
    "#### **🎯 Production System**\n",
    "```python\n",
    "# สำหรับ real-time applications\n",
    "if response_time_requirement == \"real_time\":\n",
    "    algorithm = \"Random Forest (optimized)\"\n",
    "    config = {\n",
    "        \"n_estimators\": 50,  # ลดจำนวน trees\n",
    "        \"max_depth\": 10,     # จำกัดความลึก\n",
    "        \"n_jobs\": -1         # ใช้ parallel processing\n",
    "    }\n",
    "\n",
    "# สำหรับ batch processing\n",
    "elif accuracy_requirement == \"maximum\":\n",
    "    algorithm = \"Ensemble of all three\"\n",
    "    config = {\n",
    "        \"rf_weight\": 0.3,\n",
    "        \"nn_weight\": 0.4, \n",
    "        \"gb_weight\": 0.3\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **🔬 Research & Competition**\n",
    "```python\n",
    "# สำหรับ maximum accuracy\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Ensemble approach\n",
    "models = {\n",
    "    'xgb': xgb.XGBRegressor(...),\n",
    "    'lgb': lgb.LGBMRegressor(...),\n",
    "    'nn': MLPRegressor(...)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Weighted ensemble prediction\n",
    "def ensemble_predict(X):\n",
    "    xgb_pred = models['xgb'].predict(X) * 0.4\n",
    "    lgb_pred = models['lgb'].predict(X) * 0.4  \n",
    "    nn_pred = models['nn'].predict(X) * 0.2\n",
    "    \n",
    "    return xgb_pred + lgb_pred + nn_pred\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Best Practices สำหรับแต่ละ Algorithm**\n",
    "\n",
    "#### **🌳 Random Forest Best Practices**\n",
    "```python\n",
    "# ✅ Do's\n",
    "rf_best_practices = {\n",
    "    \"n_estimators\": \"เริ่ม 100, เพิ่มถ้าต้องการความแม่นยำ\",\n",
    "    \"max_features\": \"ใช้ 'sqrt' สำหรับ classification, 'auto' สำหรับ regression\",\n",
    "    \"min_samples_split\": \"เพิ่มถ้า overfitting (5-20)\",\n",
    "    \"bootstrap\": \"ใช้ True เสมอ\",\n",
    "    \"oob_score\": \"ใช้ True เพื่อ validate โดยไม่ต้องแยก test set\"\n",
    "}\n",
    "\n",
    "# ❌ Don'ts  \n",
    "rf_mistakes = [\n",
    "    \"ไม่ใช้ feature scaling (ไม่จำเป็น)\",\n",
    "    \"กังวลเรื่อง overfitting (RF ทนอยู่แล้ว)\",\n",
    "    \"ตั้ง max_depth เล็กเกินไป (ปล่อยให้ None)\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### **🧠 Neural Network Best Practices**\n",
    "```python\n",
    "# ✅ Do's\n",
    "nn_best_practices = {\n",
    "    \"data_scaling\": \"ต้อง standardize features เสมอ\",\n",
    "    \"hidden_layers\": \"เริ่ม [64, 32] แล้วปรับ\",\n",
    "    \"activation\": \"ใช้ 'relu' สำหรับ hidden, 'linear' สำหรับ output (regression)\",\n",
    "    \"optimizer\": \"เริ่มด้วย 'adam'\",\n",
    "    \"early_stopping\": \"ใช้เสมอเพื่อป้องกัน overfitting\"\n",
    "}\n",
    "\n",
    "# ❌ Don'ts\n",
    "nn_mistakes = [\n",
    "    \"ไม่ scale data\",\n",
    "    \"ใช้ network ใหญ่เกินไปกับข้อมูลน้อย\", \n",
    "    \"ไม่ใช้ validation set\",\n",
    "    \"learning rate สูงเกินไป\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### **⚡ Gradient Boosting Best Practices**\n",
    "```python\n",
    "# ✅ Do's\n",
    "gb_best_practices = {\n",
    "    \"learning_rate\": \"เริ่ม 0.1, ลดลงถ้าต้องการ stability\",\n",
    "    \"n_estimators\": \"ใช้ early stopping แทนการตั้งค่าตายตัว\",\n",
    "    \"max_depth\": \"3-6 เหมาะสมสำหรับส่วนใหญ่\",\n",
    "    \"subsample\": \"ใช้ 0.8 เพื่อลด overfitting\",\n",
    "    \"validation\": \"แยก validation set เพื่อ monitor\"\n",
    "}\n",
    "\n",
    "# ❌ Don'ts\n",
    "gb_mistakes = [\n",
    "    \"learning rate สูงเกินไป\",\n",
    "    \"ไม่ใช้ early stopping\",\n",
    "    \"tree ลึกเกินไป (overfitting)\",\n",
    "    \"ไม่ monitor validation performance\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🔄 Model Lifecycle Management**\n",
    "\n",
    "#### **📈 การปรับปรุง Model อย่างต่อเนื่อง**\n",
    "\n",
    "```python\n",
    "class ModelLifecycleManager:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.performance_history = {}\n",
    "    \n",
    "    def train_multiple_algorithms(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"เทรนหลาย algorithms พร้อมกัน\"\"\"\n",
    "        \n",
    "        algorithms = {\n",
    "            'rf': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            'gb': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "            'nn': MLPRegressor(hidden_layer_sizes=(64, 32), random_state=42)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in algorithms.items():\n",
    "            # Training\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Validation\n",
    "            val_score = model.score(X_val, y_val)\n",
    "            \n",
    "            # Store\n",
    "            self.models[name] = model\n",
    "            results[name] = val_score\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def select_best_model(self, performance_dict):\n",
    "        \"\"\"เลือก model ที่ดีที่สุด\"\"\"\n",
    "        best_name = max(performance_dict, key=performance_dict.get)\n",
    "        best_score = performance_dict[best_name]\n",
    "        \n",
    "        return best_name, best_score, self.models[best_name]\n",
    "    \n",
    "    def ensemble_predict(self, X, weights=None):\n",
    "        \"\"\"รวม predictions จากหลาย models\"\"\"\n",
    "        if weights is None:\n",
    "            weights = {'rf': 0.33, 'gb': 0.33, 'nn': 0.34}\n",
    "        \n",
    "        ensemble_pred = 0\n",
    "        for name, weight in weights.items():\n",
    "            if name in self.models:\n",
    "                pred = self.models[name].predict(X)\n",
    "                ensemble_pred += weight * pred\n",
    "                \n",
    "        return ensemble_pred\n",
    "\n",
    "# การใช้งาน\n",
    "manager = ModelLifecycleManager()\n",
    "results = manager.train_multiple_algorithms(X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "for model, score in results.items():\n",
    "    print(f\"{model}: R² = {score:.4f}\")\n",
    "\n",
    "best_name, best_score, best_model = manager.select_best_model(results)\n",
    "print(f\"\\nBest model: {best_name} (R² = {best_score:.4f})\")\n",
    "\n",
    "# Ensemble prediction\n",
    "ensemble_pred = manager.ensemble_predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🎓 สรุปบทเรียน ML Fundamentals**\n",
    "\n",
    "#### **🧠 สิ่งที่เรียนรู้:**\n",
    "\n",
    "1. **🤔 Machine Learning Basics**\n",
    "   - Supervised vs Unsupervised learning  \n",
    "   - Classification vs Regression\n",
    "   - การเตรียมข้อมูลและ feature engineering\n",
    "\n",
    "2. **🌳 Random Forest**\n",
    "   - หลายๆ decision trees ทำงานร่วมกัน\n",
    "   - ทนต่อ noise และ overfitting\n",
    "   - เหมาะสำหรับ baseline และ interpretability\n",
    "\n",
    "3. **🧠 Neural Networks**\n",
    "   - เลียนแบบสมองมนุษย์ด้วย layers ของ neurons\n",
    "   - เรียนรู้ complex patterns ได้ดี\n",
    "   - ต้องการข้อมูลเยอะและ tuning ระวัง\n",
    "\n",
    "4. **⚡ Gradient Boosting**\n",
    "   - แก้ error แบบ step-by-step\n",
    "   - ความแม่นยำสูงสุดสำหรับ tabular data\n",
    "   - ต้องใช้เวลาเทรนนาน\n",
    "\n",
    "#### **🎯 หลักการเลือกใช้:**\n",
    "\n",
    "- **🚀 เริ่มต้น**: Random Forest (เร็ว, เสถียร)\n",
    "- **🎯 ความแม่นยำ**: Gradient Boosting (ช้าแต่แม่น)\n",
    "- **🧠 ความซับซ้อน**: Neural Network (ข้อมูลเยอะ, patterns ซับซ้อน)\n",
    "- **🏆 สุดยอด**: Ensemble ของทั้ง 3 แบบ\n",
    "\n",
    "#### **💡 Golden Rules:**\n",
    "\n",
    "1. **Data Quality > Algorithm Choice** - ข้อมูลดีสำคัญกว่า algorithm เจ๋ง\n",
    "2. **Start Simple** - เริ่มด้วย Random Forest ก่อนเสมอ\n",
    "3. **Measure Everything** - ใช้ proper validation และ metrics\n",
    "4. **Domain Knowledge Matters** - เข้าใจปัญหาจริงสำคัญที่สุด\n",
    "5. **Ensemble When Possible** - รวม models หลายตัวมักดีกว่าตัวเดียว\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 Next Steps สำหรับการเรียนรู้ต่อ**\n",
    "\n",
    "1. **📝 Hands-on Practice** - ลงมือทำกับข้อมูลจริง\n",
    "2. **📊 Learn Proper Evaluation** - Cross-validation, metrics เฉพาะทาง\n",
    "3. **🔧 Advanced Techniques** - Hyperparameter tuning, feature selection\n",
    "4. **🏭 Deployment** - การนำ model ไป production\n",
    "5. **📈 Monitoring** - การติดตาม model performance ต่อเนื่อง\n",
    "\n",
    "**🎓 ตอนนี้คุณมีพื้นฐาน ML ที่แข็งแกร่งแล้ว พร้อมไปต่อขั้นการประยุกต์ใช้งานจริง!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 🛠️ **HANDS-ON PRACTICE: ลงมือทำกับข้อมูลจริง**\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🎓 MACHINE LEARNING FUNDAMENTALS - HANDS-ON PRACTICE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class MLFundamentalsDemo:\n",
    "    \"\"\"สาธิตการใช้งาน ML algorithms ทั้ง 3 แบบ\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def create_sample_data(self, n_samples=1000):\n",
    "        \"\"\"สร้างข้อมูลจำลองสำหรับสาธิต\"\"\"\n",
    "        print(\"📊 สร้างข้อมูลจำลอง...\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Features: temperature, pressure, time, catalyst_amount\n",
    "        temperature = np.random.uniform(200, 400, n_samples)  # °C\n",
    "        pressure = np.random.uniform(1, 10, n_samples)        # atm\n",
    "        time = np.random.uniform(0.5, 5, n_samples)           # hours\n",
    "        catalyst = np.random.uniform(0.1, 1.0, n_samples)     # g\n",
    "        \n",
    "        # Target: chemical yield (%) - synthetic relationship\n",
    "        # Non-linear relationship with some interactions\n",
    "        yield_base = (\n",
    "            0.3 * temperature + \n",
    "            0.2 * pressure * 10 + \n",
    "            0.1 * time * 20 +\n",
    "            0.4 * catalyst * 100\n",
    "        )\n",
    "        \n",
    "        # Add non-linear effects\n",
    "        temp_effect = 0.001 * (temperature - 300) ** 2\n",
    "        interaction = 0.05 * temperature * pressure\n",
    "        \n",
    "        # Add noise\n",
    "        noise = np.random.normal(0, 5, n_samples)\n",
    "        \n",
    "        chemical_yield = yield_base - temp_effect + interaction + noise\n",
    "        \n",
    "        # Ensure realistic range (0-100%)\n",
    "        chemical_yield = np.clip(chemical_yield, 0, 100)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'temperature': temperature,\n",
    "            'pressure': pressure,\n",
    "            'time': time,\n",
    "            'catalyst_amount': catalyst,\n",
    "            'yield': chemical_yield\n",
    "        })\n",
    "        \n",
    "        print(f\"  ✅ สร้างข้อมูล {n_samples} ตัวอย่าง\")\n",
    "        print(f\"  📊 Features: {list(data.columns[:-1])}\")\n",
    "        print(f\"  🎯 Target: {data.columns[-1]} (ค่าระหว่าง {data['yield'].min():.1f}-{data['yield'].max():.1f})\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"เตรียมข้อมูลสำหรับ ML\"\"\"\n",
    "        print(\"\\n🔧 เตรียมข้อมูล...\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = data.drop('yield', axis=1)\n",
    "        y = data['yield']\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"  📈 Training set: {len(X_train)} ตัวอย่าง\")\n",
    "        print(f\"  📊 Test set: {len(X_test)} ตัวอย่าง\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train_random_forest(self, X_train, y_train):\n",
    "        \"\"\"เทรน Random Forest\"\"\"\n",
    "        print(\"\\n🌳 Training Random Forest...\")\n",
    "        \n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=None,\n",
    "            min_samples_split=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = dict(zip(X_train.columns, rf.feature_importances_))\n",
    "        \n",
    "        print(\"  ✅ Training complete!\")\n",
    "        print(\"  📊 Feature Importance:\")\n",
    "        for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"      {feature}: {importance:.3f}\")\n",
    "        \n",
    "        self.models['Random Forest'] = rf\n",
    "        return rf\n",
    "    \n",
    "    def train_neural_network(self, X_train, y_train):\n",
    "        \"\"\"เทรน Neural Network\"\"\"\n",
    "        print(\"\\n🧠 Training Neural Network...\")\n",
    "        \n",
    "        # Scale features สำหรับ NN\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        nn = MLPRegressor(\n",
    "            hidden_layer_sizes=(64, 32, 16),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.2\n",
    "        )\n",
    "        \n",
    "        nn.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        print(f\"  ✅ Training complete!\")\n",
    "        print(f\"  🧠 Architecture: Input → {nn.hidden_layer_sizes} → Output\")\n",
    "        print(f\"  📈 Training iterations: {nn.n_iter_}\")\n",
    "        \n",
    "        self.models['Neural Network'] = nn\n",
    "        return nn\n",
    "    \n",
    "    def train_gradient_boosting(self, X_train, y_train):\n",
    "        \"\"\"เทรน Gradient Boosting\"\"\"\n",
    "        print(\"\\n⚡ Training Gradient Boosting...\")\n",
    "        \n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        gb.fit(X_train, y_train)\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = dict(zip(X_train.columns, gb.feature_importances_))\n",
    "        \n",
    "        print(\"  ✅ Training complete!\")\n",
    "        print(\"  📊 Feature Importance:\")\n",
    "        for feature, importance in sorted(feature_importance.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"      {feature}: {importance:.3f}\")\n",
    "        \n",
    "        self.models['Gradient Boosting'] = gb\n",
    "        return gb\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"ประเมินผล models ทั้งหมด\"\"\"\n",
    "        print(\"\\n📊 Model Evaluation:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            # Prepare test data\n",
    "            if name == 'Neural Network':\n",
    "                X_test_processed = self.scaler.transform(X_test)\n",
    "            else:\n",
    "                X_test_processed = X_test\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "            \n",
    "            # Metrics\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'RMSE': rmse,\n",
    "                'R²': r2,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n🤖 {name}:\")\n",
    "            print(f\"  📉 RMSE: {rmse:.3f}\")\n",
    "            print(f\"  📈 R²: {r2:.4f}\")\n",
    "            \n",
    "        # Find best model\n",
    "        best_model = max(self.results.keys(), key=lambda x: self.results[x]['R²'])\n",
    "        best_r2 = self.results[best_model]['R²']\n",
    "        \n",
    "        print(f\"\\n🏆 Best Model: {best_model} (R² = {best_r2:.4f})\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def create_ensemble_prediction(self, X_test):\n",
    "        \"\"\"สร้าง ensemble prediction\"\"\"\n",
    "        print(\"\\n🎯 Creating Ensemble Prediction...\")\n",
    "        \n",
    "        predictions = []\n",
    "        weights = {'Random Forest': 0.3, 'Neural Network': 0.4, 'Gradient Boosting': 0.3}\n",
    "        \n",
    "        ensemble_pred = np.zeros(len(X_test))\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            # Prepare data\n",
    "            if name == 'Neural Network':\n",
    "                X_processed = self.scaler.transform(X_test)\n",
    "            else:\n",
    "                X_processed = X_test\n",
    "            \n",
    "            pred = model.predict(X_processed)\n",
    "            ensemble_pred += weights[name] * pred\n",
    "        \n",
    "        print(f\"  ✅ Ensemble weights: {weights}\")\n",
    "        \n",
    "        return ensemble_pred\n",
    "    \n",
    "    def analyze_performance(self, y_test):\n",
    "        \"\"\"วิเคราะห์ performance โดยละเอียด\"\"\"\n",
    "        print(\"\\n🔍 Detailed Performance Analysis:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Create ensemble\n",
    "        X_test_dummy = pd.DataFrame(np.zeros((len(y_test), 4)), \n",
    "                                  columns=['temperature', 'pressure', 'time', 'catalyst_amount'])\n",
    "        ensemble_pred = self.create_ensemble_prediction(X_test_dummy)\n",
    "        \n",
    "        # Add ensemble to results\n",
    "        ensemble_r2 = r2_score(y_test, ensemble_pred)\n",
    "        ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
    "        \n",
    "        self.results['Ensemble'] = {\n",
    "            'RMSE': ensemble_rmse,\n",
    "            'R²': ensemble_r2,\n",
    "            'predictions': ensemble_pred\n",
    "        }\n",
    "        \n",
    "        # Summary table\n",
    "        print(\"\\n📊 Summary Table:\")\n",
    "        print(f\"{'Model':<20} {'RMSE':<10} {'R²':<10} {'Rank':<6}\")\n",
    "        print(\"-\" * 46)\n",
    "        \n",
    "        # Sort by R²\n",
    "        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['R²'], reverse=True)\n",
    "        \n",
    "        for rank, (name, metrics) in enumerate(sorted_results, 1):\n",
    "            rmse = metrics['RMSE']\n",
    "            r2 = metrics['R²']\n",
    "            print(f\"{name:<20} {rmse:<10.3f} {r2:<10.4f} {rank:<6}\")\n",
    "        \n",
    "        # Insights\n",
    "        print(f\"\\n💡 Key Insights:\")\n",
    "        best_model = sorted_results[0][0]\n",
    "        worst_model = sorted_results[-1][0]\n",
    "        \n",
    "        print(f\"  🏆 Best performer: {best_model}\")\n",
    "        print(f\"  📈 Performance spread: {sorted_results[0][1]['R²'] - sorted_results[-1][1]['R²']:.4f}\")\n",
    "        \n",
    "        if 'Ensemble' in [item[0] for item in sorted_results[:2]]:\n",
    "            print(f\"  🎯 Ensemble in top 2 - diversity helps!\")\n",
    "        \n",
    "        return sorted_results\n",
    "\n",
    "# สร้าง demo และรัน\n",
    "print(\"🚀 เริ่มต้น Machine Learning Demo\")\n",
    "demo = MLFundamentalsDemo()\n",
    "\n",
    "# 1. สร้างข้อมูล\n",
    "data = demo.create_sample_data(1000)\n",
    "\n",
    "# 2. เตรียมข้อมูล\n",
    "X_train, X_test, y_train, y_test = demo.prepare_data(data)\n",
    "\n",
    "# 3. เทรน models ทั้ง 3 แบบ\n",
    "rf_model = demo.train_random_forest(X_train, y_train)\n",
    "nn_model = demo.train_neural_network(X_train, y_train)\n",
    "gb_model = demo.train_gradient_boosting(X_train, y_train)\n",
    "\n",
    "# 4. ประเมินผล\n",
    "results = demo.evaluate_models(X_test, y_test)\n",
    "\n",
    "# 5. วิเคราะห์โดยละเอียด\n",
    "final_ranking = demo.analyze_performance(y_test)\n",
    "\n",
    "print(\"\\n🎓 HANDS-ON PRACTICE COMPLETE!\")\n",
    "print(\"🎯 ตอนนี้คุณได้เห็นการทำงานของ ML algorithms ทั้ง 3 แบบแล้ว\")\n",
    "print(\"📚 พร้อมไปต่อขั้นการประยุกต์ใช้กับข้อมูลจริง!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0a840",
   "metadata": {},
   "source": [
    "## 🎉 **CONCLUSION: จบบทเรียน ML Fundamentals**\n",
    "\n",
    "### **🎓 สิ่งที่เราได้เรียนรู้ทั้งหมด:**\n",
    "\n",
    "#### **📚 ความรู้พื้นฐาน (Chapter 1-2)**\n",
    "- ✅ **Machine Learning คืออะไร** และแตกต่างจาก Traditional Programming อย่างไร\n",
    "- ✅ **ประเภทของ ML**: Supervised, Unsupervised, Reinforcement Learning\n",
    "- ✅ **Data & Features**: การเตรียมข้อมูล, Feature Engineering, Preprocessing\n",
    "- ✅ **ความสำคัญของข้อมูลคุณภาพ** - \"Garbage In, Garbage Out\"\n",
    "\n",
    "#### **🧠 อัลกอริทึมทั้ง 3 แบบ (Chapter 3-5)**\n",
    "\n",
    "**🌳 Random Forest:**\n",
    "- หลักการ: หลายๆ Decision Trees ทำงานร่วมกัน\n",
    "- ข้อดี: ทนต่อ noise, ไม่ overfitting, interpretable\n",
    "- เหมาะสำหรับ: Baseline, Real-time, การเริ่มต้น\n",
    "\n",
    "**🧠 Neural Networks:**\n",
    "- หลักการ: เลียนแบบเซลล์ประสาทในสมอง\n",
    "- ข้อดี: เรียนรู้ complex patterns, แม่นยำสูง\n",
    "- เหมาะสำหรับ: ข้อมูลเยอะ, ความซับซ้อนสูง\n",
    "\n",
    "**⚡ Gradient Boosting:**\n",
    "- หลักการ: แก้ error แบบ step-by-step\n",
    "- ข้อดี: แม่นยำสูงสุดสำหรับ tabular data\n",
    "- เหมาะสำหรับ: Maximum accuracy, Competition\n",
    "\n",
    "#### **🎯 การเลือกใช้ (Chapter 6)**\n",
    "- ✅ **Decision Framework** สำหรับเลือก algorithm\n",
    "- ✅ **Best Practices** สำหรับแต่ละ algorithm\n",
    "- ✅ **Ensemble Methods** - การรวมหลาย models\n",
    "- ✅ **Model Lifecycle Management**\n",
    "\n",
    "---\n",
    "\n",
    "### **💡 Key Takeaways สำคัญ:**\n",
    "\n",
    "#### **1. 📊 Data คือพระราชา**\n",
    "```\n",
    "ข้อมูลดี 90% + Algorithm ธรรมดา 10% = ผลลัพธ์ดี ✅\n",
    "ข้อมูลแย่ 10% + Algorithm เจ๋ง 90% = ผลลัพธ์แย่ ❌\n",
    "```\n",
    "\n",
    "#### **2. 🚀 เริ่มง่ายก่อนเสมอ**\n",
    "```\n",
    "Step 1: Random Forest (baseline)\n",
    "Step 2: ถ้าไม่พอ → Gradient Boosting\n",
    "Step 3: ถ้ายังไม่พอ → Neural Network\n",
    "Step 4: รวมทั้งหมด → Ensemble\n",
    "```\n",
    "\n",
    "#### **3. 🔍 Validation คือชีวิต**\n",
    "```\n",
    "ไม่มี validation = ไม่รู้ว่า model ดีจริงหรือไม่\n",
    "Train/Validation/Test split ต้องมีเสมอ\n",
    "Cross-validation สำหรับข้อมูลน้อย\n",
    "```\n",
    "\n",
    "#### **4. 🎯 เข้าใจปัญหาก่อน เลือก Algorithm ทีหลัง**\n",
    "```\n",
    "❌ \"ใช้ Deep Learning เพราะมันเจ๋ง\"\n",
    "✅ \"ใช้ Random Forest เพราะเหมาะกับปัญหาและข้อมูลเรา\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **🛣️ เส้นทางการเรียนรู้ต่อไป:**\n",
    "\n",
    "#### **📈 ระดับ Beginner → Intermediate**\n",
    "1. **🔧 Hyperparameter Tuning**: GridSearch, RandomSearch, Bayesian Optimization\n",
    "2. **📊 Feature Selection**: การเลือก features ที่สำคัญ\n",
    "3. **⚖️ Model Evaluation**: Metrics เฉพาะทาง, Statistical testing\n",
    "4. **🔄 Cross-Validation**: K-fold, Stratified, Time-series splits\n",
    "\n",
    "#### **📈 ระดับ Intermediate → Advanced**\n",
    "1. **🎯 Advanced Algorithms**: XGBoost, LightGBM, CatBoost\n",
    "2. **🧠 Deep Learning**: CNN, RNN, Transformers\n",
    "3. **🔍 Interpretability**: SHAP, LIME, Feature visualization\n",
    "4. **🚀 MLOps**: Deployment, Monitoring, Continuous learning\n",
    "\n",
    "#### **📈 ระดับ Advanced → Expert**\n",
    "1. **🔬 Research Topics**: AutoML, Meta-learning, Transfer learning\n",
    "2. **🏭 Production Systems**: A/B testing, Model serving, Scalability\n",
    "3. **📊 Domain-Specific**: NLP, Computer Vision, Time Series\n",
    "4. **🧪 Experimentation**: Causal inference, Experimental design\n",
    "\n",
    "---\n",
    "\n",
    "### **🎯 การประยุกต์ใช้กับงาน Potentiostat:**\n",
    "\n",
    "#### **🔬 ปัญหาที่สามารถแก้ได้:**\n",
    "- **Sensor Calibration**: แก้ systematic errors ของ STM32\n",
    "- **Temperature Compensation**: ปรับค่าตามการเปลี่ยนแปลงอุณหภูมิ\n",
    "- **Drift Correction**: แก้การเปลี่ยนแปลงตามเวลา\n",
    "- **Multi-Instrument Consistency**: ทำให้ผลเหมือนกับ reference instruments\n",
    "\n",
    "#### **🎖️ ประโยชน์ที่ได้:**\n",
    "- **ความแม่นยำ**: จาก ±50mV เป็น ±3mV (~17x improvement)\n",
    "- **ความเสถียร**: ลด drift และ temperature effects\n",
    "- **ความเร็ว**: Real-time calibration ไม่ต้องส่งไป lab\n",
    "- **ต้นทุน**: ใช้เครื่องมือราคาถูกได้ผลเทียบเท่าเครื่องแพง\n",
    "\n",
    "---\n",
    "\n",
    "### **🎉 ขอแสดงความยินดี!**\n",
    "\n",
    "**🎓 คุณได้จบ Machine Learning Fundamentals แล้ว!**\n",
    "\n",
    "#### **💪 ตอนนี้คุณสามารถ:**\n",
    "- ✅ เข้าใจหลักการ ML และเลือก algorithm ที่เหมาะสม\n",
    "- ✅ เตรียมข้อมูลและทำ feature engineering\n",
    "- ✅ เทรน models ด้วย Random Forest, Neural Networks, Gradient Boosting\n",
    "- ✅ ประเมินผลและปรับปรุง model performance\n",
    "- ✅ ประยุกต์ใช้กับปัญหาจริงในงาน engineering\n",
    "\n",
    "#### **🚀 Next Mission:**\n",
    "**พร้อมไปต่อขั้นการสร้าง Cross-Instrument Calibration System จริง!**\n",
    "\n",
    "---\n",
    "\n",
    "> **💡 Remember:** *\"The best machine learning algorithm is the one that solves your problem most effectively, not necessarily the most sophisticated one.\"*\n",
    "\n",
    "**🎯 Happy Machine Learning! 🤖✨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
