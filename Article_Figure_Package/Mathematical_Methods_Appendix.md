# Mathematical Methods and Statistical Analysis: Detailed Appendix

## üìê Mathematical Framework Overview

This appendix provides comprehensive mathematical details supporting the PLS analysis and statistical validation presented in the main article. All equations, algorithms, and statistical tests are documented for full transparency and reproducibility.

## üßÆ Partial Least Squares (PLS) Theory

### 1. PLS Regression Model

The fundamental PLS relationship establishes a linear mapping between electrochemical response matrix **X** and concentration vector **y**:

**y** = **X****Œ≤** + **Œµ**

**Matrix Dimensions:**
- **X**: n √ó p (samples √ó voltage points)
- **y**: n √ó 1 (samples √ó concentration)
- **Œ≤**: p √ó 1 (regression coefficients)
- **Œµ**: n √ó 1 (residual errors)

### 2. Bilinear Decomposition

PLS simultaneously decomposes both **X** and **y** into latent variable structures:

**X** = **T****P**^T + **E** = Œ£·µ¢‚Çå‚ÇÅ·¥¨ **t**·µ¢**p**·µ¢^T + **E**

**y** = **T****q**^T + **f** = Œ£·µ¢‚Çå‚ÇÅ·¥¨ **t**·µ¢**q**·µ¢^T + **f**

**Where:**
- **T** = score matrix (n √ó A) - latent variable coordinates
- **P** = X-loadings (p √ó A) - variable weights for X
- **q** = y-loadings (A √ó 1) - variable weights for y
- **E**, **f** = residual matrices after A components
- A = number of PLS components (optimized via cross-validation)

### 3. Inner Relationship

The core PLS relationship connects X-scores to y-scores:

**u** = **T****c** + **h**

Where:
- **u** = y-scores (n √ó A)
- **c** = inner weights (A √ó 1)
- **h** = inner residuals

## üî¢ NIPALS Algorithm Implementation

### Nonlinear Iterative Partial Least Squares (NIPALS)

**Initialization:**
```
X‚ÇÄ = X_centered_scaled
Y‚ÇÄ = y_centered
```

**For each component a = 1, 2, ..., A:**

**Step 1: Initialize y-score**
```
u‚Çê = first column of Y‚Çê‚Çã‚ÇÅ (or random initialization)
```

**Step 2: Iterative convergence loop**
```
Repeat until convergence:
  1. w‚Çê = X‚Çê‚Çã‚ÇÅ·µÄ u‚Çê / (u‚Çê·µÄ u‚Çê)         # X-weights
  2. w‚Çê = w‚Çê / ||w‚Çê||                   # Normalize to unit length
  3. t‚Çê = X‚Çê‚Çã‚ÇÅ w‚Çê                       # X-scores
  4. q‚Çê = Y‚Çê‚Çã‚ÇÅ·µÄ t‚Çê / (t‚Çê·µÄ t‚Çê)           # y-weights
  5. u‚Çê = Y‚Çê‚Çã‚ÇÅ q‚Çê / (q‚Çê·µÄ q‚Çê)           # y-scores
  
Convergence criterion: ||w‚Çê,new - w‚Çê,old|| < tolerance
```

**Step 3: Component extraction**
```
p‚Çê = X‚Çê‚Çã‚ÇÅ·µÄ t‚Çê / (t‚Çê·µÄ t‚Çê)               # X-loadings
b‚Çê = u‚Çê·µÄ t‚Çê / (t‚Çê·µÄ t‚Çê)                 # Inner weight
```

**Step 4: Deflation**
```
X‚Çê = X‚Çê‚Çã‚ÇÅ - t‚Çê p‚Çê·µÄ                     # Remove component from X
Y‚Çê = Y‚Çê‚Çã‚ÇÅ - b‚Çê t‚Çê q‚Çê·µÄ                 # Remove component from Y
```

**Convergence Parameters:**
- Tolerance: 1√ó10‚Åª‚Å∂
- Maximum iterations: 500
- Typical convergence: 3-8 iterations per component

## üìä Model Validation Metrics

### 1. Coefficient of Determination (R¬≤)

**Training R¬≤:**
```
R¬≤ = 1 - (SS_res / SS_tot)
SS_res = Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢)¬≤
SS_tot = Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - »≥)¬≤
```

**Cross-Validation R¬≤ (Q¬≤):**
```
Q¬≤ = 1 - (PRESS / SS_tot)
PRESS = Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢,cv)¬≤
```

Where ≈∑·µ¢,cv represents predictions from cross-validation excluding sample i.

### 2. Performance Metrics

**Mean Absolute Error (MAE):**
```
MAE = (1/n) √ó Œ£·µ¢‚Çå‚ÇÅ‚Åø |y·µ¢ - ≈∑·µ¢|
```

**Root Mean Square Error (RMSE):**
```
RMSE = ‚àö[(1/n) √ó Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢)¬≤]
```

**Mean Absolute Percentage Error (MAPE):**
```
MAPE = (100/n) √ó Œ£·µ¢‚Çå‚ÇÅ‚Åø |y·µ¢ - ≈∑·µ¢|/|y·µ¢|
```

**Bias (Systematic Error):**
```
Bias = (1/n) √ó Œ£·µ¢‚Çå‚ÇÅ‚Åø (≈∑·µ¢ - y·µ¢)
```

## üéØ Cross-Validation Strategies

### 1. Leave-One-Out Cross-Validation (LOO-CV)

**Procedure:**
```
For i = 1 to n:
  1. Remove sample i from training set
  2. Build PLS model with remaining n-1 samples
  3. Predict concentration for sample i
  4. Store prediction ≈∑·µ¢,cv
  
Calculate Q¬≤ using all LOO predictions
```

### 2. K-Fold Cross-Validation

**Procedure:**
```
1. Randomly partition data into k equal-sized folds
2. For each fold j = 1 to k:
   - Use fold j as validation set
   - Use remaining k-1 folds as training set
   - Build PLS model and predict fold j
3. Calculate average Q¬≤ across all folds
```

**Implemented with k = 5 and 100 random partitions**

### 3. Monte Carlo Cross-Validation

**Procedure:**
```
For iteration i = 1 to n_iterations:
  1. Randomly split data 80% training / 20% validation
  2. Build PLS model on training set
  3. Predict validation set
  4. Calculate Q¬≤·µ¢
  
Final Q¬≤ = mean(Q¬≤·µ¢) ¬± std(Q¬≤·µ¢)
```

**Implemented with 100 iterations**

## üîç Statistical Tests and Validation

### 1. Normality Tests

**Shapiro-Wilk Test:**
```
H‚ÇÄ: Residuals follow normal distribution
H‚ÇÅ: Residuals do not follow normal distribution
Test statistic: W = (Œ£·µ¢b·µ¢x‚Çç·µ¢‚Çé)¬≤ / Œ£·µ¢(x·µ¢ - xÃÑ)¬≤
```

**Anderson-Darling Test:**
```
A¬≤ = -n - (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø(2i-1)[ln F(X‚Çç·µ¢‚Çé) + ln(1-F(X‚Çç‚Çô‚Çä‚ÇÅ‚Çã·µ¢‚Çé))]
```

### 2. Homoscedasticity Tests

**Breusch-Pagan Test:**
```
H‚ÇÄ: œÉ¬≤·µ¢ = œÉ¬≤ (constant variance)
H‚ÇÅ: œÉ¬≤·µ¢ = œÉ¬≤g(Œ±·µÄz·µ¢) (heteroscedasticity)
Test statistic: LM = nR¬≤ ~ œá¬≤(p)
```

**White Test:**
```
Regress squared residuals on original predictors and cross-products
Test statistic: nR¬≤ ~ œá¬≤(p)
```

### 3. Independence Tests

**Durbin-Watson Test:**
```
DW = Œ£·µ¢‚Çå‚ÇÇ‚Åø(e·µ¢ - e·µ¢‚Çã‚ÇÅ)¬≤ / Œ£·µ¢‚Çå‚ÇÅ‚Åøe·µ¢¬≤
```

Where:
- DW ‚âà 2: No autocorrelation
- DW < 2: Positive autocorrelation
- DW > 2: Negative autocorrelation

## üìà Analytical Method Validation

### 1. Detection Limits

**Limit of Detection (LOD):**
```
LOD = 3.3 √ó (œÉ_blank / S)
```

**Limit of Quantification (LOQ):**
```
LOQ = 10 √ó (œÉ_blank / S)
```

Where:
- œÉ_blank = standard deviation of blank measurements
- S = calibration sensitivity (slope)

### 2. Precision Metrics

**Repeatability (Same Conditions):**
```
RSD_r = (s_r / xÃÑ) √ó 100%
s_r = ‚àö[(Œ£·µ¢‚Çå‚ÇÅ·µêŒ£‚±º‚Çå‚ÇÅ‚Åø(x·µ¢‚±º - xÃÑ·µ¢)¬≤) / (m(n-1))]
```

**Intermediate Precision (Different Days/Operators):**
```
RSD_I = (s_I / xÃÑ) √ó 100%
s_I = ‚àö[s_r¬≤ + s_L¬≤]
```

Where s_L = between-group variance component.

**Reproducibility (Different Laboratories):**
```
RSD_R = (s_R / xÃÑ) √ó 100%
s_R = ‚àö[s_I¬≤ + s_lab¬≤]
```

### 3. Accuracy Assessment

**Bias Calculation:**
```
Bias = (xÃÑ_measured - x_true) / x_true √ó 100%
```

**Recovery:**
```
Recovery% = (C_found / C_added) √ó 100%
```

**Trueness (ISO 5725):**
```
Trueness = |Œº - Œº_true|
```

Where Œº = long-term mean of measurements.

## üé≠ Device Classification Mathematics

### 1. Principal Component Analysis for Classification

**Covariance Matrix:**
```
C = (1/(n-1)) √ó X·µÄcentered √ó Xcentered
```

**Eigendecomposition:**
```
C = VŒõV·µÄ
```

Where:
- V = eigenvector matrix (principal component directions)
- Œõ = diagonal eigenvalue matrix (variance explained)

**PC Scores:**
```
T = Xcentered √ó V
```

### 2. Linear Discriminant Analysis (LDA)

**Between-Class Scatter Matrix:**
```
S_B = Œ£·µ¢‚Çå‚ÇÅ·∂ú n·µ¢(Œº·µ¢ - Œº)(Œº·µ¢ - Œº)·µÄ
```

**Within-Class Scatter Matrix:**
```
S_W = Œ£·µ¢‚Çå‚ÇÅ·∂ú Œ£‚±º‚Çå‚ÇÅ‚Åø‚Å± (x‚±º - Œº·µ¢)(x‚±º - Œº·µ¢)·µÄ
```

**Fisher's Linear Discriminant:**
```
w = S_W‚Åª¬π(Œº‚ÇÅ - Œº‚ÇÇ)
```

**Classification Boundary:**
```
w¬∑x + w‚ÇÄ = 0
w‚ÇÄ = -¬Ωw·µÄ(Œº‚ÇÅ + Œº‚ÇÇ)
```

### 3. Separation Metrics

**Mahalanobis Distance:**
```
D¬≤·µ¢‚±º = (Œº·µ¢ - Œº‚±º)·µÄ Œ£‚Åª¬π (Œº·µ¢ - Œº‚±º)
```

**Separation Power:**
```
SP = ‚àö[d¬≤ / (œÉ‚ÇÅ¬≤ + œÉ‚ÇÇ¬≤)]
```

Where d = distance between group means.

**Classification Accuracy:**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN) √ó 100%
```

## üìè Uncertainty Quantification

### 1. Error Propagation

**General Propagation Formula:**
```
œÉ_f¬≤ = Œ£·µ¢‚Çå‚ÇÅ‚Åø (‚àÇf/‚àÇx·µ¢)¬≤ œÉ_x·µ¢¬≤ + 2Œ£·µ¢Œ£‚±º‚Çå·µ¢‚Çä‚ÇÅ‚Åø (‚àÇf/‚àÇx·µ¢)(‚àÇf/‚àÇx‚±º)œÉ_x·µ¢‚±º
```

**For PLS Prediction:**
```
œÉ_pred¬≤ = œÉ_model¬≤ + œÉ_instrumental¬≤ + œÉ_environmental¬≤
```

### 2. Measurement Uncertainty (GUM Approach)

**Combined Standard Uncertainty:**
```
u_c¬≤(y) = Œ£·µ¢‚Çå‚ÇÅ·¥∫ c·µ¢¬≤u¬≤(x·µ¢) + 2Œ£·µ¢Œ£‚±º‚Çå·µ¢‚Çä‚ÇÅ·¥∫ c·µ¢c‚±ºu(x·µ¢,x‚±º)
```

**Expanded Uncertainty (k=2, 95% confidence):**
```
U = k √ó u_c(y)
```

### 3. Bootstrap Confidence Intervals

**Bootstrap Procedure:**
```
For b = 1 to B bootstrap samples:
  1. Resample training data with replacement
  2. Build PLS model on bootstrap sample
  3. Calculate performance metric Œ∏ÃÇ_b
  
95% CI = [Œ∏ÃÇ_(0.025), Œ∏ÃÇ_(0.975)]
```

## üî¨ Electrochemical Signal Processing

### 1. Baseline Correction

**Polynomial Detrending:**
```
I_corrected(V) = I_raw(V) - Œ£·µ¢‚Çå‚ÇÄ·µñ a·µ¢V·µ¢
```

Where polynomial coefficients a·µ¢ are fitted by least squares:
```
a = (V·µÄV)‚Åª¬πV·µÄI_raw
```

**Asymmetric Least Squares (ALS):**
```
Minimize: Œ£·µ¢w·µ¢(y·µ¢ - z·µ¢)¬≤ + ŒªŒ£·µ¢(Œî¬≤z·µ¢)¬≤
```

Where:
- w·µ¢ = asymmetric weights (higher for baseline points)
- Œª = smoothness parameter
- Œî¬≤z·µ¢ = second-order differences

### 2. Noise Filtering

**Savitzky-Golay Filter:**
```
≈∑·µ¢ = Œ£‚±º‚Çå‚Çã‚Çò·µê c‚±ºy·µ¢‚Çä‚±º
```

Where c‚±º are convolution coefficients from least-squares polynomial fitting.

**Moving Average Filter:**
```
≈∑·µ¢ = (1/(2m+1)) √ó Œ£‚±º‚Çå‚Çã‚Çò·µê y·µ¢‚Çä‚±º
```

### 3. Feature Selection

**Variance Threshold:**
```
Keep features where: Var(X‚±º) > threshold
```

**Correlation Filter:**
```
Remove features where: |corr(X·µ¢,X‚±º)| > threshold, i ‚â† j
```

**Variable Importance in Projection (VIP):**
```
VIP‚±º = ‚àö[p √ó Œ£‚Çï‚Çå‚ÇÅ·¥¥ SS(b‚Çïq‚Çï) √ó w‚±º‚Çï¬≤ / Œ£‚Çï‚Çå‚ÇÅ·¥¥ SS(b‚Çïq‚Çï)]
```

Where:
- SS(b‚Çïq‚Çï) = sum of squares explained by component h
- w‚±º‚Çï = weight of variable j in component h

## üìã Quality Control Parameters

### 1. Control Chart Limits

**Shewhart Control Charts:**
```
UCL = Œº + 3œÉ/‚àön
LCL = Œº - 3œÉ/‚àön
```

**CUSUM Control Charts:**
```
C‚Å∫·µ¢ = max[0, C‚Å∫·µ¢‚Çã‚ÇÅ + (x·µ¢ - Œº‚ÇÄ - K)]
C‚Åª·µ¢ = max[0, C‚Åª·µ¢‚Çã‚ÇÅ - (x·µ¢ - Œº‚ÇÄ - K)]
```

Where K = allowable slack parameter.

### 2. Capability Indices

**Process Capability:**
```
Cp = (USL - LSL)/(6œÉ)
Cpk = min[(USL - Œº)/(3œÉ), (Œº - LSL)/(3œÉ)]
```

**Method Capability:**
```
Cm = (specification range)/(analytical range)
```

## üéØ Optimization Algorithms

### 1. Component Number Selection

**Cross-Validation PRESS Minimization:**
```
PRESS(A) = Œ£·µ¢‚Çå‚ÇÅ‚Åø (y·µ¢ - ≈∑·µ¢,cv(A))¬≤
A_optimal = argmin(PRESS(A))
```

**Akaike Information Criterion (AIC):**
```
AIC = n √ó ln(RSS/n) + 2k
```

Where k = number of parameters (components).

**Bayesian Information Criterion (BIC):**
```
BIC = n √ó ln(RSS/n) + k √ó ln(n)
```

### 2. Hyperparameter Optimization

**Grid Search:**
```
For each parameter combination (Œ±‚ÇÅ, Œ±‚ÇÇ, ..., Œ±‚Çñ):
  1. Perform cross-validation
  2. Calculate performance metric
  3. Store best parameters
```

**Random Search:**
```
For i = 1 to n_iterations:
  1. Sample random parameter values
  2. Evaluate model performance
  3. Update best parameters if improved
```

## üé® Visualization Mathematics

### 1. Principal Component Biplots

**Sample Scores:**
```
T = XV‚Çê
```

**Variable Loadings:**
```
P = X·µÄT(T·µÄT)‚Åª¬π
```

**Biplot Approximation:**
```
X ‚âà TP·µÄ
```

### 2. Confidence Ellipses

**Hotelling's T¬≤ Ellipse:**
```
(x - Œº)·µÄS‚Åª¬π(x - Œº) = (p(n-1)/(n-p)) √ó F_p,n-p,Œ±
```

**Normal Probability Ellipse:**
```
(x - Œº)·µÄŒ£‚Åª¬π(x - Œº) = œá¬≤_p,Œ±
```

## üìö References and Standards

### Analytical Chemistry Standards
- **ICH Q2(R1)**: Validation of Analytical Procedures
- **AOAC Guidelines**: Statistical Manual
- **ISO 5725**: Accuracy and Precision of Measurement Methods
- **IUPAC Recommendations**: Analytical Method Validation

### Statistical Methods
- **Wold, S. et al.**: PLS Regression: A Basic Tool
- **Martens, H. & Naes, T.**: Multivariate Calibration
- **Hastie, T. et al.**: Elements of Statistical Learning
- **ISO/IEC Guide 98**: Uncertainty of Measurement (GUM)

### Software Implementation
- **scikit-learn**: Python machine learning library
- **NumPy/SciPy**: Numerical computing foundation
- **Pandas**: Data manipulation and analysis
- **Matplotlib/Seaborn**: Statistical visualization

---

**Note**: All mathematical procedures were implemented with numerical stability considerations, including regularization for matrix inversions and convergence monitoring for iterative algorithms. Code availability and detailed implementation notes are provided in the accompanying software repository.
