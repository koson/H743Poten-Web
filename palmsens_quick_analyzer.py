#!/usr/bin/env python3
"""
Palmsens Quick PLS Analysis
==========================
Quick analysis of Palmsens data by scan rate with sample selection
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
import re
from datetime import datetime

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

class QuickPalmsensAnalyzer:
    """Quick PLS analysis for Palmsens CV data"""
    
    def __init__(self, max_files_per_condition: int = 3):
        """Initialize analyzer with limited file processing"""
        self.max_files_per_condition = max_files_per_condition
        self.processed_data = []
        
    def extract_metadata(self, filename: str) -> Dict:
        """Extract concentration and scan rate from filename"""
        filename_lower = filename.lower()
        
        # Concentration from filename path or name
        concentration = None
        
        # Try directory name first (e.g., "Palmsens_0.5mM")
        if 'palmsens_' in filename_lower:
            conc_match = re.search(r'palmsens_(\d+\.?\d*)mm', filename_lower)
            if conc_match:
                try:
                    concentration = float(conc_match.group(1))
                except ValueError:
                    pass
        
        # Scan rate extraction
        scan_rate = None
        sr_patterns = [
            r'(\d+\.?\d*)\s*mv[\/\-_]?s',
            r'cv[_\-](\d+\.?\d*)mvps'
        ]
        
        for pattern in sr_patterns:
            match = re.search(pattern, filename_lower)
            if match:
                try:
                    scan_rate = float(match.group(1))
                    break
                except ValueError:
                    continue
        
        return {
            'concentration': concentration,
            'scan_rate': scan_rate,
            'filename': filename
        }
    
    def process_file(self, file_path: Path) -> Optional[Dict]:
        """Process a single Palmsens CSV file"""
        try:
            # Check for metadata line
            with open(file_path, 'r') as f:
                first_line = f.readline().strip()
            
            skiprows = 1 if first_line.startswith('FileName:') else 0
            df = pd.read_csv(file_path, encoding='utf-8-sig', skiprows=skiprows)
            
            # Assume standard format: V, uA
            if 'V' not in df.columns or 'uA' not in df.columns:
                return None
            
            voltage = df['V'].values
            current = df['uA'].values
            
            # Remove NaN values
            mask = ~(np.isnan(voltage) | np.isnan(current))
            voltage = voltage[mask]
            current = current[mask]
            
            if len(voltage) < 10:
                return None
            
            # Simple baseline correction (linear fit to first/last 20%)
            n_points = len(voltage)
            baseline_indices = np.concatenate([
                np.arange(0, int(0.2 * n_points)),
                np.arange(int(0.8 * n_points), n_points)
            ])
            
            if len(baseline_indices) >= 2:
                baseline_v = voltage[baseline_indices]
                baseline_i = current[baseline_indices]
                slope, intercept, _, _, _ = linregress(baseline_v, baseline_i)
                baseline_fit = slope * voltage + intercept
                corrected_current = current - baseline_fit
            else:
                corrected_current = current
            
            # Calculate features
            peak_height = np.max(corrected_current) - np.min(corrected_current)
            peak_area = abs(np.trapz(corrected_current, voltage))
            
            # Extract metadata
            metadata = self.extract_metadata(str(file_path))
            
            if metadata['concentration'] is None or metadata['scan_rate'] is None:
                return None
            
            # Additional features for PLS
            features = {
                'peak_height': peak_height,
                'peak_area': peak_area,
                'voltage_range': np.max(voltage) - np.min(voltage),
                'current_std': np.std(corrected_current),
                'peak_position': voltage[np.argmax(corrected_current)]
            }
            
            result = {
                **metadata,
                **features,
                'filepath': str(file_path)
            }
            
            return result
            
        except Exception as e:
            logger.debug(f"Error processing {file_path.name}: {e}")
            return None
    
    def sample_files_intelligently(self, data_dir: Path) -> List[Path]:
        """Sample files to avoid overwhelming processing"""
        logger.info("üìÇ Sampling Palmsens files for analysis...")
        
        all_files = list(data_dir.rglob("*.csv"))
        logger.info(f"   Found {len(all_files)} total files")
        
        # Group files by concentration and scan rate
        file_groups = {}
        
        for file_path in all_files:
            metadata = self.extract_metadata(str(file_path))
            if metadata['concentration'] is not None and metadata['scan_rate'] is not None:
                key = (metadata['concentration'], metadata['scan_rate'])
                if key not in file_groups:
                    file_groups[key] = []
                file_groups[key].append(file_path)
        
        logger.info(f"   Found {len(file_groups)} unique conditions")
        
        # Sample from each group
        sampled_files = []
        for (conc, sr), files in file_groups.items():
            # Sort files for consistent sampling
            files.sort()
            sampled = files[:self.max_files_per_condition]
            sampled_files.extend(sampled)
            
            logger.debug(f"   {conc}mM @ {sr}mV/s: {len(sampled)}/{len(files)} files")
        
        logger.info(f"üìä Selected {len(sampled_files)} files for analysis")
        return sampled_files
    
    def run_analysis(self):
        """Run quick PLS analysis"""
        logger.info("üöÄ Starting Quick Palmsens PLS Analysis")
        logger.info("=" * 50)
        
        data_dir = Path("Test_data/Palmsens")
        
        # Sample files intelligently
        sampled_files = self.sample_files_intelligently(data_dir)
        
        # Process sampled files
        logger.info("üîÑ Processing sampled files...")
        successful = 0
        failed = 0
        
        for file_path in sampled_files:
            result = self.process_file(file_path)
            if result:
                self.processed_data.append(result)
                successful += 1
            else:
                failed += 1
        
        logger.info(f"‚úÖ Processed {successful} files successfully")
        logger.info(f"‚ùå Failed to process {failed} files")
        
        if len(self.processed_data) < 10:
            logger.warning("‚ö†Ô∏è Insufficient data for reliable analysis")
            return
        
        # Analyze by scan rate
        self.analyze_by_scan_rate()
        
        # Generate summary
        self.generate_summary()
    
    def analyze_by_scan_rate(self):
        """Quick PLS analysis by scan rate"""
        logger.info("\nüìä PLS Analysis by Scan Rate")
        
        # Group by scan rate
        scan_rate_groups = {}
        for data in self.processed_data:
            sr = data['scan_rate']
            if sr not in scan_rate_groups:
                scan_rate_groups[sr] = []
            scan_rate_groups[sr].append(data)
        
        scan_rates = sorted(scan_rate_groups.keys())
        logger.info(f"üìä Found scan rates: {scan_rates} mV/s")
        
        # Create plots
        n_rates = len(scan_rates)
        n_cols = min(3, n_rates)
        n_rows = (n_rates + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
        if n_rates == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = [axes]
        else:
            axes = axes.flatten()
        
        results_summary = {}
        
        for i, scan_rate in enumerate(scan_rates):
            data_group = scan_rate_groups[scan_rate]
            
            if len(data_group) < 5:
                logger.warning(f"‚ö†Ô∏è Insufficient data for {scan_rate} mV/s")
                continue
            
            # Prepare data
            concentrations = np.array([d['concentration'] for d in data_group])
            peak_heights = np.array([d['peak_height'] for d in data_group])
            peak_areas = np.array([d['peak_area'] for d in data_group])
            
            # Check concentration variety
            unique_concentrations = len(np.unique(concentrations))
            if unique_concentrations < 3:
                logger.warning(f"‚ö†Ô∏è Insufficient concentration variety for {scan_rate} mV/s")
                continue
            
            # Simple linear regression
            slope_h, intercept_h, r_value_h, _, _ = linregress(concentrations, peak_heights)
            slope_a, intercept_a, r_value_a, _, _ = linregress(concentrations, peak_areas)
            
            r2_height = r_value_h ** 2
            r2_area = r_value_a ** 2
            
            # PLS regression with multiple features
            try:
                features_matrix = np.column_stack([
                    [d['peak_height'] for d in data_group],
                    [d['peak_area'] for d in data_group],
                    [d['voltage_range'] for d in data_group],
                    [d['current_std'] for d in data_group],
                    [d['peak_position'] for d in data_group]
                ])
                
                # Standardize features
                scaler = StandardScaler()
                features_scaled = scaler.fit_transform(features_matrix)
                
                # PLS with 2 components
                pls = PLSRegression(n_components=min(2, unique_concentrations-1))
                pls.fit(features_scaled, concentrations)
                concentrations_pred = pls.predict(features_scaled).flatten()
                
                r2_pls = r2_score(concentrations, concentrations_pred)
                
            except Exception as e:
                logger.warning(f"PLS failed for {scan_rate} mV/s: {e}")
                r2_pls = None
            
            # Plot results
            ax = axes[i]
            
            # Peak height calibration
            ax.scatter(concentrations, peak_heights, alpha=0.7, s=50, 
                      label=f'Peak Height (R¬≤={r2_height:.3f})')
            
            # Fit line
            conc_range = np.linspace(min(concentrations), max(concentrations), 100)
            height_fit = slope_h * conc_range + intercept_h
            ax.plot(conc_range, height_fit, 'r--', alpha=0.8)
            
            ax.set_xlabel('Concentration (mM)')
            ax.set_ylabel('Peak Height (¬µA)')
            ax.set_title(f'{scan_rate} mV/s Palmsens\n' + 
                        (f'PLS R¬≤={r2_pls:.3f}' if r2_pls else 'Linear only'))
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # Store results
            results_summary[scan_rate] = {
                'n_samples': len(data_group),
                'height_r2': r2_height,
                'area_r2': r2_area,
                'pls_r2': r2_pls,
                'height_slope': slope_h,
                'area_slope': slope_a,
                'concentrations': sorted(set(concentrations))
            }
            
            logger.info(f"   {scan_rate} mV/s: {len(data_group)} samples, "
                       f"Height R¬≤={r2_height:.3f}, Area R¬≤={r2_area:.3f}" +
                       (f", PLS R¬≤={r2_pls:.3f}" if r2_pls else ""))
        
        # Hide extra subplots
        for i in range(len(scan_rates), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"palmsens_quick_analysis_{timestamp}.png"
        plt.savefig(plot_filename, dpi=300, bbox_inches='tight')
        logger.info(f"üìä Analysis plots saved: {plot_filename}")
        
        # Save results
        report_filename = f"palmsens_quick_report_{timestamp}.json"
        with open(report_filename, 'w') as f:
            json.dump(results_summary, f, indent=2)
        logger.info(f"üìÑ Analysis report saved: {report_filename}")
        
        return results_summary
    
    def generate_summary(self):
        """Generate summary report"""
        logger.info("\nüìä Palmsens Quick Analysis Summary:")
        logger.info("=" * 40)
        
        total_files = len(self.processed_data)
        concentrations = sorted(set(d['concentration'] for d in self.processed_data))
        scan_rates = sorted(set(d['scan_rate'] for d in self.processed_data))
        
        peak_heights = [d['peak_height'] for d in self.processed_data]
        peak_areas = [d['peak_area'] for d in self.processed_data]
        
        logger.info(f"üìÇ Total processed files: {total_files}")
        logger.info(f"üß™ Concentrations: {concentrations} mM")
        logger.info(f"‚ö° Scan rates: {scan_rates} mV/s")
        logger.info(f"üìè Peak height range: {min(peak_heights):.1f} to {max(peak_heights):.1f} ¬µA")
        logger.info(f"üìê Peak area range: {min(peak_areas):.1f} to {max(peak_areas):.1f} ¬µA‚ãÖV")
        
        logger.info("\n‚úÖ Palmsens Quick Analysis completed!")
        logger.info("   - Linear calibration curves generated")
        logger.info("   - PLS models tested where possible")
        logger.info("   - Ready for comparison with STM32 data")

def main():
    """Main execution"""
    analyzer = QuickPalmsensAnalyzer(max_files_per_condition=3)
    analyzer.run_analysis()

if __name__ == "__main__":
    main()