# ğŸ§ª Peak Detection Methods Implementation Guide

## ğŸ¯ Overview
à¹€à¸­à¸à¸ªà¸²à¸£à¸™à¸µà¹‰à¸­à¸˜à¸´à¸šà¸²à¸¢à¸à¸²à¸£ implement 3 à¸§à¸´à¸˜à¸µà¸à¸²à¸£ peak detection à¹à¸¥à¸°à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹à¸šà¹ˆà¸‡à¹à¸¥à¹‰à¸§à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸—à¸µà¹ˆà¸¡à¸µà¸„à¸¸à¸“à¸ à¸²à¸

## ğŸ“Š Data Splits Available

### âœ… à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹à¸šà¹ˆà¸‡à¹à¸¥à¹‰à¸§ (seed=42):
- **ğŸ“š Training**: 2,114 files (63.4%)
- **ğŸ” Validation**: 302 files (9.1%)  
- **ğŸ§ª Test**: 916 files (27.5%)

### ğŸ”„ Cross-Instrument Splits:
- **PalmSensâ†’STM32**: Train=1,650, Test=1,682
- **STM32â†’PalmSens**: Train=1,682, Test=1,650

### ğŸŒŸ LOCO (Leave-One-Condition-Out):
- **Leave-Concentration-Out**: 24 splits
- **Leave-ScanRate-Out**: 11 splits
- **Leave-Electrode-Out**: 11 splits

## ğŸ† 3-Method Peak Detection Framework

### Method 1: ğŸ“ Baseline Detection (à¸à¸²à¸£à¸¥à¸²à¸à¹€à¸ªà¹‰à¸™)

#### ğŸ“– Theory:
- Manual à¸«à¸£à¸·à¸­ semi-automatic baseline correction
- à¹ƒà¸Šà¹‰à¸«à¸¥à¸±à¸à¸à¸²à¸£à¸—à¸²à¸‡à¸ªà¸–à¸´à¸•à¸´à¹à¸¥à¸° signal processing
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ training data

#### ğŸ’» Implementation Strategy:
```python
def baseline_peak_detection(cv_data):
    """
    1. Load CV data (V, uA)
    2. Apply smoothing (Savitzky-Golay, moving average)
    3. Find baseline (linear, polynomial, or adaptive)
    4. Subtract baseline
    5. Detect peaks (prominence, height thresholds)
    6. Return peak positions and properties
    """
    pass
```

#### ğŸ“Š Data Usage:
- **All Data**: à¹ƒà¸Šà¹‰à¸—à¸¸à¸à¹„à¸Ÿà¸¥à¹Œ (3,332 files) à¹€à¸à¸·à¹ˆà¸­ statistical robustness
- **No Training**: à¹„à¸¡à¹ˆà¹à¸šà¹ˆà¸‡ train/test à¹€à¸à¸£à¸²à¸°à¹€à¸›à¹‡à¸™ deterministic method
- **Parameter Tuning**: à¹ƒà¸Šà¹‰ validation set à¹€à¸à¸·à¹ˆà¸­ optimize parameters

#### ğŸ“ˆ Expected Output:
- Peak positions (V)
- Peak heights (uA)
- Peak widths
- Peak areas
- Signal-to-noise ratio

---

### Method 2: ğŸ“Š Statistical Peak Detection

#### ğŸ“– Theory:
- Advanced statistical methods
- Adaptive thresholding
- Multi-scale peak detection
- Noise characterization

#### ğŸ’» Implementation Strategy:
```python
def statistical_peak_detection(cv_data, params):
    """
    1. Noise estimation (MAD, robust statistics)
    2. Multi-scale smoothing
    3. Derivative-based peak detection
    4. Statistical significance testing
    5. Peak clustering and refinement
    6. Confidence intervals
    """
    pass
```

#### ğŸ“Š Data Usage:
- **Training Set**: 2,114 files à¸ªà¸³à¸«à¸£à¸±à¸š parameter optimization
- **Validation Set**: 302 files à¸ªà¸³à¸«à¸£à¸±à¸š hyperparameter tuning
- **Test Set**: 916 files à¸ªà¸³à¸«à¸£à¸±à¸š final evaluation

#### ğŸ›ï¸ Parameters to Optimize:
- Smoothing window size
- Peak prominence threshold  
- Noise estimation method
- Clustering parameters

---

### Method 3: ğŸ¤– Machine Learning Peak Detection

#### ğŸ“– Theory:
- Deep learning à¸«à¸£à¸·à¸­ classical ML
- Feature extraction from CV curves
- Pattern recognition
- Transfer learning à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸¡à¸·à¸­

#### ğŸ’» Implementation Strategy:
```python
def ml_peak_detection(cv_data, model):
    """
    1. Feature extraction (shape, derivatives, transforms)
    2. Data preprocessing and normalization
    3. Model prediction (CNN, LSTM, SVM, etc.)
    4. Post-processing and peak refinement
    5. Uncertainty quantification
    """
    pass
```

#### ğŸ“Š Data Usage:
- **Training Set**: 2,114 files à¸ªà¸³à¸«à¸£à¸±à¸š model training
- **Validation Set**: 302 files à¸ªà¸³à¸«à¸£à¸±à¸š model selection & hyperparameters
- **Test Set**: 916 files à¸ªà¸³à¸«à¸£à¸±à¸š unbiased evaluation

#### ğŸ¯ ML Approaches:
1. **CNN-based**: Direct peak detection from CV curves
2. **Feature-based**: Extract features â†’ classical ML
3. **Transfer Learning**: Pre-train on PalmSens â†’ adapt to STM32
4. **Ensemble Methods**: Combine multiple approaches

## ğŸ”„ Validation Strategies

### 1. **Primary Validation (70/15/15)**
```python
# Train algorithms
train_data = load_file_list('validation_data/splits/train_files.txt')
val_data = load_file_list('validation_data/splits/validation_files.txt')

# Final evaluation
test_data = load_file_list('validation_data/splits/test_files.txt')
```

### 2. **Cross-Instrument Validation**
```python
# PalmSens â†’ STM32 transfer
palmsens_train = load_file_list('validation_data/splits/cross_instrument/palmsens_train_stm32_test_train.txt')
stm32_test = load_file_list('validation_data/splits/cross_instrument/palmsens_train_stm32_test_test.txt')

# STM32 â†’ PalmSens transfer  
stm32_train = load_file_list('validation_data/splits/cross_instrument/stm32_train_palmsens_test_train.txt')
palmsens_test = load_file_list('validation_data/splits/cross_instrument/stm32_train_palmsens_test_test.txt')
```

### 3. **LOCO Validation**
```python
# Leave-One-Concentration-Out example
for conc_split in glob('validation_data/splits/loco_splits/leave_concentration_out/*_train.txt'):
    train_data = load_file_list(conc_split)
    test_data = load_file_list(conc_split.replace('_train.txt', '_test.txt'))
    # Train and evaluate
```

## ğŸ“ Evaluation Metrics

### 1. **Peak Detection Accuracy**
- **Precision**: True peaks detected / Total peaks detected
- **Recall**: True peaks detected / Total true peaks  
- **F1-Score**: Harmonic mean of precision and recall
- **mAP**: Mean Average Precision

### 2. **Peak Characterization Accuracy**
- **Position Error**: |detected_position - true_position|
- **Height Error**: |detected_height - true_height| / true_height
- **Area Error**: |detected_area - true_area| / true_area

### 3. **Cross-Instrument Performance**
- **Transfer Accuracy**: Performance when trained on one instrument, tested on another
- **Magnitude Robustness**: Performance across different current magnitudes
- **Consistency**: Std dev of results across conditions

### 4. **Computational Metrics**
- **Processing Time**: Time per CV curve
- **Memory Usage**: Peak memory consumption
- **Scalability**: Performance vs dataset size

## ğŸ“ Implementation File Structure

```
validation_data/
â”œâ”€â”€ peak_detection/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ baseline_detection.py       # Method 1
â”‚   â”œâ”€â”€ statistical_detection.py   # Method 2  
â”‚   â”œâ”€â”€ ml_detection.py            # Method 3
â”‚   â”œâ”€â”€ evaluation_metrics.py      # Evaluation tools
â”‚   â””â”€â”€ utils.py                   # Common utilities
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ experiment_1_baseline.py
â”‚   â”œâ”€â”€ experiment_2_statistical.py
â”‚   â”œâ”€â”€ experiment_3_ml.py
â”‚   â””â”€â”€ experiment_4_comparison.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ baseline_results/
â”‚   â”œâ”€â”€ statistical_results/
â”‚   â”œâ”€â”€ ml_results/
â”‚   â””â”€â”€ comparison_analysis/
â””â”€â”€ notebooks/
    â”œâ”€â”€ data_exploration.ipynb
    â”œâ”€â”€ method_development.ipynb
    â””â”€â”€ results_visualization.ipynb
```

## ğŸ¯ Expected Research Questions

### 1. **Method Comparison**
- Which method performs best overall?
- How do methods perform across different conditions?
- What are the trade-offs (accuracy vs speed vs complexity)?

### 2. **Cross-Instrument Transferability**  
- Can we train on PalmSens and deploy on STM32H743?
- How much performance degradation occurs?
- What adaptation strategies work best?

### 3. **Condition Robustness**
- How do methods perform on unseen concentrations?
- Are methods robust to different scan rates?
- Do electrode differences affect performance?

### 4. **Practical Deployment**
- Which method is most suitable for real-time analysis?
- What are the computational requirements?
- How should we handle edge cases?

## ğŸ”„ Implementation Timeline

### Phase 1: Baseline Implementation (Week 1)
- [x] Data splitting completed
- [ ] Implement Method 1 (Baseline Detection)
- [ ] Basic evaluation pipeline

### Phase 2: Statistical Methods (Week 2)  
- [ ] Implement Method 2 (Statistical Detection)
- [ ] Parameter optimization framework
- [ ] Cross-validation setup

### Phase 3: Machine Learning (Week 3)
- [ ] Implement Method 3 (ML Detection)
- [ ] Feature engineering
- [ ] Model training and validation

### Phase 4: Comparison & Analysis (Week 4)
- [ ] Comprehensive comparison
- [ ] Cross-instrument analysis
- [ ] LOCO validation
- [ ] Final report and visualization

## ğŸ‰ Success Criteria

- **Technical**: All 3 methods implemented and evaluated
- **Scientific**: Clear ranking of methods with statistical significance
- **Practical**: Deployment recommendations for STM32H743
- **Reproducible**: All results reproducible with provided data splits

---

*Next: Implement Method 1 - Baseline Peak Detection*  
*Last Updated: August 16, 2025*
